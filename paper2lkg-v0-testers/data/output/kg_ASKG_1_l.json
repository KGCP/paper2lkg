{
  "iri": "Paper-MEL_Metadata_Extractor__Loader",
  "title": "MEL: Metadata Extractor & Loader",
  "authors": [
    "Sergio J. Rodr\u00edguez M\u00e9ndez",
    "Pouya G. Omran",
    "Armin Haller",
    "KerryTaylor"
  ],
  "keywords": [
    "Metadata Extraction",
    "Information Extraction",
    "Data Preprocessing",
    "Knowledge Graph Construction",
    "Data Analysis Pipeline"
  ],
  "sections": [
    {
      "iri": "Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-1-Sentence-1",
              "text": "The metadata and content-based information extraction tasks from heterogeneous file sets are pre-processing steps of many Knowledge Graph Construction Pipelines (KGCP)."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-2",
              "text": "These tasks often take longer than necessary due to the lack of proper tools that integrate several complementary extraction methods and properties to get a rich output set."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-3",
              "text": "This paper presents MEL, a Python-based tool that implements a set of methods to extract metadata and content-based information from unstructured information encoded in different source document formats."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-4",
              "text": "The results are generated as JSON files, which can: (a) optionally be stored in a document store, and (b) easily be mapped to RDF using a variety of tools such as J2RM."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-5",
              "text": "MEL supports more than 20 different file types, making it a versatile tool that aids pre-processing tasks as part of a KGCP based on comprehensive configurable settings."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-2",
      "subtitle": "Introduction",
      "paragraphs": [
        {
          "iri": "Section-2-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-1-Sentence-1",
              "text": "This paper introduces MEL, a tool that implements a set of methods to extract metadata and content-based information from various file formats as JSON objects."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-2",
              "text": "For each supported file type, MEL extracts the textual content from the source document and performs specific pre-processing and data cleaning tasks."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-3",
              "text": "Also, it performs basic text analysis tasks (pattern matching and keyword extraction) and generates the results in a machine-readable format (JSON), preparing the ground for content-based analysis."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-4",
              "text": "MEL is integrated with \u201cThe NLP -NER Toolkit\u201d (TNNT), which automates the extraction task of categorised named entities from the MEL results by using diverse state-of-the-art NLP tools and NER models [5]."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-5",
              "text": "MEL implements primitives for metadata and content extraction from unstructured data sets of heterogeneous formats, and along with the TNNT results, it provides the groundwork for content-based analysis."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-6",
              "text": "MEL and TNNT were developed in conjunction with J2RM [4], to easily map the JSON results to RDF as part of an automated KGCP."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-3",
      "subtitle": "Core Features",
      "paragraphs": [
        {
          "iri": "Section-3-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-1-Sentence-1",
              "text": "MEL has comprehensive metadata extraction support of various file types and formats."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-2",
              "text": "In a nutshell: (1) it takes as input a document (file) set; (2) then, for each document, it extracts its related metadata and content-based information, while performing basic text analysis (such as applying a configurable set of regular expressions and keyword extraction task); and, (3) as output, it generates a JSON file with the extracted metadata and text content with a structure based on the supported formats' document object model."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-3",
              "text": "It can store the results in a document store."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-4",
              "text": "MEL's general output structure is presented in Table 1."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-5",
              "text": "MEL has a detailed configuration JSON file that defines how the processing will be performed through a set of parameters and flags that establish the initial settings related to the document store, input document sets, TNNT general configuration, file extension mappings, the \u201cAssociated-Metadata\u201d processing (Table 1), and regular expressions to apply in the text analysis task, among other settings."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-6",
              "text": "The supported file types are presented in Table 2."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-7",
              "text": "The third column shows the theoretical number of attributes that the tool is able to extract per document type, whilst the fourth column shows the average of the extracted attributes from four use case document sets."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-8",
              "text": "OLE 2 file types and .docm can only be processed on Windows operating systems."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-9",
              "text": "Specifically for OLE 2 file types, MEL uses the olemeta tool."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-4",
      "subtitle": "Architecture",
      "paragraphs": [
        {
          "iri": "Section-4-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-1-Sentence-1",
              "text": "MEL is fully integrated with TNNT as depicted in Figure 1."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-2",
              "text": "The set of Python-based methods implemented in MEL are generic and can be applied to extract the content and metadata of all supported file types."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-3",
              "text": "MEL uses various opensource packages and tools with complementary capabilities to form a \u201cSwiss army knife\u201d of metadata and content-based information extraction from heterogeneous document sets."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-4",
              "text": "As part of the \u201cGeneral-Metadata\u201d extraction task, MEL optionally uses the XML output from the NLNZ Metadata Extractor tool, a Java standalone tool that extracts a comprehensive attribute and property list from dozens of file formats."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-5",
              "text": "The MEL general processing model is presented in Figure 2."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-6",
              "text": "It is important to note that each file type has its own specific processing model as well as the text analysis task, which is the last step that is performed for any output."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-5",
      "subtitle": "Related Work",
      "paragraphs": [
        {
          "iri": "Section-5-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-1-Sentence-1",
              "text": "The most comprehensive and current state-of-the-art tool for content extraction and analysis is Apache Tika, which is a complete and complex Java-based general-purpose system."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-2",
              "text": "While MEL's core goals resemble the ones of Apache Tika, the main difference and benefit of MEL as compared to Apache Tika is that it is a lightweight Python-based package for the metadata extraction of common file formats aimed to be used in a KGCP."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-3",
              "text": "Although there is a wide range of Python-based tools and libraries for metadata extraction, to the best of our knowledge, there is no package available that fully integrates in one system a comprehensive set of methods for metadata and content extraction of common file formats that generate the results in JSON structures based on the document object model of each format type."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-4",
              "text": "Last, MEL can assist in the information extraction stage of several KGCPs, such as the ones described in [6], [2], and [3]."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-6",
      "subtitle": "Conclusions and Future Work",
      "paragraphs": [
        {
          "iri": "Section-6-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-1-Sentence-1",
              "text": "MEL provides a versatile mechanism to extract metadata and content-based information from unstructured data sets of heterogeneous file formats, agnostic of the data sets' domain (general purpose)."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-2",
              "text": "It has been tested over thousands of documents using different formats and datasets as part of the AGRIF project."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-3",
              "text": "Based on the structure of the MEL's JSON results, it is possible to easily add a vocabulary or light-weight ontology using JSON-LD annotations, in order to make the extracted metadata \u201cRDF ready\u201d."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-4",
              "text": "This will be explored in the near future leveraging on the integration with JSON-LD ontologies."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-5",
              "text": "More file formats will be added in a per use-case requirements basis, in order to support KGCP tasks."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-6",
              "text": "Additionally, a project to \u201ccontainerise\u201d the MEL+TNNT tools is planned in the near future."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-7",
              "text": "The major contributions of this tool are: (1) the ability to extract metadata sets and content-based information from different source document formats; (2) the comprehensive support of over 20 different file types/formats integrated into one easy-to-use Python-based system; (3) integration with TNNT which automates the extraction of categorised named entities from the results by using diverse state-of-the-art NLP tools and NER models; and (4) the JSON result files can be easily mapped to RDF using J2RM."
            }
          ]
        }
      ]
    }
  ],
  "summary": "The metadata and content-based information extraction tasks from heterogeneous file sets are pre-processing steps of many Knowledge Graph Construction Pipelines (KGCP). These tasks often take longer than necessary due to the lack of proper tools that integrate several complementary extraction methods and properties to get a rich output set. This paper presents MEL, a Python-based tool that implements a set of methods to extract metadata and content-based information from unstructured information encoded in different source document formats. The results are generated as JSON files, which can: (a) optionally be stored in a document store, and (b) easily be mapped to RDF using a variety of tools such as J2RM. MEL supports more than 20 different file types, making it a versatile tool that aids pre-processing tasks as part of a KGCP based on comprehensive configurable settings.\n\nThis paper introduces MEL, a tool that implements a set of methods to extract metadata and content-based information from various file formats as JSON objects. For each supported file type, MEL extracts the textual content from the source document and performs specific pre-processing and data cleaning tasks. Also, it performs basic text analysis tasks (pattern matching and keyword extraction) and generates the results in a machine-readable format (JSON), preparing the ground for content-based analysis. MEL is integrated with \u201cThe NLP -NER Toolkit\u201d (TNNT), which automates the extraction task of categorised named entities from the MEL results by using diverse state-of-the-art NLP tools and NER models [5]. MEL implements primitives for metadata and content extraction from unstructured data sets of heterogeneous formats, and along with the TNNT results, it provides the groundwork for content-based analysis. MEL and TNNT were developed in conjunction with J2RM [4], to easily map the JSON results to RDF as part of an automated KGCP.\n\nMEL has comprehensive metadata extraction support of various file types and formats. In a nutshell: (1) it takes as input a document (file) set; (2) then, for each document, it extracts its related metadata and content-based information, while performing basic text analysis (such as applying a configurable set of regular expressions and keyword extraction task); and, (3) as output, it generates a JSON file with the extracted metadata and text content with a structure based on the supported formats' document object model. It can store the results in a document store. MEL's general output structure is presented in Table 1. MEL has a detailed configuration JSON file that defines how the processing will be performed through a set of parameters and flags that establish the initial settings related to the document store, input document sets, TNNT general configuration, file extension mappings, the \u201cAssociated-Metadata\u201d processing (Table 1), and regular expressions to apply in the text analysis task, among other settings. The supported file types are presented in Table 2. The third column shows the theoretical number of attributes that the tool is able to extract per document type, whilst the fourth column shows the average of the extracted attributes from four use case document sets. OLE 2 file types and .docm can only be processed on Windows operating systems. Specifically for OLE 2 file types, MEL uses the olemeta tool.\n\nMEL is fully integrated with TNNT as depicted in Figure 1. The set of Python-based methods implemented in MEL are generic and can be applied to extract the content and metadata of all supported file types. MEL uses various opensource packages and tools with complementary capabilities to form a \u201cSwiss army knife\u201d of metadata and content-based information extraction from heterogeneous document sets. As part of the \u201cGeneral-Metadata\u201d extraction task, MEL optionally uses the XML output from the NLNZ Metadata Extractor tool, a Java standalone tool that extracts a comprehensive attribute and property list from dozens of file formats. The MEL general processing model is presented in Figure 2. It is important to note that each file type has its own specific processing model as well as the text analysis task, which is the last step that is performed for any output.\n\nThe most comprehensive and current state-of-the-art tool for content extraction and analysis is Apache Tika, which is a complete and complex Java-based general-purpose system. While MEL's core goals resemble the ones of Apache Tika, the main difference and benefit of MEL as compared to Apache Tika is that it is a lightweight Python-based package for the metadata extraction of common file formats aimed to be used in a KGCP. Although there is a wide range of Python-based tools and libraries for metadata extraction, to the best of our knowledge, there is no package available that fully integrates in one system a comprehensive set of methods for metadata and content extraction of common file formats that generate the results in JSON structures based on the document object model of each format type. Last, MEL can assist in the information extraction stage of several KGCPs, such as the ones described in [6], [2], and [3].\n\nMEL provides a versatile mechanism to extract metadata and content-based information from unstructured data sets of heterogeneous file formats, agnostic of the data sets' domain (general purpose). It has been tested over thousands of documents using different formats and datasets as part of the AGRIF project. Based on the structure of the MEL's JSON results, it is possible to easily add a vocabulary or light-weight ontology using JSON-LD annotations, in order to make the extracted metadata \u201cRDF ready\u201d. This will be explored in the near future leveraging on the integration with JSON-LD ontologies. More file formats will be added in a per use-case requirements basis, in order to support KGCP tasks. Additionally, a project to \u201ccontainerise\u201d the MEL+TNNT tools is planned in the near future. The major contributions of this tool are: (1) the ability to extract metadata sets and content-based information from different source document formats; (2) the comprehensive support of over 20 different file types/formats integrated into one easy-to-use Python-based system; (3) integration with TNNT which automates the extraction of categorised named entities from the results by using diverse state-of-the-art NLP tools and NER models; and (4) the JSON result files can be easily mapped to RDF using J2RM.",
  "kg2text": [
    "This paper presents MEL, a Python-based tool that extracts metadata and content- based information from various file types. The MEL is implemented by a versatile tool that aids pre-processing tasks as part of a Knowledge Graph Construction Pipeline (KGCP) with comprehensive configurable settings. Additionally, the MEL can assist in the information extraction stage of several KGCPs, such as those described in [6], [2], and [3]. Furthermore, this paper presents MEL+TNNT, which integrates metadata extraction, content-based information extraction, and named entity recognition (NER) capabilities from diverse file formats. The toolset is implemented by the same versatile tool that aids pre-processing tasks.",
    "The MEL (Metadata Extractor & Loader) tool, implemented by a versatile tool that aids pre-processing tasks as part of a Knowledge Graph Construction Pipeline based on comprehensive configurable settings, extracts metadata and content-based information from various file formats. This paper presents the MEL, which can assist in the information extraction stage of several KGCPs, such as those described in [6], [2], and [3]. The tool is designed to extract textual content from source documents and generate JSON output for use in Knowledge Graph Construction Pipelines.",
    "The MEL, a lightweight Python-based package for metadata extraction of common file formats, aids pre-processing tasks as part of Knowledge Graph Construction Pipelines (KGCPs) with comprehensive configurable settings. It implements methods to extract metadata and content-based information from various file types as JSON objects. The toolset, MEL+TNNT, assists in the information extraction stage of several KGCPs, including those described in [6], [2], and [3]. Additionally, MEL extracts textual content from source documents and generates output as JSON files.",
    "The MEL, a Python-based tool, aids pre-processing tasks as part of Knowledge Graph Construction Pipelines (KGCP) with comprehensive configurable settings. It provides metadata and content-based information extraction capabilities from various file formats, generating JSON files containing extracted metadata and text content. The MEL can assist in the information extraction stage of several KGCPs, such as those described in [6], [2], and [3]. Additionally, it integrates named entity recognition (NER) capabilities through its integration with TNNT tools.",
    "MEL, a Python-based metadata extractor and loader, extracts textual content from source documents to generate JSON files. It integrates with MEL+TNNT tools to extract metadata and content- based information from various file formats. The tool can assist in pre-processing tasks as part of Knowledge Graph Construction Pipelines (KGCPs) and is generic enough to be applied to all supported file types. Additionally, it generates a JSON file containing extracted metadata and text content.",
    "MEL, a lightweight Python-based package for metadata extraction of common file formats, can be applied to extract content and metadata from various file types. It extracts textual content from source documents and generates JSON files with extracted data. MEL's primitives for metadata and content extraction are implemented by the tool itself, which is part of MEL. Additionally, MEL assists in information extraction stages of several Knowledge Graph Construction Pipelines (KGCPs). Furthermore, MEL+TNNT tools integrate metadata extraction, content-based information extraction, and named entity recognition capabilities from diverse file formats.",
    "MEL, a Python-based tool for metadata extraction and content- based information processing from unstructured file sets, implements methods to extract metadata and content- based information from various file formats as JSON objects. It generates The metadata by extracting related metadata and content- based information from input files, performing basic text analysis such as applying regular expressions and keyword extraction tasks. MEL's 'It' component extracts metadata and content-based information, which can be applied to extract the content and metadata of all supported file types, generating JSON files with extracted data.",
    "MEL, a Python-based tool for metadata extraction and content- based information processing from unstructured file sets, implements primitives for metadata and content extraction from unstructured data sets of heterogeneous formats. It generates the results in a machine-readable format (JSON) and supports more than 20 different file types. MEL can assist in the information extraction stage of several Knowledge Graph Construction Pipelines (KGCPs), such as those described in [6], [2], and [3]. Additionally, it extracts textual content from source documents, performs specific pre-processing and data cleaning tasks, and optionally uses the XML output from the NLNZ Metadata Extractor tool. The extracted metadata and content-based information can be stored in a document store.",
    "MEL, a versatile tool that aids pre-processing tasks as part of a Knowledge Graph Construction Pipeline (KGCP) based on comprehensive configurable settings. It has broadened its capabilities to integrate with TNNT and support various file types, including OLE 2 formats. MEL's JSON results can be easily mapped to RDF using J2RM, making it 'RDF ready'. The tool is fully integrated with the AGRIF project, which tests MEL's capabilities on thousands of documents using different formats and datasets. Furthermore, MEL performs basic text analysis tasks such as pattern matching and keyword extraction. Its extracted metadata has a broader term that encompasses detailed data or information about a topic.",
    "MEL, a Python-based tool, extracts metadata and content- based information from various file types. It defines configuration JSON files for specific document formats. MEL can be compared to Apache Tika, another comprehensive software project for content extraction and analysis. For each supported file type, it performs pre-processing and data cleaning tasks. The extracted metadata includes detailed information about the topic, such as attributes per document type. Additionally, MEL uses open-source packages and tools with complementary capabilities. Its related metadata and content-based information can be mapped to RDF using J2RM.",
    "MEL, a metadata extractor and loader tool, supports more than 20 different file types. It extracts comprehensive attribute and property lists from these files using its own specific processing model. The extracted attributes can be used for content- based information extraction tasks, such as extracting relevant information from unstructured data sources like documents or files. MEL's capabilities are further enhanced by the addition of more file formats to support Knowledge Graph Construction Pipeline (KGCP) tasks.",
    "MEL, presented in Table 1, offers a software framework for extracting metadata and content-based information from unstructured data sources. Each file type has its own processing model, with MEL capable of analyzing specific types of documents using tools like TNNT. The tool uses open-source packages to extract relevant information, generating results as JSON structures based on the document object model of each format type. On average, MEL extracts a certain number of attributes per document type from four use case sets, showcasing its capabilities in content extraction and analysis tasks.",
    "The J2RM tool maps JSON files to RDF, enabling metadata extraction and content-based information processing. MEL's JSON results are generated as a product of text analysis tasks, including keyword extraction. The AGRIF project utilizes MEL to extract metadata from heterogeneous file formats. This paper introduces the Metadata Extractor & Loader (MEL) framework for extracting metadata and content- based information from unstructured data sets. Common file formats can generate JSON structures based on document object models of each format type, making them RDF ready. The results refer to the metadata extraction outputs generated as JSON files that can optionally be stored in a document store or easily mapped to RDF.",
    "The TNNT results, generated by The NLP-NER Toolkit (TNNT), automate the extraction task of categorized named entities from MEL's metadata and content-based information. These results are based on regular expressions to apply in the text analysis task, which involves pattern matching and keyword extraction. To support KGCP tasks, a document set with various file types is required, including JSON files that represent source document formats. Apache Tika, being the most comprehensive and current state-of-the-art tool for content extraction and analysis, can handle heterogeneous file formats as JSON objects. Additionally, Python-based tools and libraries generate results in JSON structures based on the document object model of each format type. Furthermore, TNNT has a broader term that is ontologies, which provide a formal system to represent and structure knowledge.",
    "The integration of MEL's results with JSON-LD ontologies enables the extraction of metadata from unstructured information encoded in different source document formats. This process involves identifying key words or phrases through keyword extraction, which can be achieved using various methods. The extracted metadata can then be represented as structured data in file types such as JSON files and XML output. In the near future, this technology will be explored further to make extracted metadata 'RDF ready'.",
    "The Knowledge Graph Construction Pipeline tasks rely on file formats for organizing digital data. The regular expressions to apply in text analysis are a subset of pattern matching techniques, which can be used to extract meaningful information from data. JSON structures and source document formats provide standardized representations for storing and exchanging structured information. Metadata extraction of common file formats is one of the many tasks involved in this project. RDF and JSON-LD annotations enable semantic meaning and context to be added to JSON data by providing machine-readable descriptions of entities, properties, and relationships within ontologies. The results are extracted metadata and content-based information generated as JSON files.",
    "The AGRIF project, a research initiative that involves testing MEL's capabilities on thousands of documents using different formats and datasets. The project utilizes various file types such as .docm, which can only be processed by Windows. To achieve this goal, the team employs content-based information extraction tasks, including basic text analysis techniques like pattern matching and keyword extraction. These tasks are part of a broader set of Knowledge Graph Construction Pipeline (KGCP) tasks that involve specific pre-processing and data cleaning operations. The results refer to the metadata extraction outputs generated as JSON files that can optionally be stored in a document store or easily mapped to RDF.",
    "The outcome or product of a test or experiment using Terminal Nucleotidyl Transferase (TNNT) can be considered as having a broader term, which refers to the general concept of results. This implies that TNNT results are a specific type of result."
  ],
  "times": [
    128.49695587158203
  ]
}