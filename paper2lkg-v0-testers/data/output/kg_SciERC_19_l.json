{
  "iri": "Paper-19",
  "title": "INTERSPEECH_2008_20_abs",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-19-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-19-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-1",
              "text": "In the study of expressive speech communication , it is commonly accepted that the emotion perceived by the listener is a good approximation of the intended emotion conveyed by the speaker ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-2",
              "text": "This paper analyzes the validity of this assumption by comparing the mismatches between the assessments made by na \u00a8 \u0131ve listeners and by the speakers that generated the data ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-3",
              "text": "The analysis is based on the hypothesis that people are better decoders of their own emotions ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-4",
              "text": "Therefore , self-assessments will be closer to the intended emotions ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-5",
              "text": "Using the IEMOCAP database , discrete -LRB- categorical -RRB- and continuous -LRB- attribute -RRB- emotional assessments evaluated by the actors and na \u00a8 \u0131ve listeners are compared ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-6",
              "text": "The results indicate that there is a mismatch between the expression and perception of emotion ."
            },
            {
              "iri": "Paper-19-Section-1-Paragraph-1-Sentence-7",
              "text": "The speakers in the database assigned their own emotions to more specific emotional categories , which led to more extreme values in the activation-valence space ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "In the study of expressive speech communication , it is commonly accepted that the emotion perceived by the listener is a good approximation of the intended emotion conveyed by the speaker . This paper analyzes the validity of this assumption by comparing the mismatches between the assessments made by na \u00a8 \u0131ve listeners and by the speakers that generated the data . The analysis is based on the hypothesis that people are better decoders of their own emotions . Therefore , self-assessments will be closer to the intended emotions . Using the IEMOCAP database , discrete -LRB- categorical -RRB- and continuous -LRB- attribute -RRB- emotional assessments evaluated by the actors and na \u00a8 \u0131ve listeners are compared . The results indicate that there is a mismatch between the expression and perception of emotion . The speakers in the database assigned their own emotions to more specific emotional categories , which led to more extreme values in the activation-valence space .",
  "kg2text": [
    "This study, based on actors and naive listeners compared using the IEMOCAP database, analyzes the validity of an assumption that people are better decoders of their own emotions. The analysis reveals that speakers assigned their own emotions to more specific emotional categories, which deals with emotion perceived by the listener. Furthermore, it concerns intended emotion conveyed by the speaker, which has a broader term as 'intended emotions'. This paper also explores how self-assessments will be closer to intended emotion conveyed by the speaker.",
    "A comparison of emotional assessments made by speakers and naive listeners, revealing a discrepancy in the expression and perception of emotions. The results show that intended emotions led to more extreme values in the activation-valence space, which supports the hypothesis that people are better decoders of their own emotions. This assumption is based on an analysis of expressive speech communication, which involves studying how spoken language conveys and interprets emotions, attitudes, and intentions. The study used IEMOCAP database, a publicly available collection of audio, video, and text recordings with corresponding emotional annotations. Furthermore, the results also highlight that speakers are people who convey information or ideas through verbal communication."
  ],
  "times": [
    13.91616415977478
  ]
}