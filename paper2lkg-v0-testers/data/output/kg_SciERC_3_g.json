{
  "iri": "Paper-3",
  "title": "INTERSPEECH_2013_21_abs",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-3-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-3-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-1",
              "text": "This work proposes a new research direction to address the lack of structures in traditional n-gram models ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-2",
              "text": "It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-3",
              "text": "Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-4",
              "text": "Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-5",
              "text": "This posterior encodes sparse se-lectional preferences between a head word and its dependents ."
            },
            {
              "iri": "Paper-3-Section-1-Paragraph-1-Sentence-6",
              "text": "The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "This work proposes a new research direction to address the lack of structures in traditional n-gram models . It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus . Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge . Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus . This posterior encodes sparse se-lectional preferences between a head word and its dependents . The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions .",
  "kg2text": [
    "This work proposes a new research direction that utilizes The model, which is a weakly supervised dependency parser designed to enhance traditional n-gram models. This new research direction addresses the lack of structures in traditional n-gram models, which limits their ability to capture complex linguistic relationships. The model has a broader term, model, and is evaluated on English and Czech newspaper texts as well as validated on French broadcast news transcriptions. The model creates complex tree structures through the application of hand-crafted rules, which encode basic syntactic knowledge. The weakly supervised dependency parser can model speech syntax without relying on an annotated training corpus, instead employing Bayesian inference to sample rules. This approach represents a significant advancement in the field of natural language processing, as it provides a structured method to analyze language without the need for extensive labeled data.",
    "Hand-crafted rules encode basic syntactic knowledge, which serves as a foundational element in understanding syntax. These rules have a broader term, encompassing the concept of basic syntactic knowledge itself. In the realm of textual data, English and Czech newspaper texts represent a broader category known as newspaper texts. Within the structure of phrases, a head word has its dependents, which are the words that rely on it for their grammatical function. This relationship is also true for a head word, emphasizing the importance of these central words in sentence construction. Furthermore, labeled data, which is crucial for training machine learning models, is replaced by hand-crafted rules, highlighting the shift towards expert-defined guidelines. Lastly, hand-crafted rules also have a broader term that includes syntactic knowledge, underscoring their role in the study of language structure."
  ],
  "times": [
    8.638163566589355
  ]
}