{
  "iri": "Paper-Learning_SHACL_shapes_from_knowledge_graphs",
  "title": "Learning SHACL shapes from knowledge graphs",
  "authors": [
    "Pouya Ghiasnezhad Omran",
    "Kerry Taylor",
    "Sergio Rodr\u00edguez M\u00e9ndez",
    "Armin Haller"
  ],
  "keywords": [
    "HACL shape learning",
    "hapes constraint language",
    "knowledge graph",
    "inverse open path rule"
  ],
  "sections": [
    {
      "iri": "Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-1-Sentence-1",
              "text": "Knowledge Graphs (KGs) have proliferated on the Web since the introduction of knowledge panels to Google search in 2012."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-2",
              "text": "KGs are large data-first graph databases with weak inference rules and weakly-constraining data schemes."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-3",
              "text": "SHACL, the Shapes Constraint Language, is a W3C recommendation for expressing constraints on graph data as shapes."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-4",
              "text": "SHACL shapes serve to validate a KG, to underpin manual KG editing tasks, and to offer insight into KG structure."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-5",
              "text": "Often in practice, large KGs have no available shape constraints and so cannot obtain these benefits for ongoing maintenance and extension."
            }
          ]
        },
        {
          "iri": "Section-1-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-2-Sentence-1",
              "text": "We introduce Inverse Open Path (IOP) rules, a predicate logic formalism which presents specific shapes in the form of paths over connected entities that are present in a KG."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-2",
              "text": "IOP rules express simple shape patterns that can be augmented with minimum cardinality constraints and also used as a building block for more complex shapes, such as trees and other rule patterns."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-3",
              "text": "We define formal quality measures for IOP rules and propose a novel method to learn high-quality rules from KGs."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-4",
              "text": "We show how to build high-quality tree shapes from the IOP rules."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-5",
              "text": "Our learning method, SHACLEARNER, is adapted from a state-of-the-art embedding-based open path rule learner."
            }
          ]
        },
        {
          "iri": "Section-1-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-3-Sentence-1",
              "text": "We evaluate SHACLEARNER on some real-world massive KGs, including YAGO2s with 4 million facts, DBpedia 3.8 with 11 million facts, and Wikidata with 8 million facts."
            },
            {
              "iri": "Section-1-Paragraph-3-Sentence-2",
              "text": "The experiments show that our SHACLEARNER can effectively learn informative and intuitive shapes from massive KGs."
            },
            {
              "iri": "Section-1-Paragraph-3-Sentence-3",
              "text": "The shapes are diverse in structural features such as depth and width, and also in quality measures that indicate confidence and generality."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-2",
      "subtitle": "Introduction",
      "paragraphs": [
        {
          "iri": "Section-2-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-1-Sentence-1",
              "text": "While public knowledge graphs (KGs) became popular with the development of DBpedia and Yago more than a decade ago, interest in enterprise knowledge graphs has taken off since the inclusion of knowledge panels on the Google Search engine in 2012, driven by its internal knowledge graph."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-2",
              "text": "Although these KGs are massive and diverse, they are typically incomplete."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-3",
              "text": "Regardless of the method that is used to build a KG, whether collaboratively or individually, manually or automatically, it will be incomplete because of the evolving nature of human knowledge, cultural bias, and resource constraints."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-4",
              "text": "Consider Wikidata, for example, where there is more complete information for some types of entities, such as pop stars, while less for others, like opera singers."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-5",
              "text": "Even for the same type of entity, such as computer scientists, there are different depths of detail for similarly accomplished scientists, depending on their country of residence."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-2-Sentence-1",
              "text": "However, the power of KGs comes from their data-first approach, enabling contributors to extend a KG in a relatively arbitrary manner."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-2",
              "text": "By contrast, a relational database typically employs not-null and other schema-based constraints that require some attributes to be instantiated in a defined way at all times."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-3",
              "text": "Large KGs are typically populated by automatic and semi-automatic methods using non-structured sources such as Wikipedia that are prone to errors of omission and commission."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-4",
              "text": "Both kinds of errors can be highlighted for correction by a careful application of schema constraints."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-5",
              "text": "However, such constraints are commonly unavailable and, if available, uncertain and frequently violated in a KG for valid reasons, arising from the intended data-first approach of KG applications."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-3-Sentence-1",
              "text": "SHACL was formally recommended by the W3C in 2017 to express such constraints on a KG as shapes."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-2",
              "text": "For example, SHACL can be used to express that a person in a specific use case needs to have a name, birth date, and place of birth, and that these attributes have particular types: a string, a date, and a location."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-3",
              "text": "The shapes are used to guide the population of a KG, although they are not necessarily enforced."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-4",
              "text": "Typically, SHACL shapes are manually specified."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-5",
              "text": "However, as for multidimensional relational database schemes, shapes could, in principle, be inferred from KG data."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-6",
              "text": "As frequent patterns, the shapes characterize a KG and can be used for subsequent data cleaning or ongoing data entry."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-7",
              "text": "There is scant previous research on this topic."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-4-Sentence-1",
              "text": "While basic SHACL and its advanced features allow the modeling of diverse shapes including rules and constraints, most of these shapes are previously well known when expressed by alternative formalisms, including closed rules, trees, existential rules, and graph functional dependencies."
            },
            {
              "iri": "Section-2-Paragraph-4-Sentence-2",
              "text": "We claim that the common underlying form of all these shapes is the path, over which additional constraints induce alternative shapes."
            },
            {
              "iri": "Section-2-Paragraph-4-Sentence-3",
              "text": "For example, in DBpedia we see the following path: if an entity x is a song, then x is in an album y which has a record label z."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-5-Sentence-1",
              "text": "Since the satisfaction of a less-constrained shape is a necessary condition for satisfaction of a more complex shape but not a sufficient condition, in this paper we focus on learning paths, the least constrained shape for our purposes."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-2",
              "text": "In addition, we learn cardinality constraints that can express, for example, that a song has at least 2 producers."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-3",
              "text": "We also investigate the process of constructing one kind of more complex shape, that is a tree, out of paths."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-4",
              "text": "For example, we discover a tree about an entity which has song as its type."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-5",
              "text": "In a KG context, the tree suggests that if we have an entity of type song in the KG, then we would expect to have the associated facts too."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-6-Sentence-1",
              "text": "In this paper, we present a system, SHACLEARNER, that mines shapes from KG data."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-2",
              "text": "For this purpose, we propose a predicate calculus formalism in which rules have one body atom and a chain of conjunctive atoms in the head with a specific variable binding pattern."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-3",
              "text": "Since these rules are an inverse version of open path rules, we call them inverse open path (IOP) rules."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-4",
              "text": "To learn IOP rules, we adapt an embedding-based open path rule learner."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-5",
              "text": "We define quality measures to express the validity of IOP rules in a KG."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-6",
              "text": "SHACLEARNER uses the mined IOP rules to subsequently discover more complex tree shapes."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-7",
              "text": "Each IOP rule or tree is a SHACL shape, in the sense that it can be syntactically rewritten in SHACL."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-8",
              "text": "Our mined shapes are augmented with a novel numerical confidence measure to express the strength of evidence in the KG for each shape."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-7-Sentence-1",
              "text": "In summary, the main contributions of this paper are: we introduce a new formalism called Inverse Open Path rules, that serves as a building block for more complex shapes such as trees, together with cardinality constraints and quality measurements; we extend the Open Path rule learning method to learn IOP rules annotated with cardinality constraints, while introducing unary predicates that can act as class or type constraints; and we propose a method to aggregate IOP rules to produce tree shapes."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-8-Sentence-1",
              "text": "This paper is organized as follows: after presenting some foundations in Section 2, we describe our SHACL learning method in Section 3, including the formalism of IOP rules, the embedding-based method that discovers IOP rules from a KG, and the method for aggregating IOP rules into trees."
            },
            {
              "iri": "Section-2-Paragraph-8-Sentence-2",
              "text": "In Section 4, we present related work."
            },
            {
              "iri": "Section-2-Paragraph-8-Sentence-3",
              "text": "We discuss results of an experimental evaluation in Section 5 before we conclude in Section 6."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-3",
      "subtitle": "Preliminaries: Closed-path rules",
      "paragraphs": [
        {
          "iri": "Section-3-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-1-Sentence-1",
              "text": "The presentation of closed path rules and open path rules in this section is adapted and extended."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-2-Sentence-1",
              "text": "An entity e is an identifier for an object such as a place or a person."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-2",
              "text": "A fact (also known as a link) is an RDF triple (e, P, e'), written here as P(e, e'), meaning that the subject entity e is related to an object entity e' via the binary predicate (also known as a property), P."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-3",
              "text": "In addition, we allow unary predicates of the form P(e), also equivalently written here as the binary fact P(e, e)."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-4",
              "text": "We model unary predicates as self-loops to have a unary predicate act as the label of a link in the graph, as shown in Fig. 1, just as for binary predicates."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-5",
              "text": "Unary predicates may, but are not limited to, represent class assertions expressed in an RDF triple as (e, rdf:type, P) where P is a class or a datatype."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-6",
              "text": "A knowledge graph (KG) is a pair K = (E, F), where E is a set of entities and F is a set of facts and all the entities occurring in F also occur in E."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-3-Sentence-1",
              "text": "2.1. Closed-path rules"
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-2",
              "text": "KG rule learning systems employ various rule languages to express rules."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-3",
              "text": "RLvLR and SCALEKB use so-called closed path (CP) rules."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-4",
              "text": "Each consists of a head at the front of the implication arrow and a body at the tail."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-5",
              "text": "We say the rule is about the predicate of the head."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-6",
              "text": "The rule forms a closed path, or single unbroken loop of links between the variables."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-7",
              "text": "It has the following general form."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-4-Sentence-1",
              "text": "Pt(x, y) <- P1(x, z1) ^ P2(z1, z2) ^ ... ^ Pn(zn-1, y). (1)"
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-5-Sentence-1",
              "text": "We interpret these kinds of rules with universal quantification of all variables at the outside, and so we can infer a fact that instantiates the head of the rule by finding an instantiation of the body of the rule in the KG."
            },
            {
              "iri": "Section-3-Paragraph-5-Sentence-2",
              "text": "For example, from the rule citizenOf(x, y) <- livesIn(x, z) ^ locatedIn(z, y) and the facts in the KG: livesIn(Mary, Canberra) and locatedIn(Canberra, Australia), we can infer and assert the new fact: citizenOf(Mary, Australia)."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-6-Sentence-1",
              "text": "Rules are considered more use if they generalise well, that is, they explain many facts."
            },
            {
              "iri": "Section-3-Paragraph-6-Sentence-2",
              "text": "To quantify this idea we recall measures support, head coverage and standard confidence that are used in some major approaches to rule learning."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-7-Sentence-1",
              "text": "Definition 1 (satisfies, support)."
            },
            {
              "iri": "Section-3-Paragraph-7-Sentence-2",
              "text": "Let r be a CP rule of the form (1)."
            },
            {
              "iri": "Section-3-Paragraph-7-Sentence-3",
              "text": "A pair of entities (e, e') satisfies the body of r, denoted body_r(e, e'), if there exist entities e1, ..., e(n-1) in the KG such that all of {P1(e, e1), P2(e1, e2), ..., Pn(e(n-1), e')} are facts in the KG."
            },
            {
              "iri": "Section-3-Paragraph-7-Sentence-4",
              "text": "Further (e, e') satisfies the head of r, denoted Pt(e, e'), if Pt(e, e') is a fact in the KG."
            },
            {
              "iri": "Section-3-Paragraph-7-Sentence-5",
              "text": "Then the support of r counts the rule instances for which the body and the head are both satisfied in the KG."
            },
            {
              "iri": "Section-3-Paragraph-7-Sentence-6",
              "text": "suppr = |{ (e, e') : body_r(e, e') and Pt(e, e') }|"
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-8-Sentence-1",
              "text": "Standard confidence (SC) and head coverage (HC) offer standardised measures for comparing rule quality."
            },
            {
              "iri": "Section-3-Paragraph-8-Sentence-2",
              "text": "SC describes how frequently the rule is true, i.e., of the number of entity pairs that satisfy the body in the KG, what proportion of the inferred head instances are satisfied?"
            },
            {
              "iri": "Section-3-Paragraph-8-Sentence-3",
              "text": "It is closely related to confidence widely used in association rule mining."
            },
            {
              "iri": "Section-3-Paragraph-8-Sentence-4",
              "text": "HC measures the explanatory power of the rule, i.e., what proportion of the facts satisfying the head of the rule could be inferred by satisfying the rule body?"
            },
            {
              "iri": "Section-3-Paragraph-8-Sentence-5",
              "text": "It is closely related to cover which is widely used for rule learning in inductive logic programming."
            },
            {
              "iri": "Section-3-Paragraph-8-Sentence-6",
              "text": "A non-recursive rule that has both 100% SC and HC is redundant with respect to the KG, and every KG fact that is an instance of the rule head is redundant with respect to the rule."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-9-Sentence-1",
              "text": "Definition 2 (standard confidence, head coverage)."
            },
            {
              "iri": "Section-3-Paragraph-9-Sentence-2",
              "text": "Let r, e, e', body_r be as given in Definition 1."
            },
            {
              "iri": "Section-3-Paragraph-9-Sentence-3",
              "text": "Then standard confidence is SC(r) = suppr / |{ (e, e') : body_r(e, e') }|"
            },
            {
              "iri": "Section-3-Paragraph-9-Sentence-4",
              "text": "and head coverage is HC(r) = suppr / |{ (e, e') : Pt(e, e') }|"
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-4",
      "subtitle": "Preliminaries: Open-path rules: Rules with open variables",
      "paragraphs": [
        {
          "iri": "Section-4-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-1-Sentence-1",
              "text": "Unlike earlier work in rule mining for KG completion, OPRL for active knowledge graph completion defines open path (OP) rules of the form:"
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-2",
              "text": "P1(z0, z1) \u2227 P2(z1, z2) \u2227 ... \u2227 Pn(zn\u22121, y) \u2192 \u2203x Pt(x, z0)."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-3",
              "text": "Here, Pi is a predicate in the KG, each of {x, zi, y} are entity variables, and all free variables are universally quantified at the outside."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-2-Sentence-1",
              "text": "We call a variable closed if it occurs in at least two distinct predicate terms, such as z0 here, and otherwise it is open."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-2",
              "text": "If all variables of a rule are closed then the rule is closed."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-3",
              "text": "An OP rule has two open variables, y and x."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-4",
              "text": "OP rules are used since they imply the existence of a fact, like spouse(x, y) \u2192 \u2203z child(x, z)."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-5",
              "text": "Unlike CP rules, OP rules do not necessarily form a loop, but a straightforward variable unification transforms an OP rule to a CP rule."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-6",
              "text": "Every entity-instantiation of a CP rule is also an entity-instantiation of the related OP rule, but not vice versa."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-3-Sentence-1",
              "text": "To assess the quality of OP rules, we use open path standard confidence (OPSC) and open path head coverage (OPHC), which are derived from the closed path forms."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-4-Sentence-1",
              "text": "Definition 3 (open path: OPsupp, OPSC, OPHC)."
            },
            {
              "iri": "Section-4-Paragraph-4-Sentence-2",
              "text": "Let r be an OP rule of the given form."
            },
            {
              "iri": "Section-4-Paragraph-4-Sentence-3",
              "text": "A pair of entities (e, e') satisfies the body of r, denoted as body_r(e, e'), if there exist entities e1, ..., en\u22121 in the KG such that P1(e, e1), P2(e1, e2), ..., Pn(en\u22121, e') are facts in the KG."
            },
            {
              "iri": "Section-4-Paragraph-4-Sentence-4",
              "text": "Also, (e', e) satisfies the head of r, denoted Pt(e', e), if Pt(e', e) is a fact in the KG."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-5-Sentence-1",
              "text": "The open path support, open path standard confidence, and open path head coverage of r are given respectively by:"
            },
            {
              "iri": "Section-4-Paragraph-5-Sentence-2",
              "text": "OPsupp(r) = |{e : \u2203e', e'' such that body_r(e, e') and Pt(e'', e)}|."
            },
            {
              "iri": "Section-4-Paragraph-5-Sentence-3",
              "text": "OPSC(r) = OPsupp(r) / |{e : \u2203e' such that body_r(e, e')}|."
            },
            {
              "iri": "Section-4-Paragraph-5-Sentence-4",
              "text": "OPHC(r) = OPsupp(r) / |{e : \u2203e' such that Pt(e', e)}|."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-5",
      "subtitle": "Preliminaries: SHACL shapes",
      "paragraphs": [
        {
          "iri": "Section-5-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-1-Sentence-1",
              "text": "A KG is a schema-free database and does not need to be augmented with schema information natively."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-2",
              "text": "However, many KGs are augmented with type information that can be used to understand and validate data and can also be very helpful for inference processes on the KG."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-3",
              "text": "In 2017 the Shapes Constraint Language (SHACL) was introduced as a W3C recommendation to define schema information for KGs stored as an RDF graph."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-4",
              "text": "SHACL defines constraints for graphs as shapes."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-5",
              "text": "KGs can then be validated against a set of shapes."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-2-Sentence-1",
              "text": "Shapes can serve two main purposes: validating the quality of a KG and characterising the frequent patterns in a KG."
            },
            {
              "iri": "Section-5-Paragraph-2-Sentence-2",
              "text": "In Fig. 2, we illustrate an example of a shape from Wikidata, where x and z_i are variables that are instantiated by entities."
            },
            {
              "iri": "Section-5-Paragraph-2-Sentence-3",
              "text": "Although the shape is originally expressed in ShEx, we translate it to SHACL in the following."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-3-Sentence-1",
              "text": "SHACL, together with SHACL advanced features is extensive."
            },
            {
              "iri": "Section-5-Paragraph-3-Sentence-2",
              "text": "Here we focus on the core of SHACL in which node shapes constrain a target predicate (e.g., the unary predicate human in Fig. 2), with property shapes expressing constraints over facts related to the target predicate."
            },
            {
              "iri": "Section-5-Paragraph-3-Sentence-3",
              "text": "We particularly focus on property shapes which act to constrain an argument of the target predicate."
            },
            {
              "iri": "Section-5-Paragraph-3-Sentence-4",
              "text": "In Fig. 2 the shape expresses that each entity x which satisfies human(x) should satisfy the following paths: (1) citizenOf(x, z1) ^ country(z1), (2) father(x, z2) ^ human(z2), and (3) nativeLanguage(x, z3) ^ language(z3)."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-4-Sentence-1",
              "text": "Various formalisms with corresponding shapes have been proposed to express diverse kinds of patterns exhibited in KGs, such as k-cliques, Closed rules (CR) (that include closed path rules), Functional Graph Dependency (FGD), and trees."
            },
            {
              "iri": "Section-5-Paragraph-4-Sentence-2",
              "text": "CRs are used for inferring new facts."
            },
            {
              "iri": "Section-5-Paragraph-4-Sentence-3",
              "text": "FGDs define constraints like the type of entities in the domain and range of predicates, or the number of entities to which an entity can be related by a specific predicate."
            },
            {
              "iri": "Section-5-Paragraph-4-Sentence-4",
              "text": "These constraints are deployed to make the KG consistent."
            },
            {
              "iri": "Section-5-Paragraph-4-Sentence-5",
              "text": "Regardless of differences between the formalisms, they share a key feature in their syntax."
            },
            {
              "iri": "Section-5-Paragraph-4-Sentence-6",
              "text": "The building block for expressing all of these shape constraints is a sequence of predicates."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-5-Sentence-1",
              "text": "We focus on such path shapes for our shape learning system."
            },
            {
              "iri": "Section-5-Paragraph-5-Sentence-2",
              "text": "A path is a sequence of predicates connected by closed intermediate variables but terminating with open variables at both ends."
            },
            {
              "iri": "Section-5-Paragraph-5-Sentence-3",
              "text": "Although shapes in the form of a path are less constrained than some more complex shapes, they are a more general template for more complex shapes like closed rules or trees, which are also paths (with further restrictions)."
            },
            {
              "iri": "Section-5-Paragraph-5-Sentence-4",
              "text": "We will define Inverse Open Path rules induced from paths that have a straightforward interpretation as shapes, and also propose a method to mine such rules from a KG."
            },
            {
              "iri": "Section-5-Paragraph-5-Sentence-5",
              "text": "To demonstrate the potential for these kinds of shapes to serve as building blocks for more complex trees, we then propose a method that builds trees out of mined rules, and briefly discuss the application of such trees to KG completion."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-6",
      "subtitle": "SHACL learning: Rules with open variables or uncertain shapes",
      "paragraphs": [
        {
          "iri": "Section-6-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-1-Sentence-1",
              "text": "We observe that the converse of OP rules, which we call inverse open path rules (IOP), correspond to SHACL shapes."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-2",
              "text": "For example, a shape can be derived from the following three IOP rules: human(x, x) -> citizenOf(x, z1) ^ country(z1, z1), human(x, x) -> father(x, z2) ^ human(z2, z2), and human(x, x) -> nativeLanguage(x, z3) ^ language(z3, z3)."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-2-Sentence-1",
              "text": "The general form of an IOP rule is given by Pt'(x, z0) -> exists(z1, ..., z(n-1), y) P1'(z0, z1) ^ P2'(z1, z2) ^ ... ^ Pn'(z(n-1), y). (3)"
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-2",
              "text": "Here each P'i is either a predicate in the KG or its reverse with subject and object bindings swapped, and free variables are universally quantified at the outside."
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-3",
              "text": "We often omit the quantifiers when writing IOP rules."
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-4",
              "text": "In an IOP rule, the body of the rule is Pt, and its head is the sequence of predicates P1 ^ P2 ^ ... ^ Pn."
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-5",
              "text": "Hence we instantiate the atomic body to predict an instance of the head."
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-6",
              "text": "IOP rules are not Horn."
            },
            {
              "iri": "Section-6-Paragraph-2-Sentence-7",
              "text": "The pattern of existential variables in the head and universal variables in the body has been investigated in the literature as existential rules."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-3-Sentence-1",
              "text": "To assess the quality of IOP rules, we follow a style of quality measures similar to those used for OP rules."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-4-Sentence-1",
              "text": "Definition 4 (inverse open path: IOPsupp, IOPSC, IOPHC)."
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-2",
              "text": "Let r be an IOP rule of the form (3)."
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-3",
              "text": "Then a pair of entities (e, e') satisfies the head of r, denoted head_r(e, e'), if there exist entities e1, ..., e(n-1) in the KG such that P1(e, e1), P2(e1, e2), ..., Pn(e(n-1), e') are facts in the KG."
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-4",
              "text": "A pair of entities (e'', e) satisfies the body of r, denoted Pt(e'', e), if Pt(e'', e) is a fact in the KG."
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-5",
              "text": "The inverse open path support, inverse open path standard confidence, and inverse open path head coverage of r are given respectively by"
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-6",
              "text": "IOPsupp(r) = |{ (e', e'') : there exists e', e'' such that head_r(e, e') and Pt(e'', e) }|"
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-7",
              "text": "IOPSC(r) = IOPsupp(r) / |{ e' : Pt(e', e) }|"
            },
            {
              "iri": "Section-6-Paragraph-4-Sentence-8",
              "text": "IOPHC(r) = IOPsupp(r) / |{ e : there exists e' such that head_r(e, e') }|"
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-5-Sentence-1",
              "text": "Notably, because any instantiated open path in a KG includes both an OP and IOP rule, the support for an IOP rule is the same as the corresponding OPSC."
            },
            {
              "iri": "Section-6-Paragraph-5-Sentence-2",
              "text": "This close relationship between OP and IOP rules helps us to mine both OP and IOP rules in one process."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-6-Sentence-1",
              "text": "We show the relationship between an OP rule and its converse IOP version in the following example."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-2",
              "text": "Consider the OP rule P1(x, z0) <- P2(z0, z1) ^ P3(z1, z2)."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-3",
              "text": "Assume we have three entities (e3, e4, e5) which can instantiate z0 and satisfy both P1(x, z0) and P2(z0, z1) ^ P3(z1, z2)."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-4",
              "text": "Assume the number of entities that can instantiate z0 to satisfy the head part is 5 ({e1, e2, e3, e4, e5}) and the number of entities that can instantiate z0 to satisfy the body part is 7 ({e2, e4, e5, e6, e7, e8, e9})."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-5",
              "text": "Hence, for this rule OPsupp = 3, OPSC = 3/7, and OPHC = 3/5."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-6",
              "text": "For the IOP version of the same rule, P1(x, z0) -> P2(z0, z1) ^ P3(z1, z2), the same entities instantiate z0, but now the body and head predicates are swapped."
            },
            {
              "iri": "Section-6-Paragraph-6-Sentence-7",
              "text": "Hence, we have IOPsupp = 3, IOPSC = 3/5, and IOPHC = 3/7."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-7-Sentence-1",
              "text": "In many cases, we need rules to express not only the necessity of a chain of facts (the facts in the head of the IOP rule), but also the number of different chains which should exist."
            },
            {
              "iri": "Section-6-Paragraph-7-Sentence-2",
              "text": "For example, we may need a rule to express that each human has at least two parents."
            },
            {
              "iri": "Section-6-Paragraph-7-Sentence-3",
              "text": "Thus, we introduce IOP rules annotated with a cardinality Car, giving the following form."
            },
            {
              "iri": "Section-6-Paragraph-7-Sentence-4",
              "text": "IOPSC, IOPHC, Car : Pt'(x, z0) -> P1'(z0, z1) ^ P2'(z1, z2) ^ ... ^ Pn'(z(n-1), y). (4)"
            },
            {
              "iri": "Section-6-Paragraph-7-Sentence-5",
              "text": "Here IOPSC and IOPHC belong to [0, 1] and denote the qualities of the rule, while Car is an integer >= 1."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-8-Sentence-1",
              "text": "Definition 5 (Cardinality of an IOP rule, Car)."
            },
            {
              "iri": "Section-6-Paragraph-8-Sentence-2",
              "text": "Let r be an annotated IOP rule of the form (4), and let Car(r) be the cardinality annotation for r."
            },
            {
              "iri": "Section-6-Paragraph-8-Sentence-3",
              "text": "Then r satisfies Car(r) if and only if, for each entity e such that Pt(e', e) holds, the number of distinct instantiations of head_r(e, e') is at least Car(r)."
            },
            {
              "iri": "Section-6-Paragraph-8-Sentence-4",
              "text": "In other words, the cardinality expresses a lower bound on the number of head paths that are satisfied in the KG for every instantiation linking the body to the head."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-9-Sentence-1",
              "text": "Rules with the same head and the same body may have different cardinalities."
            },
            {
              "iri": "Section-6-Paragraph-9-Sentence-2",
              "text": "While a rule might have a certain cardinality c1, lower-cardinality versions of that rule may have the same or higher IOPSC values, since a larger required cardinality places a stricter lower bound on the number of instances of the head in the KB."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-10",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-10-Sentence-1",
              "text": "Lemma 1 (IOPSC is non-increasing with length)."
            },
            {
              "iri": "Section-6-Paragraph-10-Sentence-2",
              "text": "Let r be an IOP rule of the form (3) with n >= 2, and let r' be the same rule shortened by removing the last head predicate."
            },
            {
              "iri": "Section-6-Paragraph-10-Sentence-3",
              "text": "Then IOPSC(r) <= IOPSC(r')."
            },
            {
              "iri": "Section-6-Paragraph-10-Sentence-4",
              "text": "This result follows because the denominator of IOPSC(r) is not affected by the head of the rule, yet the numerator can only decrease when additional head predicates are included."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-11",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-11-Sentence-1",
              "text": "This lemma is useful for rule learning, because it shows that if we discard a rule due to its low IOPSC, we do not need to check versions of the rule extended with additional head atoms, since their IOPSC values would be at most as low."
            }
          ]
        },
        {
          "iri": "Section-6-Paragraph-12",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-12-Sentence-1",
              "text": "Algorithm 1: SHACLearner"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-2",
              "text": "Input: a KG K, a target predicate Pt"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-3",
              "text": "Parameters: max rule length l, max rule cardinality MCar, MinIOPSC, MinIOPHC, and MinTreeSC"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-4",
              "text": "Output: a set of IOP rules R and Tree"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-5",
              "text": "1: K' := Sampling(K, Pt)"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-6",
              "text": "2: (P, A) := Embeddings(K')"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-7",
              "text": "3: R' := empty"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-8",
              "text": "4: for k from 2 to l do"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-9",
              "text": "   Add PathFinding(K', Pt, P, A, k) to R'"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-10",
              "text": "end for"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-11",
              "text": "7: R := Ev(R', K, MCar, MinIOPSC, MinIOPHC)"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-12",
              "text": "8: Tree := GreedySearch(R, MinTreeSC)"
            },
            {
              "iri": "Section-6-Paragraph-12-Sentence-13",
              "text": "9: return Tree and R"
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-7",
      "subtitle": "SHACL learning: IOP learning through representation learning",
      "paragraphs": [
        {
          "iri": "Section-7-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-7-Paragraph-1-Sentence-1",
              "text": "To mine IOP rules, we start with the open path rule learner OPRL and adapt its embedding-based OP rule learning to learn annotated IOP rules."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-2",
              "text": "We call this new IOP rule learner SHACLEARNER, shown in Algorithm 1."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-3",
              "text": "SHACLEARNER uses a sampling method Sampling, shown in Algorithm 2, to prune the entities and predicates that are less relevant to the target predicate to obtain a sampled knowledge graph."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-4",
              "text": "The sample is fed to an embedding learner, RESCAL, in Embeddings."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-5",
              "text": "Then in PathFinding, SHACLEARNER uses the computed embedding representations of predicates and entities in heuristic functions that inform the generation of IOP rules bounded by a maximum length."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-6",
              "text": "Then, potential IOP rules are evaluated, annotated, and filtered in Ev to produce annotated IOP rules."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-7",
              "text": "Eventually, a tree is discovered for each argument of each target predicate by aggregating mined IOP rules in GreedySearch."
            }
          ]
        },
        {
          "iri": "Section-7-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-7-Paragraph-2-Sentence-1",
              "text": "While the overall algorithm structure of SHACLEARNER is similar to OPRL, as is the embedding-based scoring function, the following elements are novel in SHACLEARNER:"
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-2",
              "text": "OPRL cannot handle unary predicates while SHACLEARNER admits unary predicates both in the head and the body of IOP rules."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-3",
              "text": "SHACLEARNER can discover and evaluate IOP rules with minimum cardinality constraints in the head of the IOP rule, while OPRL is effectively limited to learning the special case of minimum cardinality 1 for all rules."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-4",
              "text": "For this reason, the evaluation method of SHACLEARNER, Ev, differs from the OPRL evaluation module."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-5",
              "text": "The aggregation module that produces trees out of learnt IOP rules, ready for translation to SHACL, is novel in SHACLEARNER."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-8",
      "subtitle": "SHACL learning: Sampling",
      "paragraphs": [
        {
          "iri": "Section-8-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-8-Paragraph-1-Sentence-1",
              "text": "In more detail, the Sampling() method in line 1 of Algorithm 1 computes a fragment of the KG K', consisting of a bounded number of entities that are related to the target predicate Pt."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-2",
              "text": "This sampling is essential since embedding learners (e.g., HOLE and RESCAL) cannot handle massive KGs with millions of entities (e.g., YAGO2)."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-3",
              "text": "The sampling method, first introduced, is shown in Algorithm 2."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-4",
              "text": "Since we search for IOP rules with up to l atoms (including the specific body target predicate, Pt), the entity set Esample and corresponding fact set K' contains the information needed for learning such rules."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-5",
              "text": "Predicates less relevant to the target predicate are pruned in the sampling process and no facts about those predicates remain in K'."
            }
          ]
        },
        {
          "iri": "Section-8-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-8-Paragraph-2-Sentence-1",
              "text": "This simple approach reduces the size of the problem significantly, as discussed."
            },
            {
              "iri": "Section-8-Paragraph-2-Sentence-2",
              "text": "For a KG K with entities E and facts F, the set of sampled entities for a target predicate will be of size 2l|F||E| in the worst case."
            },
            {
              "iri": "Section-8-Paragraph-2-Sentence-3",
              "text": "Hence, the complexity of the sampling algorithm is O(|K|) where |K| = |E||F|."
            },
            {
              "iri": "Section-8-Paragraph-2-Sentence-4",
              "text": "In the worst case, the sampled KG is the same as the original KG, but real-world KGs are sparse with only a very small proportion of entities associated with any predicate within distance l."
            },
            {
              "iri": "Section-8-Paragraph-2-Sentence-5",
              "text": "In practice, the sampled KG is far smaller than the original KG."
            }
          ]
        },
        {
          "iri": "Section-8-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-8-Paragraph-3-Sentence-1",
              "text": "Algorithm 2: Sampling"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-2",
              "text": "Input: a KG K, a target predicate Pt"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-3",
              "text": "Parameters: max rule length l"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-4",
              "text": "Output: K' a subgraph of the input graph"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-5",
              "text": "1: E1 = { e | there exists e': Pt(e, e') or Pt(e', e) }"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-6",
              "text": "2: for 2 <= k <= l do"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-7",
              "text": "3:   Ek = { e | there exists e': e' in E(k-1) and (Pi(e, e') or Pi(e', e)) }"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-8",
              "text": "4: end for"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-9",
              "text": "5: Esample = union (from i=1 to l) of Ei"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-10",
              "text": "6: K' = { Pi(e, e') | (e in Esample) and (e' in Esample) }"
            },
            {
              "iri": "Section-8-Paragraph-3-Sentence-11",
              "text": "7: return K'"
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-9",
      "subtitle": "SHACL learning: Embeddings",
      "paragraphs": [
        {
          "iri": "Section-9-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-1-Sentence-1",
              "text": "After sampling, in line 2 Embeddings(), we compute predicate embeddings as well as subject and object argument embeddings for all predicates in the sampled K'."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-2",
              "text": "The embedding is obtained from RESCAL as it can provide an extensive representation of predicates and entities as shown in previous heuristic rule learners."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-3",
              "text": "Briefly, we use RESCAL to embed each entity e_i to a vector E_i in R^d and each predicate P_k to a matrix P_k in R^(d x d), where R is the set of real numbers and d is an integer parameter of RESCAL."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-4",
              "text": "For each given fact P0(e1, e2), the following scoring function is computed: f(e1, P0, e2) = E1^T . P0 . E2."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-5",
              "text": "The scoring function indicates the plausibility of the fact that e1 has relation P0 with e2."
            }
          ]
        },
        {
          "iri": "Section-9-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-2-Sentence-1",
              "text": "In Embeddings() we additionally compute argument embeddings."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-2",
              "text": "To compute the subject (respectively object) argument embeddings of a predicate P_k, we aggregate the embeddings of entities that occur as the subject (respectively object) of P_k in the KG."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-3",
              "text": "Hence, for each predicate P_k we have two vectors, P1_k and P2_k, that represent the subject argument and object argument of P_k respectively."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-10",
      "subtitle": "SHACL learning: Generating and pruning rules",
      "paragraphs": [
        {
          "iri": "Section-10-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-1-Sentence-1",
              "text": "After that, in line 3 to line 7 of Algorithm 1, PathFinding() produces candidate IOP rules based on the embedding representation of the predicates involved in each rule."
            },
            {
              "iri": "Section-10-Paragraph-1-Sentence-2",
              "text": "The candidate rules are pruned by the scoring function heuristic for OP rules."
            },
            {
              "iri": "Section-10-Paragraph-1-Sentence-3",
              "text": "Due to the close relationship between OP and IOP rules, a high-scoring candidate OP rule suggests both a good OP rule and a good IOP rule."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-2-Sentence-1",
              "text": "An IOP rule Pt(x, y) -> P1(y, z) ^ P2(z, t) acts to connect entities satisfying the subject argument of the body predicate, Pt, to entities forming the object argument of the last predicate, P2, along a path of entities that satisfy a chain of predicates in the rule."
            },
            {
              "iri": "Section-10-Paragraph-2-Sentence-2",
              "text": "There is a relationship between the logical statement of the rule and certain properties in the embedding space."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-3-Sentence-1",
              "text": "1. The predicate arguments that have the same variable in the rule should have similar argument embeddings."
            },
            {
              "iri": "Section-10-Paragraph-3-Sentence-2",
              "text": "For example, we should have the following similarities: P2_t ~ P1_1 and P2_1 ~ P2_1."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-4-Sentence-1",
              "text": "2. The whole path forms a composite predicate, like P*(x, t) = Pt(x, y) ^ P1(y, z) ^ P2(z, t)."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-2",
              "text": "We compute the embedding representation of the composite predicate based on its components: P*(x, t) = Pt(x, y) . P1(y, z) . P2(z, t)."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-3",
              "text": "Now we could check the plausibility of P*(x, t) for any pair of entities using the scoring function introduced earlier."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-4",
              "text": "However, since we are interested in the existence of an entity-free rule, the following similarity will hold: 1 approx P1_t . Pt . P1 . P2 . P2_2."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-5-Sentence-1",
              "text": "Based on the above two properties, two scoring functions are defined to help heuristically mine the space of all possible IOP rules, producing a reduced set of candidate IOP rules."
            },
            {
              "iri": "Section-10-Paragraph-5-Sentence-2",
              "text": "The ultimate evaluation of an IOP rule will be done in the next step as described below."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-11",
      "subtitle": "SHACL learning: Efficient computation of quality measures",
      "paragraphs": [
        {
          "iri": "Section-11-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-1-Sentence-1",
              "text": "Now we focus our attention on the efficient matrix-computation of the quality measures that are novel for SHACLearner."
            },
            {
              "iri": "Section-11-Paragraph-1-Sentence-2",
              "text": "Ev() in Algorithm 1 first evaluates candidate rules on the small sampled KG, and selects only the rules with IOPsupp(r) >= 1."
            },
            {
              "iri": "Section-11-Paragraph-1-Sentence-3",
              "text": "They may still include a large number of redundant and low quality rules and so are further downselected based on their IOPSC and IOPHC calculated over the full KG."
            },
            {
              "iri": "Section-11-Paragraph-1-Sentence-4",
              "text": "We show next how to efficiently compute these measures over massive KGs using an adjacency matrix representation of the KG."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-2-Sentence-1",
              "text": "Let K = (E, F) with E = {e1, ..., en} be the set of all entities and P = {P1, ..., Pm} be the set of all predicates in F."
            },
            {
              "iri": "Section-11-Paragraph-2-Sentence-2",
              "text": "We represent K as a set of square n x n adjacency matrices by defining the function A."
            },
            {
              "iri": "Section-11-Paragraph-2-Sentence-3",
              "text": "Specifically, the [i, j]-th element A(Pk)[i, j] is 1 if the fact Pk(ei, ej) is in F, and 0 otherwise."
            },
            {
              "iri": "Section-11-Paragraph-2-Sentence-4",
              "text": "Thus, A(Pk) is a matrix of binary values, and the set {A(Pk) | k in 1..m} represents K."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-3-Sentence-1",
              "text": "We illustrate the computation of IOPSC and IOPHC through an example."
            },
            {
              "iri": "Section-11-Paragraph-3-Sentence-2",
              "text": "Consider the IOP rule r: Pt(x, z0) -> P1(z0, x) ^ P2(z1, y)."
            },
            {
              "iri": "Section-11-Paragraph-3-Sentence-3",
              "text": "Let E = {e1, e2, e3} and F = {P1(e1, e2), P1(e2, e1), P1(e2, e3), P1(e3, e1), P2(e1, e2), P2(e3, e2), P2(e3, e3), P1(e1, e3), Pt(e3, e2), Pt(e3, e3)}."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-4-Sentence-1",
              "text": "The adjacency matrices for the predicates P1, P2, and Pt are:"
            },
            {
              "iri": "Section-11-Paragraph-4-Sentence-2",
              "text": "A(P1) = [0 1 0; 1 0 1; 0 0 0],"
            },
            {
              "iri": "Section-11-Paragraph-4-Sentence-3",
              "text": "A(P2) = [0 1 0; 0 0 0; 0 1 1],"
            },
            {
              "iri": "Section-11-Paragraph-4-Sentence-4",
              "text": "A(Pt) = [0 0 0; 0 0 0; 0 1 1]."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-5-Sentence-1",
              "text": "For IOPSC and IOPHC (Definition 4), we need to calculate:"
            },
            {
              "iri": "Section-11-Paragraph-5-Sentence-2",
              "text": "1) The number of entities that satisfy the body of the rule, i.e. the count of e' such that there exists e'' with Pt(e'', e')."
            },
            {
              "iri": "Section-11-Paragraph-5-Sentence-3",
              "text": "2) The number of entities that satisfy the head of the rule, i.e. the count of e' such that there exists e'' with head_r(e'', e')."
            },
            {
              "iri": "Section-11-Paragraph-5-Sentence-4",
              "text": "3) The number of entities that join the head of the rule to its body, i.e. the count of e' for which there exist e'', e''' such that head_r(e'', e') and Pt(e''', e')."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-6-Sentence-1",
              "text": "For (1), we can read the pairs (e', e) directly from the matrix A(Pt)."
            },
            {
              "iri": "Section-11-Paragraph-6-Sentence-2",
              "text": "To find distinct e's we sum each column and transpose to obtain the vector V^2(Pt)."
            },
            {
              "iri": "Section-11-Paragraph-6-Sentence-3",
              "text": "Each non-zero element indicates a satisfying e, and the number of distinct e's is the count of non-zero elements."
            },
            {
              "iri": "Section-11-Paragraph-6-Sentence-4",
              "text": "In the example, the only non-zero element in A(Pt) is A(Pt)[1, 3], and after summing columns and transposing we have V^2(Pt) = [0 1 2]^T, so {e2, e3} satisfies the head with count 2."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-7-Sentence-1",
              "text": "For (2), the pairs (e, e') satisfying the head are connected by the path P1, P2, ..., Pm."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-2",
              "text": "They can be obtained from the matrix product A(P1)*A(P2)*...*A(Pm), taking elements with a value >= Car for rules with cardinality Car."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-3",
              "text": "We find distinct e's by summing each row of this product to obtain the vector V^1(A(P1)*A(P2)*...*A(Pm))."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-4",
              "text": "Each element with value >= Car indicates a satisfying e, and the count of distinct e's is the count of non-zero elements in that vector."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-5",
              "text": "In the example, A(P1)*A(P2) = [[0,0,0],[0,2,1],[1,0,0]], and V^1(P1, P2) = [0,3,1]."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-6",
              "text": "Thus, for Car=1 we have satisfying entities e2 and e3 with a count of 2."
            },
            {
              "iri": "Section-11-Paragraph-7-Sentence-7",
              "text": "For Car=2, only e2 satisfies the rule, so the count is 1."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-8-Sentence-1",
              "text": "Computing (3) is then straightforward."
            },
            {
              "iri": "Section-11-Paragraph-8-Sentence-2",
              "text": "The row index of non-zero elements of V^2(Pt) indicates entities that satisfy the second argument of the body, and the row index of elements with value >= Car of V^1(A(P1)*A(P2)*...*A(Pm)) indicates entities that satisfy the first argument of the head."
            },
            {
              "iri": "Section-11-Paragraph-8-Sentence-3",
              "text": "We find the entities satisfying both conditions by pairwise multiplication of those vectors."
            },
            {
              "iri": "Section-11-Paragraph-8-Sentence-4",
              "text": "For the example, when Car=1, the set of such entities is {e2, e3} with count 2."
            },
            {
              "iri": "Section-11-Paragraph-8-Sentence-5",
              "text": "When Car=2, the set is {e2} with count 1."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-9-Sentence-1",
              "text": "Hence, we could have three versions of r, namely r1, r2, and r3, with three different Car values 1, 2, and 3 respectively."
            },
            {
              "iri": "Section-11-Paragraph-9-Sentence-2",
              "text": "For Car=1, IOPsupp(r1) = |{e2, e3}| = 2."
            },
            {
              "iri": "Section-11-Paragraph-9-Sentence-3",
              "text": "From Definition 4, IOPHC(r1) = 2/2 and IOPSC(r1) = 2/2."
            },
            {
              "iri": "Section-11-Paragraph-9-Sentence-4",
              "text": "For Car=2, IOPsupp(r2) = |{e2}| = 1, IOPHC(r2) = 1/1, and IOPSC(r2) = 1/2."
            },
            {
              "iri": "Section-11-Paragraph-9-Sentence-5",
              "text": "In this case, for Car=3 we get the same values as for Car=2."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-12",
      "subtitle": "SHACL learning: From IOP rules to tree shapes",
      "paragraphs": [
        {
          "iri": "Section-12-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-1-Sentence-1",
              "text": "Now in line 8 of Algorithm 1 we turn to deriving SHACL trees, as illustrated in Fig. 3, from annotated IOP rules."
            },
            {
              "iri": "Section-12-Paragraph-1-Sentence-2",
              "text": "This procedure is used in SHACLearner."
            },
            {
              "iri": "Section-12-Paragraph-1-Sentence-3",
              "text": "We use a greedy search to aggregate the IOP rules for the subject argument and the object argument of each target predicate."
            },
            {
              "iri": "Section-12-Paragraph-1-Sentence-4",
              "text": "For example, the shape of Fig. 2 has the following tree: human(x, x) -> citizenOf(x, z1) ^ country(z1, z1) ^ father(x, z2) ^ human(z2, z2) ^ nativeLanguage(x, z3) ^ language(z3, z3)."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-2-Sentence-1",
              "text": "The general form of a tree is given by P't(x, z0) -> exists(z*s, y*s) P'1( z0, z1 ) ^ P'1_1(z1, z2 ) ^ ... ^ P'n_1(z(n-1), y1 ) ^ P'1_2( z0, z1 ) ^ P'2_2(z1, z2 ) ^ ... ^ P'm_2(z(m-1), y2 ) ... ^ P'1^q(z0, z1^q ) ^ P'2^q(z1^q, z2^q ) ^ ... ^ P't^q(z(q-1)^q, y^q)."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-2",
              "text": "Here each P'i is either a predicate in the KG or its reverse with the subject and object bindings swapped."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-3",
              "text": "Free variables are universally quantified at the outside."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-4",
              "text": "In a tree we say the body of the shape is Pt and its head is the sequence of paths or branches, Path1 ^ Path2 ^ ... ^ Pathq."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-5",
              "text": "Hence we instantiate the atomic body to predict an instance of the head."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-6",
              "text": "All head branches and the body join in one shared variable, z0."
            },
            {
              "iri": "Section-12-Paragraph-2-Sentence-7",
              "text": "To assess the quality of a tree we follow the quality measures for IOP rules."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-3-Sentence-1",
              "text": "Definition 6 (Tree: Treesupp, TreeSC)."
            },
            {
              "iri": "Section-12-Paragraph-3-Sentence-2",
              "text": "Let r be a tree of the above form."
            },
            {
              "iri": "Section-12-Paragraph-3-Sentence-3",
              "text": "Then a set of pairs of entities (e, e1), ..., (e, eq) satisfies the head of r, denoted headr(e), if there exist sequences of entities e1, ..., e(n-1), e1', ..., e(m-1), e1^q, e(q-1)' in the KG such that P1(e, e1), P2(e1, e2), ..., Pt(e(q-1)', eq) are facts in the KG."
            },
            {
              "iri": "Section-12-Paragraph-3-Sentence-4",
              "text": "A pair of entities (e'', e) satisfies the body of r, denoted Pt(e'', e), if Pt(e'', e) is a fact in the KG."
            },
            {
              "iri": "Section-12-Paragraph-3-Sentence-5",
              "text": "The tree support and tree standard confidence of r are given respectively by Treesupp(r) = |{ e : there exists e' such that headr(e) and Pt(e', e) }| and TreeSC(r) = Treesupp(r) / |{ e' : Pt(e'', e') }|."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-4-Sentence-1",
              "text": "To learn each tree we employ a greedy search, GreedySearch, in line 8 of Algorithm 1."
            },
            {
              "iri": "Section-12-Paragraph-4-Sentence-2",
              "text": "To do so, we sort all rules that bind the subject argument (for the left-hand tree in Fig. 3) in a non-increasing order with respect to IOPSC."
            },
            {
              "iri": "Section-12-Paragraph-4-Sentence-3",
              "text": "Then we iteratively try to add the first rule in the list to the tree and compute the TreeSC."
            },
            {
              "iri": "Section-12-Paragraph-4-Sentence-4",
              "text": "If the TreeSC drops below the defined threshold (TreeSCMIN) we dismiss the rule; otherwise we add it to the tree."
            },
            {
              "iri": "Section-12-Paragraph-4-Sentence-5",
              "text": "For the right-hand tree we do the same with the rules that bind the object argument of the target predicate."
            },
            {
              "iri": "Section-12-Paragraph-4-Sentence-6",
              "text": "Since a conjunction of IOP rules forms a tree, TreeSC is bounded above by the minimum IOPSC of its constituent IOP rules."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-5-Sentence-1",
              "text": "These uncertain shapes can be presented as standard SHACL shapes by ignoring those that fail to satisfy minimum quality thresholds and deleting the quality annotations."
            },
            {
              "iri": "Section-12-Paragraph-5-Sentence-2",
              "text": "Aside from the cardinality, the tree may be straightforwardly interpreted as a set of SHACL shapes by reading off every path from the target predicate terminating at a node in the tree."
            },
            {
              "iri": "Section-12-Paragraph-5-Sentence-3",
              "text": "The body predicate is declared as sh:nodeShape and the path of head predicates as nested sh:path declarations within a sh:property declaration."
            },
            {
              "iri": "Section-12-Paragraph-5-Sentence-4",
              "text": "Cardinality of a path is read from the annotation of the branch at the terminating node, and declared by sh:minCount within the property declaration."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-6-Sentence-1",
              "text": "SHACLearner supports all the SHACL Core features (node and property shapes)."
            },
            {
              "iri": "Section-12-Paragraph-6-Sentence-2",
              "text": "The limitations of SHACLearner with respect to SHACL Core are: (1) it treats all properties, both object and datatype properties, as plain predicates so there is no distinction; (2) it does not perform any kind of data type validation; and (3) of cardinality expressions, only min cardinality is handled."
            },
            {
              "iri": "Section-12-Paragraph-6-Sentence-3",
              "text": "SHACLearner does not mine SPARQL-like constraints (SHACL-SPARQL)."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-7-Sentence-1",
              "text": "3.7.1. Tree shapes are useful for human interaction"
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-8-Sentence-1",
              "text": "Shapes offer KG documentation as readable patterns and also provide a way to validate a KG."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-2",
              "text": "Our novel tree shapes can additionally be used for KG-completion."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-3",
              "text": "While there are several methods proposed to complete KGs automatically by predicting missing facts, these methods traverse the KG in a breadth-first manner."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-4",
              "text": "Our proposed tree shapes instead provide an opportunity to work sequentially along a path of dependent questions such as birthPlace(Trump, ?) followed by capitalOf(?, ?)."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-5",
              "text": "The latter question cannot even be asked until we have an answer for the former question, and the existence of an answer to the former gives us the confidence to proceed to the next question along the path."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-6",
              "text": "This completion strategy is depth-first as it works through a shape tree."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-7",
              "text": "Importantly, when we want to ask such completion questions of a human, this depth-first questioning strategy will reduce the cognitive load due to the contextual connections between successive questions."
            },
            {
              "iri": "Section-12-Paragraph-8-Sentence-8",
              "text": "This strategy for human-KG-completion is applied in a smart KG editor using trees that can be generated by SHACLearner."
            }
          ]
        },
        {
          "iri": "Section-12-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-12-Paragraph-9-Sentence-1",
              "text": "Tree shapes can also help a human expert extract a more intuitive concise sub-tree out of a deeper, more complex tree when desired for human interpretability."
            },
            {
              "iri": "Section-12-Paragraph-9-Sentence-2",
              "text": "If a tree with confidence TreeSCorig is pruned either by removing a branch or by removing an entire path of shape predicates, it remains a valid tree with a new TreeSCnew, with the property that TreeSCnew >= TreeSCorig."
            },
            {
              "iri": "Section-12-Paragraph-9-Sentence-3",
              "text": "Hence, by pruning a tree we obtain a simpler tree with higher confidence in the KG."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-13",
      "subtitle": "Related work",
      "paragraphs": [
        {
          "iri": "Section-13-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-1-Sentence-1",
              "text": "There are some exploratory attempts to address learning SHACL shapes from KGs."
            },
            {
              "iri": "Section-13-Paragraph-1-Sentence-2",
              "text": "They are procedural methods without logical foundations and are not shown to be scalable to handle real-world KGs."
            },
            {
              "iri": "Section-13-Paragraph-1-Sentence-3",
              "text": "They work with a small amount of data and the representation formalism they use for their output is difficult to compare with the IOP rules which we use in this paper."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-2-Sentence-1",
              "text": "One approach carries out the task in a semi-automatic manner: it provides a sample of data to an off-the-shelf graph structure learner and provides the output in an interactive interface for a human user to create SHACL shapes."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-3-Sentence-1",
              "text": "Another work provides an interactive framework to define SHACL shapes with different complexities, including nested patterns that are similar to the trees that we use."
            },
            {
              "iri": "Section-13-Paragraph-3-Sentence-2",
              "text": "A formalisation of the approach is given, but there are no shape quality measures that are essential for large scale shape mining."
            },
            {
              "iri": "Section-13-Paragraph-3-Sentence-3",
              "text": "Because the paper does not provide a system that discovers patterns from massive KGs, we cannot deploy their method for comparison purposes."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-4-Sentence-1",
              "text": "There are some works that use existing ontologies for KGs to generate SHACL shapes."
            },
            {
              "iri": "Section-13-Paragraph-4-Sentence-2",
              "text": "One work uses two different kinds of knowledge to automatically generate SHACL shapes: ontology constraint patterns as well as input ontologies."
            },
            {
              "iri": "Section-13-Paragraph-4-Sentence-3",
              "text": "In our work, we use the KG itself to discover the shapes, without relying on external modelling artefacts."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-5-Sentence-1",
              "text": "From an application point of view, there are papers which investigate the application of SHACL shapes to the validation of RDF databases, but these do not contribute to the discovery of shapes."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-6-Sentence-1",
              "text": "One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs."
            },
            {
              "iri": "Section-13-Paragraph-6-Sentence-2",
              "text": "When a set of rules and shapes are provided, a method is proposed to detect which shapes could be violated by applying a rule."
            }
          ]
        },
        {
          "iri": "Section-13-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-13-Paragraph-7-Sentence-1",
              "text": "There are some attempts to provide logical foundations for the semantics of the SHACL language, including one that presents the semantics of recursive SHACL shapes."
            },
            {
              "iri": "Section-13-Paragraph-7-Sentence-2",
              "text": "By contrast, in our work we approach SHACL semantics in the reverse direction."
            },
            {
              "iri": "Section-13-Paragraph-7-Sentence-3",
              "text": "We start with logical formalisms with both well-defined semantics and motivating use cases to derive shapes that can be trivially expressed in a fragment of SHACL."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-14",
      "subtitle": "Experiments: Transforming KGs with type predicates for experiments",
      "paragraphs": [
        {
          "iri": "Section-14-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-1-Sentence-1",
              "text": "We have implemented our SHACLEARNER based on Algorithm 1 and conducted experiments to assess it."
            },
            {
              "iri": "Section-14-Paragraph-1-Sentence-2",
              "text": "Our experiments are designed to prove the effectiveness of our SHACLEARNER at capturing shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs (KGs)."
            },
            {
              "iri": "Section-14-Paragraph-1-Sentence-3",
              "text": "Since our proposed system is the first method to learn shapes from massive KGs automatically, we have no benchmark with which to compare."
            },
            {
              "iri": "Section-14-Paragraph-1-Sentence-4",
              "text": "However, the performance of our system shows that it can handle the task satisfactorily and can be applied in practice."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-2-Sentence-1",
              "text": "We demonstrate that SHACLEARNER is scalable so it can handle real-world massive KGs including DBpedia with over 11 million facts."
            },
            {
              "iri": "Section-14-Paragraph-2-Sentence-2",
              "text": "SHACLEARNER can learn several shapes each for various target predicates."
            },
            {
              "iri": "Section-14-Paragraph-2-Sentence-3",
              "text": "SHACLEARNER can discover diverse shapes with respect to the quality measurements of IOPSC and IOPHC."
            },
            {
              "iri": "Section-14-Paragraph-2-Sentence-4",
              "text": "SHACLEARNER discovers shapes of varying complexity and diversity with respect to length and cardinality."
            },
            {
              "iri": "Section-14-Paragraph-2-Sentence-5",
              "text": "SHACLEARNER discovers every high-quality rule (with IOPSC greater than or equal to 0.9) for a small complete KG, by comparison with an ideal learner."
            },
            {
              "iri": "Section-14-Paragraph-2-Sentence-6",
              "text": "SHACLEARNER discovers more complex shapes (trees) by aggregating learned IOP rules efficiently."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-3-Sentence-1",
              "text": "Our four benchmark KGs are described in Table 1."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-2",
              "text": "Three benchmarks, namely YAGO2s, Wikidata, and DBpedia, are common KGs and have been used in rule learning experiments previously."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-3",
              "text": "The fourth is a small synthetic KG, Poker, for analyzing the completeness of our algorithm."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-4",
              "text": "The Poker KG was adapted from the classic version to be a rich and correct KG for evaluation experiments."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-5",
              "text": "Each poker hand comprises five playing cards drawn from a deck with thirteen ranks and four suits."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-6",
              "text": "Each card is described using two attributes: suit and rank."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-7",
              "text": "Each hand is assigned to any or all of nine different ranks, including High Card, One Pair, Two Pair, etc."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-8",
              "text": "We randomly generate five hundred poker hands and all facts related to them to build a small but complete and correct KG."
            },
            {
              "iri": "Section-14-Paragraph-3-Sentence-9",
              "text": "Twenty-eight out of thirty-five predicates are unary predicates, such as fullHouse(x) where x is a specific poker hand."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-4-Sentence-1",
              "text": "All experiments were conducted on an Intel Xeon CPU E5-2650 v4 at 2.20 GHz, with 66 GB RAM and running CentOS 8."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-5-Sentence-1",
              "text": "Transforming KGs with type predicates for experiments is necessary since real-world KG models treat predicates and entities in a variety of ways."
            },
            {
              "iri": "Section-14-Paragraph-5-Sentence-2",
              "text": "We require a common representation for this work that clearly distinguishes entities and predicates."
            },
            {
              "iri": "Section-14-Paragraph-5-Sentence-3",
              "text": "We employ an abstract model that is used in Description Logic ontologies, where classes and types are named unary predicates, and roles (also called properties) are named binary predicates."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-6-Sentence-1",
              "text": "Presenting the class and type information as unary predicates also allows us to learn fully abstracted (entity-free) shapes instead of partially instantiated shapes."
            },
            {
              "iri": "Section-14-Paragraph-6-Sentence-2",
              "text": "This feature is important since learning partially instantiated shapes can cause an explosion in the space of possible shapes."
            },
            {
              "iri": "Section-14-Paragraph-6-Sentence-3",
              "text": "Using self-loop links for unary predicates is convenient syntactic sugar to keep the presentation in the triple format, as for the original input KGs."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-7-Sentence-1",
              "text": "In real-world KGs, concept or class membership may be modeled as entity instances of a binary fact."
            },
            {
              "iri": "Section-14-Paragraph-7-Sentence-2",
              "text": "For example, DBpedia contains predicates where the second arguments of these predicates are types or classes."
            },
            {
              "iri": "Section-14-Paragraph-7-Sentence-3",
              "text": "Instead, we choose to model types and classes with unary predicates."
            },
            {
              "iri": "Section-14-Paragraph-7-Sentence-4",
              "text": "To do so, we make new predicates from facts in the form, where x is the name of an album."
            },
            {
              "iri": "Section-14-Paragraph-7-Sentence-5",
              "text": "Then we produce new unary facts based on the new predicate and related facts."
            }
          ]
        },
        {
          "iri": "Section-14-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-14-Paragraph-8-Sentence-1",
              "text": "We use the two type-like predicates from DBpedia 3.8 and the one from Wikidata to generate our unary predicates and facts."
            },
            {
              "iri": "Section-14-Paragraph-8-Sentence-2",
              "text": "These predicates each have a class as their second argument."
            },
            {
              "iri": "Section-14-Paragraph-8-Sentence-3",
              "text": "To prune the classes with few instances for which learning may be pointless, we consider only our unary predicates which have at least one hundred facts."
            },
            {
              "iri": "Section-14-Paragraph-8-Sentence-4",
              "text": "We retain the original predicates and facts in the KG as well as extending it with our new ones."
            },
            {
              "iri": "Section-14-Paragraph-8-Sentence-5",
              "text": "In Table 1, we report the specifications of two benchmarks where we have added the unary predicates and facts."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-15",
      "subtitle": "Experiments: Learning IOP rules",
      "paragraphs": [
        {
          "iri": "Section-15-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-1-Sentence-1",
              "text": "We follow the established approach for evaluating KG rule-learning methods, that is, measuring the quantity and quality of distinct rules learnt."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-2",
              "text": "Rule quality is measured by Inverse open path standard confidence (IOPSC) and Inverse open path head coverage (IOPHC)."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-3",
              "text": "We randomly select 50 target predicates from Wikidata and DBPedia unary predicates (157 and 355 respectively)."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-4",
              "text": "We use all binary predicates of YAGO2s (i.e., 37) as target predicates."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-5",
              "text": "Each binary target predicate serves as two target predicates, once in the straight form (where the object argument of the predicate is the common variable to connect the head) and secondly in its reverse form (where the subject argument serves to connect)."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-6",
              "text": "In this manner, we ensure that the results of SHACLEARNER on YAGO2s with its binary predicates as targets is comparable with the results for Wikidata and DBpedia that have unary predicates as targets."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-7",
              "text": "Hence for YAGO2s we have 74 target predicates."
            },
            {
              "iri": "Section-15-Paragraph-1-Sentence-8",
              "text": "A 10 hour limit is set for learning each target predicate."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-2-Sentence-1",
              "text": "Table 2 shows the number of rules, the average numbers of quality IOP rules found, the proportion of target predicates for which at least one IOP rule was found, and the running times in hours, averaged over the targets."
            },
            {
              "iri": "Section-15-Paragraph-2-Sentence-2",
              "text": "Only high quality rules meeting minimum quality thresholds are included in these figures, that is, with IOPSC greater than or equal to 0.1 and IOPHC greater than or equal to 0.01, thresholds established in comparative work."
            },
            {
              "iri": "Section-15-Paragraph-2-Sentence-3",
              "text": "These thresholds are quite low, so rules of low quality are also included."
            },
            {
              "iri": "Section-15-Paragraph-2-Sentence-4",
              "text": "Generally, selecting a low threshold at the time of learning is a safe choice since the rules can be further pruned by applying stricter quality thresholds later on."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-3-Sentence-1",
              "text": "SHACLEARNER shows satisfactory performance in terms of both the runtime and the numbers of quality rules mined."
            },
            {
              "iri": "Section-15-Paragraph-3-Sentence-2",
              "text": "Note that rules found have a variety of lengths and cardinalities."
            },
            {
              "iri": "Section-15-Paragraph-3-Sentence-3",
              "text": "To better present the quality performance of rules, we illustrate the distribution of rules with respect to the features, IOPSC, IOPHC, cardinality and length, and also IOPSC vs length."
            },
            {
              "iri": "Section-15-Paragraph-3-Sentence-4",
              "text": "In the following, the proportion of mined rules having the various feature values is presented, to more evenly demonstrate the quality of performance over the three very different KGs."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-4-Sentence-1",
              "text": "The distribution of mined rules with respect to their IOPSC and IOPHC is shown in the figures."
            },
            {
              "iri": "Section-15-Paragraph-4-Sentence-2",
              "text": "In the left-hand chart we observe a consistent decrease in the proportion of quality rules as the IOPSC increases."
            },
            {
              "iri": "Section-15-Paragraph-4-Sentence-3",
              "text": "In the right hand chart we see a similar pattern for increasing IOPHC, but the decrease is not as consistent, showing there must be many relevant rules with many covered head instances."
            },
            {
              "iri": "Section-15-Paragraph-4-Sentence-4",
              "text": "Over all benchmarks, the majority of learned rules have lower IOPSC and IOPHC."
            },
            {
              "iri": "Section-15-Paragraph-4-Sentence-5",
              "text": "This is expected because the statistical likelihood of poor quality rules is much higher."
            },
            {
              "iri": "Section-15-Paragraph-4-Sentence-6",
              "text": "Indeed, we show experimentally that our pruning techniques, that are necessary for scalability, prune away predominantly lower quality rules."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-5-Sentence-1",
              "text": "With respect to IOPSC, proportionally more quality rules are learnt from DBpedia than from the other KGs, with Wikidata coming second, ahead of YAGO2s."
            },
            {
              "iri": "Section-15-Paragraph-5-Sentence-2",
              "text": "This phenomenon might be a result of our deliberate selection of type-like unary predicates as targets for DBPedia and Wikidata, whereas for YAGO2s we use every one of the 37 binary predicates as a target."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-6-Sentence-1",
              "text": "The distribution of mined rules with respect to their cardinalities shows that the largest proportion of rules has a cardinality of 1, as expected, as they have the least stringent requirements to be met in the KG."
            },
            {
              "iri": "Section-15-Paragraph-6-Sentence-2",
              "text": "We observe an expected decrease with greater cardinalities as they demand tighter restrictions to be satisfied."
            },
            {
              "iri": "Section-15-Paragraph-6-Sentence-3",
              "text": "YAGO2 demonstrates a tendency towards higher cardinalities than the other KGs, possibly a result of its more curated development."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-7-Sentence-1",
              "text": "The distribution of mined rules with respect to their lengths shows that as the length increases, the number of rules would increase since the space of possible rules grows, and this is what we see."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-8-Sentence-1",
              "text": "For a concrete example of SHACL learning, we show the following three IOP rules mined from DBpedia in the experiments."
            },
            {
              "iri": "Section-15-Paragraph-8-Sentence-2",
              "text": "The IOPSC, IOPHC, and Cardinality annotations respectively prefix each rule."
            },
            {
              "iri": "Section-15-Paragraph-8-Sentence-3",
              "text": "The first rule indicates x should belong to an album that has y as record label."
            },
            {
              "iri": "Section-15-Paragraph-8-Sentence-4",
              "text": "The second rule requires a song (x) to have at least one producer while the third rule requires a song to have at least two producers, and these two rules are distinguished by the cardinality annotation."
            },
            {
              "iri": "Section-15-Paragraph-8-Sentence-5",
              "text": "As we discussed, the third rule is more constraining than the second, so the confidence of the third rule is lower than the confidence of the second, based on the KG data."
            }
          ]
        },
        {
          "iri": "Section-15-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-15-Paragraph-9-Sentence-1",
              "text": "Using rules found in the experiments, we further illustrate the practical meaning of the IOPSC and IOPHC qualities."
            },
            {
              "iri": "Section-15-Paragraph-9-Sentence-2",
              "text": "While IOPSC determines the confidence of a rule based on counting the proportion of target predicate instances for which the rule holds true in the KG, IOPHC indicates the proportion of rule consequent instances that are justified by target predicate instances in the KG, thereby indicating the relevance of the rule to the target."
            },
            {
              "iri": "Section-15-Paragraph-9-Sentence-3",
              "text": "In Wikidata+UP, all unary predicates are occupations such as singer or entrepreneur, so all the entities which have these types turn out to be persons even though there is no explicit person type in our KG."
            },
            {
              "iri": "Section-15-Paragraph-9-Sentence-4",
              "text": "Thus, the occupations all have very similar IOP rules about each of them with high IOPSC and low IOPHC."
            },
            {
              "iri": "Section-15-Paragraph-9-Sentence-5",
              "text": "On the other hand, for these unary occupation predicates there are also some IOP rules with high IOPHC that apply only to one specific unary predicate."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-16",
      "subtitle": "Experiments: Completeness analysis of IOP rule learning",
      "paragraphs": [
        {
          "iri": "Section-16-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-1-Sentence-1",
              "text": "SHACLEARNER uses two tricks to significantly reduce the search space for IOP rules in PathFinding of Algorithm 1, namely the prior Sampling and the heuristic pruning used inside PathFinding that uses the embedding-based scoring function."
            },
            {
              "iri": "Section-16-Paragraph-1-Sentence-2",
              "text": "We conduct experiments to explore how these two pruning methods affect SHACLEARNER with regard to the quality and quantity of learnt rules."
            },
            {
              "iri": "Section-16-Paragraph-1-Sentence-3",
              "text": "To do so, we create three variants of SHACLEARNER as follows."
            }
          ]
        },
        {
          "iri": "Section-16-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-2-Sentence-1",
              "text": "(-S+H): SHACLEARNER that does not sample and so uses the complete input KG in all components, including embedding learning, heuristic pruning, and ultimate evaluation."
            },
            {
              "iri": "Section-16-Paragraph-2-Sentence-2",
              "text": "(+S-H): SHACLEARNER that samples but does not use heuristic pruning and so generates rules based on the sampled KG and evaluates rules on the complete KG."
            },
            {
              "iri": "Section-16-Paragraph-2-Sentence-3",
              "text": "(-S-H): SHACLEARNER that does not use sampling nor heuristic pruning."
            },
            {
              "iri": "Section-16-Paragraph-2-Sentence-4",
              "text": "This system is an ideal IOP rule learner that generates and evaluates all possible rules up to the maximum length parameter."
            },
            {
              "iri": "Section-16-Paragraph-2-Sentence-5",
              "text": "Since this method is a brute-force search, it cannot handle any real-world KG such as those we used in the performance evaluation."
            },
            {
              "iri": "Section-16-Paragraph-2-Sentence-6",
              "text": "(+S+H): SHACLEARNER with its full functionality."
            }
          ]
        },
        {
          "iri": "Section-16-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-3-Sentence-1",
              "text": "Since we use only the small Poker KG for this experiment, we can handle the task without sampling or heuristic mechanisms."
            },
            {
              "iri": "Section-16-Paragraph-3-Sentence-2",
              "text": "We use all 28 unary predicates as the target predicates."
            },
            {
              "iri": "Section-16-Paragraph-3-Sentence-3",
              "text": "The first row shows the number of IOP rules that are learnt by ideal, modified SHACLEARNER (-S-H) with no pruning, for all target predicates, separated into various IOPSC intervals."
            },
            {
              "iri": "Section-16-Paragraph-3-Sentence-4",
              "text": "The latter rows show, for each variant, the percentage difference in the number of rules found, relative to the first row."
            },
            {
              "iri": "Section-16-Paragraph-3-Sentence-5",
              "text": "The last row corresponds to unmodified SHACLEARNER of Algorithm 1."
            }
          ]
        },
        {
          "iri": "Section-16-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-4-Sentence-1",
              "text": "For example, consider the first column (the number of learnt rules with IOPSC in range of [0.1, 0.3))."
            },
            {
              "iri": "Section-16-Paragraph-4-Sentence-2",
              "text": "In the first row, we have 163 rules learnt by ideal rule learner (SHACLEARNER (-S-H)) that is inefficient yet complete."
            },
            {
              "iri": "Section-16-Paragraph-4-Sentence-3",
              "text": "In the second row, we have the performance of the SHACLEARNER (-S+H), the system without sampling, but with the heuristic rule learning module, in comparison with the ideal rule learner (SHACLEARNER (-S-H)), given as -10%."
            },
            {
              "iri": "Section-16-Paragraph-4-Sentence-4",
              "text": "That means SHACLEARNER (-S+H) learns 146 rules with IOPSC in range of [0.1, 0.3): 17 or 10% fewer rules than the ideal learner."
            }
          ]
        },
        {
          "iri": "Section-16-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-5-Sentence-1",
              "text": "We observe that SHACLEARNER does not miss any rules of the highest quality, i.e., with IOPSC greater than or equal to 0.9."
            },
            {
              "iri": "Section-16-Paragraph-5-Sentence-2",
              "text": "SHACLEARNER\u2019s pruning methods cause it to fail to discover more rules of lower quality, with the number of missing rules increasing as quality decreases."
            },
            {
              "iri": "Section-16-Paragraph-5-Sentence-3",
              "text": "This is a reassuring property, since the goal of pruning is to improve the computational performance without missing high-quality rules."
            },
            {
              "iri": "Section-16-Paragraph-5-Sentence-4",
              "text": "In real applications, we will typically retain and action only the highest quality rules."
            }
          ]
        },
        {
          "iri": "Section-16-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-16-Paragraph-6-Sentence-1",
              "text": "We observe that, unlike the other pruning variants, using heuristic pruning alone in (-S+H) does not uniformly increase in effectiveness with decreasing rule quality."
            },
            {
              "iri": "Section-16-Paragraph-6-Sentence-2",
              "text": "This may be because using the complete KG for learning rules about all target predicates could harm the quality of the learnt embeddings used in the scoring function of SHACLEARNER."
            },
            {
              "iri": "Section-16-Paragraph-6-Sentence-3",
              "text": "The better quality of embeddings extracted from the sampled KG arises from our sampling method that creates a KG that is customised for the target predicate."
            },
            {
              "iri": "Section-16-Paragraph-6-Sentence-4",
              "text": "All entities in the sampled KG are either directly related to the target predicate or close neighbours of directly related entities."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-17",
      "subtitle": "Experiments: Learning trees from IOP rules",
      "paragraphs": [
        {
          "iri": "Section-17-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-17-Paragraph-1-Sentence-1",
              "text": "Now we turn to presenting results for the trees that are built based on the IOP rules discovered in the experiments."
            },
            {
              "iri": "Section-17-Paragraph-1-Sentence-2",
              "text": "We report the characteristics of discovered trees and use a value of 0.1 for the TreeSCMIN parameter, showing average TreeSC for each KG, along with the average number of branches in the trees and the average tree-building runtime."
            },
            {
              "iri": "Section-17-Paragraph-1-Sentence-3",
              "text": "The number of trees for each KG is defined by the number of target predicates for which we have at least one IOP rule."
            }
          ]
        },
        {
          "iri": "Section-17-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-17-Paragraph-2-Sentence-1",
              "text": "The results show the running time for aggregating IOP rules into trees is lower than the initial IOP mining time by a factor greater than 10."
            },
            {
              "iri": "Section-17-Paragraph-2-Sentence-2",
              "text": "If, on the other hand, we wanted to discover such complex shapes from scratch, it would be exhaustively time-consuming due to the sensitivity of rule learners to the maximum length of rules."
            },
            {
              "iri": "Section-17-Paragraph-2-Sentence-3",
              "text": "The number of potential rules in the search space grows exponentially with the maximum number of predicates of the rules."
            }
          ]
        },
        {
          "iri": "Section-17-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-17-Paragraph-3-Sentence-1",
              "text": "The average number of branches in the mined trees are 50%, 31%, and 56% of the corresponding number of mined rules."
            },
            {
              "iri": "Section-17-Paragraph-3-Sentence-2",
              "text": "Hence, by imposing the additional tree-shaped constraint over the basic IOP-shaped constraint, at least 44% of IOP rules are pruned."
            }
          ]
        },
        {
          "iri": "Section-17-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-17-Paragraph-4-Sentence-1",
              "text": "For an example of tree shape learning, we show a fragment of a 39-branched tree mined from DBpedia by aggregating IOP rules in the experiments."
            },
            {
              "iri": "Section-17-Paragraph-4-Sentence-2",
              "text": "Here, the first annotation value (0.13) presents the SC of the tree and the subsequent values at the beginning of each branch indicate the branch cardinality."
            },
            {
              "iri": "Section-17-Paragraph-4-Sentence-3",
              "text": "This tree can be read as saying that a song has an album with a record label, an album with two producers, an album with a genre, and an artist who is a musical artist."
            }
          ]
        },
        {
          "iri": "Section-17-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-17-Paragraph-5-Sentence-1",
              "text": "As can be seen here, there remains an opportunity for improving tree shapes for simplicity and easier interpretation by unifying some variables that occur in predicates that occur in multiple branches."
            },
            {
              "iri": "Section-17-Paragraph-5-Sentence-2",
              "text": "We plan to investigate this potential post-processing step in future work."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-18",
      "subtitle": "Conclusion",
      "paragraphs": [
        {
          "iri": "Section-18-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-18-Paragraph-1-Sentence-1",
              "text": "In this paper we propose a method to learn SHACL shapes from KGs as a way to describe KG patterns, to validate KGs, and also to support new data entry."
            },
            {
              "iri": "Section-18-Paragraph-1-Sentence-2",
              "text": "For entities that satisfy target predicates, our shapes describe conjunctive paths of constraints over properties, enhanced with minimum cardinality constraints."
            },
            {
              "iri": "Section-18-Paragraph-1-Sentence-3",
              "text": "We reduce the SHACL learning problem to learning a novel kind of rules, Inverse Open Path rules (IOP)."
            },
            {
              "iri": "Section-18-Paragraph-1-Sentence-4",
              "text": "We introduce rule quality measures IOP Standard Confidence, IOP Head Coverage, and Cardinality which augment the rules."
            }
          ]
        },
        {
          "iri": "Section-18-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-18-Paragraph-2-Sentence-1",
              "text": "IOPSC effectively extends SHACL with shapes, representing the quantified uncertainty of a candidate shape to be selected for interestingness or for KG verification."
            },
            {
              "iri": "Section-18-Paragraph-2-Sentence-2",
              "text": "We also propose a method to aggregate learnt IOP rules in order to discover more complex shapes, that is, trees."
            },
            {
              "iri": "Section-18-Paragraph-2-Sentence-3",
              "text": "The shapes support efficient and interpretable human validation in a depth-first manner and are employed, for example, in an editor called Schimatos for manual knowledge graph completion."
            },
            {
              "iri": "Section-18-Paragraph-2-Sentence-4",
              "text": "The shapes can also be used to complete information triggered by entities with only a type or class declaration by automatically generating dynamic data entry forms."
            }
          ]
        },
        {
          "iri": "Section-18-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-18-Paragraph-3-Sentence-1",
              "text": "In this manual mode, they can also be used more traditionally to complete missing facts for a target predicate, as well as other predicates related to the target, while enabling the acquisition of facts about entities that are entirely missing from the KG."
            },
            {
              "iri": "Section-18-Paragraph-3-Sentence-2",
              "text": "To learn such shapes we adapt an embedding-based Open Path Rule Learner (OPRL) by introducing the following novel components: (1) we propose an IOP rule language that allows us to mine rules with open variables, with one predicate forming the body and a chain of predicates as the head; (2) we introduce cardinality constraints and tree shapes for more expressive patterns; and (3) we propose an efficient method to evaluate IOP rules and trees by exactly computing the quality measures of each rule using fast matrix and vector operations."
            }
          ]
        },
        {
          "iri": "Section-18-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-18-Paragraph-4-Sentence-1",
              "text": "Our experiments show that SHACLearner can mine IOP rules of various lengths, cardinalities, and qualities from three massive real-world benchmark KGs including Yago, Wikidata, and DBpedia."
            },
            {
              "iri": "Section-18-Paragraph-4-Sentence-2",
              "text": "Learning shape constraints from schema-free knowledge bases, such as most modern KGs, is a challenging task, beginning with the formalism of constraints that determine the scope of knowledge that can be acquired."
            },
            {
              "iri": "Section-18-Paragraph-4-Sentence-3",
              "text": "The next challenge is designing an efficient learning method, where dealing with uncertainty in the constraints and the learning process adds an extra dimension of challenge but also adds utility."
            },
            {
              "iri": "Section-18-Paragraph-4-Sentence-4",
              "text": "A good learning algorithm should scale gracefully so that discovered constraints are relatively more certain than those that are missed, and SHACLearner establishes a benchmark for this problem."
            }
          ]
        },
        {
          "iri": "Section-18-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-18-Paragraph-5-Sentence-1",
              "text": "In future work, we will validate the shapes we learn with SHACLearner via formal human-expert evaluation and further extend the expressivity of the shapes we can discover."
            },
            {
              "iri": "Section-18-Paragraph-5-Sentence-2",
              "text": "We also propose to redesign the SHACLearner algorithm for a MapReduce implementation to handle extremely massive KGs with tens of billions of facts, such as the most recent version of Wikidata."
            }
          ]
        }
      ]
    }
  ],
  "summary": "Knowledge Graphs (KGs) have proliferated on the Web since the introduction of knowledge panels to Google search in 2012. KGs are large data-first graph databases with weak inference rules and weakly-constraining data schemes. SHACL, the Shapes Constraint Language, is a W3C recommendation for expressing constraints on graph data as shapes. SHACL shapes serve to validate a KG, to underpin manual KG editing tasks, and to offer insight into KG structure. Often in practice, large KGs have no available shape constraints and so cannot obtain these benefits for ongoing maintenance and extension.\n\nWe introduce Inverse Open Path (IOP) rules, a predicate logic formalism which presents specific shapes in the form of paths over connected entities that are present in a KG. IOP rules express simple shape patterns that can be augmented with minimum cardinality constraints and also used as a building block for more complex shapes, such as trees and other rule patterns. We define formal quality measures for IOP rules and propose a novel method to learn high-quality rules from KGs. We show how to build high-quality tree shapes from the IOP rules. Our learning method, SHACLEARNER, is adapted from a state-of-the-art embedding-based open path rule learner.\n\nWe evaluate SHACLEARNER on some real-world massive KGs, including YAGO2s with 4 million facts, DBpedia 3.8 with 11 million facts, and Wikidata with 8 million facts. The experiments show that our SHACLEARNER can effectively learn informative and intuitive shapes from massive KGs. The shapes are diverse in structural features such as depth and width, and also in quality measures that indicate confidence and generality.\n\nThe paper discusses the importance of knowledge graphs (KGs) and their limitations, particularly with regards to completeness and constraints. It introduces SHACL shapes as a way to express constraints on KG data and proposes a method for learning these shapes from KG data using inverse open path rules. The approach is based on predicate calculus formalism and uses embedding-based rule learner. The mined shapes are augmented with confidence measures to reflect the strength of evidence in the KG.\n\nThis section presents closed path rules and open path rules. An entity is an identifier for an object or person, while a fact (RDF triple) represents relationships between entities via predicates. A knowledge graph consists of entities and facts. Closed-path rules are used in KG rule learning systems to express rules with universal quantification of variables. The support measure counts the number of instances where both body and head are satisfied. Standard confidence measures how frequently the rule is true, while head coverage measures explanatory power.\n\nOPRL defines open path (OP) rules for active knowledge graph completion, which imply the existence of a fact. OP rules have two open variables and are used to assess their quality using open path standard confidence (OPSC), open path head coverage (OPHC).\n\nA Knowledge Graph (KG) can be augmented with type information using schema-free databases like SHACL, which defines constraints for graphs as shapes. Shapes serve two main purposes: validating KG quality and characterising frequent patterns. We focus on property shapes that constrain an argument of a target predicate, such as expressing relationships between entities in Wikidata.\n\nInverse Open Path (IOP) rules are introduced as a converse to OP rules, corresponding to SHACL shapes. IOP rules have a general form and can be used to predict instances of their head predicates. The quality measures for IOP rules include inverse open path support, standard confidence, and head coverage. Annotated IOP rules with cardinality Car express the number of different chains that should exist. A lemma shows that IOPSC is non-increasing with length, which can be useful in rule learning.\n\nSHACLEARNER is an IOP rule learner that adapts OPRL's embedding-based learning to mine annotated rules. It uses sampling, embeddings, and heuristic functions to generate potential rules bounded by maximum length. The algorithm can handle unary predicates and minimum cardinality constraints in the head of the rule. SHACLEARNER also aggregates mined rules into trees for translation to SHACL.\n\nThe Sampling() method reduces a massive knowledge graph (KG) to a smaller subgraph, K', by sampling entities related to a target predicate Pt. This approach helps embedding learners handle large KGs and enables learning IOP rules with up to l atoms.\n\nThe Embeddings() function computes predicate embeddings, as well as subject and object argument embeddings for all predicates in the sampled KG. This is done using RESCAL to embed entities and predicates into vectors or matrices. The scoring function f(e1, P0, e2) = E1^T . P0 . E2 measures the plausibility of a fact that e1 has relation P0 with e2.\n\nPathFinding() generates candidate IOP rules based on predicate embeddings, pruning them using a scoring function heuristic for OP rules. The relationship between logical statements and embedding space properties ensures similar argument embeddings and composite predicates form plausible paths.\n\nThe efficient computation of quality measures (IOPSC and IOPHC) for SHACLearner involves evaluating candidate rules on a small sampled knowledge graph, then downselecting based on their support calculated over the full graph. The paper shows how to efficiently compute these measures using an adjacency matrix representation of the graph.\n\nSHACLearner uses Algorithm 1 to derive SHACL trees from annotated IOP rules, which are used for knowledge graph (KG) completion and validation. The algorithm employs a greedy search to aggregate IOP rules into tree shapes, considering quality measures such as TreeSC. These uncertain shapes can be presented as standard SHACL shapes by ignoring those that fail to satisfy minimum quality thresholds. SHACLearner supports node and property shapes, but has limitations in handling object properties, data type validation, and cardinality expressions.\n\nThe current approaches for learning SHACL shapes from KGs are procedural and lack logical foundations, making them unsuitable for real-world applications. Some methods use semi-automatic or interactive frameworks to define SHACL shapes, but these do not provide a system that discovers patterns from massive KGs. In contrast, our approach uses the KG itself to discover shapes without relying on external modelling artefacts.\n\nWe implemented SHACLEARNER, an algorithm for learning shapes from massive knowledge graphs (KGs). Our experiments show that it can handle real-world KGs like DBpedia and discover diverse shapes. We also demonstrated scalability by handling large KGs including DBpedia with over 11 million facts. The performance of our system shows promise in practice.\n\nThe paper evaluates KG rule-learning methods using established approaches, measuring quantity and quality of distinct rules learned. Rule quality is measured by Inverse open path standard confidence (IOPSC) and Inverse open path head coverage (IOPHC). The SHACLEARNER algorithm shows satisfactory performance in terms of runtime and number of high-quality rules mined from three different KGs: Wikidata, DBPedia, and YAGO2. The distribution of mined rules is presented with respect to IOPSC, IOPHC, cardinality, and length, showing a consistent decrease in quality as these features increase.\n\nSHACLEARNER uses two pruning methods, prior sampling and heuristic pruning, to reduce the search space for IOP rules. We experiment with three variants of SHACLEARNER: (-S+H) without sampling but with heuristic pruning, (+S- H) that samples but doesn't use heuristic pruning, and (-S-H) which does neither. The results show that SHACLEARNER's pruning methods don't miss high-quality rules (IOPSC >= 0.9), but do fail to discover lower-quality rules. Using the complete KG for learning embeddings can harm their quality.\n\nThe results show that aggregating IOP rules into trees reduces runtime by a factor greater than 10 compared to initial IOP mining time. The average number of branches in mined trees ranges from 31% to 56% of the corresponding number of mined rules, with at least 44% of IOP rules pruned due to tree-shaped constraints. A fragment of a 39-branched tree mined from DBpedia is presented as an example.\n\nIn this paper we propose a method to learn SHACL shapes from KGs as a way to describe KG patterns, to validate KGs, and also to support new data entry. For entities that satisfy target predicates, our shapes describe conjunctive paths of constraints over properties, enhanced with minimum cardinality constraints. We reduce the SHACL learning problem to learning a novel kind of rules, Inverse Open Path rules (IOP). We introduce rule quality measures IOP Standard Confidence, IOP Head Coverage, and Cardinality which augment the rules.\n\nIOPSC effectively extends SHACL with shapes, representing the quantified uncertainty of a candidate shape to be selected for interestingness or for KG verification. We also propose a method to aggregate learnt IOP rules in order to discover more complex shapes, that is, trees. The shapes support efficient and interpretable human validation in a depth-first manner and are employed, for example, in an editor called Schimatos for manual knowledge graph completion. The shapes can also be used to complete information triggered by entities with only a type or class declaration by automatically generating dynamic data entry forms.\n\nIn this manual mode, they can also be used more traditionally to complete missing facts for a target predicate, as well as other predicates related to the target, while enabling the acquisition of facts about entities that are entirely missing from the KG. To learn such shapes we adapt an embedding-based Open Path Rule Learner (OPRL) by introducing the following novel components: (1) we propose an IOP rule language that allows us to mine rules with open variables, with one predicate forming the body and a chain of predicates as the head; (2) we introduce cardinality constraints and tree shapes for more expressive patterns; and (3) we propose an efficient method to evaluate IOP rules and trees by exactly computing the quality measures of each rule using fast matrix and vector operations.\n\nOur experiments show that SHACLearner can mine IOP rules of various lengths, cardinalities, and qualities from three massive real-world benchmark KGs including Yago, Wikidata, and DBpedia. Learning shape constraints from schema-free knowledge bases, such as most modern KGs, is a challenging task, beginning with the formalism of constraints that determine the scope of knowledge that can be acquired. The next challenge is designing an efficient learning method, where dealing with uncertainty in the constraints and the learning process adds an extra dimension of challenge but also adds utility. A good learning algorithm should scale gracefully so that discovered constraints are relatively more certain than those that are missed, and SHACLearner establishes a benchmark for this problem.\n\nIn future work, we will validate the shapes we learn with SHACLearner via formal human-expert evaluation and further extend the expressivity of the shapes we can discover. We also propose to redesign the SHACLearner algorithm for a MapReduce implementation to handle extremely massive KGs with tens of billions of facts, such as the most recent version of Wikidata.",
  "kg2text": [
    "Our proposed system uses a method to learn SHACL shapes from knowledge graphs as a way to describe KG patterns, validate KGs, and support new data entry. This method can be used to express constraints on graph data as shapes in Output, which are learned from massive knowledge graphs using the proposed algorithm SHACLEARNER. The output refers to a set of Inverse Open Path rules that are used to describe conjunctive paths of constraints over properties and enhance SHACL shapes. Our system uses instantiated open path rules derived from these IOP rules to learn informative and intuitive shapes from massive KGs, which can be validated using the proposed algorithm OPRL. Furthermore, our system addresses learning SHACL shapes from KGs by offering insight into KG structure.",
    "Our proposed system, which uses Open Path rule learning method and SHACLEARNER algorithm, can effectively learn informative and intuitive shapes from massive knowledge graphs. These learned shapes are used to describe KG patterns, validate KGs, and support new data entry. The number of trees for each KG is defined by the number of target predicates for which we have at least one IOP rule. Our system also represents right-hand tree, validates to offer insight into KG structure, and provides a way to generate high-quality SHACL shapes from KGs. Moreover, our proposed system can be used with existing ontologies for KGs to generate SHACL shapes. The instantiated open path corresponds to the learned SHACL shapes, which are derived from it and express constraints on graph data as shapes in SHACL.",
    "Our proposed system uses annotated IOP rules to learn high-quality SHACL shapes from massive knowledge graphs. These instantiated open paths correspond to a set of Inverse Open Path (IOP) rules, which can be learned from knowledge graphs using our SHACLEARNER algorithm. This method is closely related to the process of learning SHACL shapes from KGs as a way to describe KG patterns, validate KGs, and support new data entry. Furthermore, it uses this product to offer insight into KG structure.",
    "Our SHACLEARNER can effectively learn informative and intuitive shapes from massive knowledge graphs, facilitating insight into their structure. This learning method uses Open Path Rule Learner (OPRL) to enhance SHACL shapes derived from these KGs. Our proposed system utilizes inverse open path rules to generate right-hand trees that validate instantiated open paths. One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs, which our system can support. The number of trees for each KG is defined by the presence of at least one IOP rule. Our approach addresses learning SHACL shapes from KGs, generating high-quality constraints that represent these patterns. This paper proposes a method to learn SHACL shapes from KGs as a way to describe KG patterns, validate them, and support new data entry.",
    "The Open Path Rule Learning Method derives instantiated open paths, which correspond to right-hand trees. This method can be used as input for generating IOP rules and learning SHACL shapes from KGs. Our proposed system uses this approach to learn high-quality SHACL shapes from massive KGs by leveraging existing ontologies and annotated IOP rules. The learned SHACL shapes are then used to validate and provide insight into the structure of the knowledge graph, facilitating manual editing tasks and offering a way to describe patterns in the data.",
    "The right-hand tree represents a type of SHACL shape learned from knowledge graphs using Inverse Open Path rules, which can be used to validate and complete massive KGs. This approach utilizes existing ontologies for KGs to generate SHACL shapes that offer insight into KG structure by validating the output. The number of trees for each KG is defined by the number of target predicates with at least one IOP rule. Our SHACLEARNER algorithm can effectively learn informative and intuitive shapes from massive KGs, which are used to express constraints on graph data as shapes in SHACL. One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs.",
    "The inverse open path (IOP) rules correspond to instantiated open paths, which express specific shapes in the form of paths over connected entities present in a Knowledge Graph. These IOP rules are derived from an Inverse Open Path rule and can be used as building blocks for more complex shapes such as trees and rule patterns. The SHACL learning method is implied by the existence of these instantiated open paths, which correspond to this product. One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs, which defines the number of trees for each KG based on the presence of IOP rules for specific target predicates. Annotated IOP rules express Output, learned from knowledge graphs using Open Path Rule Learner (OPRL). The OPRL algorithm adapts to learn high-quality SHACL shapes and represents right-hand tree structures. Address learning SHACL shapes from KGs validates and facilitates manual editing tasks to offer insight into KG structure.",
    "This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs (KGs) as a way to describe KG patterns, validate KGs, and support new data entry. The approach uses existing ontologies for KGs to generate SHACL shapes, which are then used by the Open Path Rule Learner (OPRL) to learn inverse open path rules. These IOP rules can be used to represent right-hand trees that can be used to validate massive KGs. Furthermore, this paper introduces our SHACLEARNER as a method that can effectively learn informative and intuitive shapes from large knowledge graphs. The learned SHACL shapes can express the concept of offering insight into KG structure by providing address learning SHACL shapes from KGs. Additionally, the inverse open path rules can be used to mine SHACL shapes from KGs.",
    "This paper proposes a novel approach to learning SHACL shapes from knowledge graphs (KGs) using instantiated open paths. The method, called SHACLEARNER, learns high-quality SHACL shapes by addressing IOP rules and validating KG structure. To achieve this, it uses existing ontologies for KGs to generate SHACL shapes. Furthermore, the paper demonstrates that Open Path rule learning methods can be used to validate and facilitate insight into KG structure, providing right-hand trees as a result. Additionally, the number of tree shapes in each KG is defined by the presence of IOP rules for specific target predicates. The proposed algorithm, SHACLEARNER, offers valuable insights into KG structure and can be learned from output.",
    "This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs (KGs) as a way to describe KG patterns, validate KGs, and support new data entry. The approach uses existing ontologies for KGs to generate SHACL shapes, which are then used to express constraints on graph data as shapes in Output. Open Path rule learning method represents right-hand trees that can be used to offer insight into KG structure by validating annotated IOP rules. Furthermore, the paper suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs, proposing a way to detect which SHACL shapes could be violated by applying inference rules.",
    "The Open Path Rule Learner (OPRL) uses SHACL learning method to adapt and learn from this product, which represents a set of Inverse Open Path rules. These IOP rules are mined and used to generate SHACL shapes that can be applied to knowledge graphs. One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs, which is enhanced by OPRL. This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs as a way to describe KG patterns, validate KGs, and support new data entry. The output refers to a set of IOP rules used to express constraints on graph data as shapes in SHACL. Addressing learning SHACL shapes from KGs involves using existing ontologies for generating SHACL shapes, mining inverse open path rules, and suggesting proposals like the one mentioned above. Finally, the sampled KG is expressed and generated into Output.",
    "The sampled knowledge graph learns SHACL shapes from its structure, utilizing the Open Path Rule Learner (OPRL) to propose this paper. The OPRL introduces IOP rules that facilitate insight into KG structure and provide inverse open path rules as building blocks for more complex shapes. One proposal suggests an extended validation framework for inference rules and SHACL shapes in knowledge graphs, which expresses constraints on graph data and validates the insights offered by the proposed method. This product is a result of using the OPRL to validate the sampled KG's structure, providing insight into its underlying pattern. The number of trees for each KG is defined by the presence of IOP rules for target predicates.",
    "The number of trees for each KG is defined by the number of target predicates with at least one IOP rule. One proposal suggests an extended validation framework for inference rules and SHACL shapes, which can be used to learn high-quality SHACL shapes from knowledge graphs. This paper proposes a method called SHACLEARNER that learns SHACL shapes from KGs as a way to describe patterns, validate KGs, and support new data entry. The Open Path rule learning method uses IOP rules to generate right-hand trees, which represent the learned SHACL shape constraints. These constraints can be used to validate massive knowledge graphs. Additionally, existing ontologies for KGs are used to generate SHACL shapes, providing a way to address learning SHACL shapes from KGs.",
    "One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in KGs. This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs (KGs) as a way to describe KG patterns, validate KGs, and support new data entry. The approach uses existing ontologies for KGs to generate SHACL shapes. Annotated IOP rules are used in learning high-quality SHACL shapes from knowledge graphs. Inverse Open Path rules are presented specific shapes in the form of paths over connected entities present in a Knowledge Graph (KG), used as building blocks for more complex shapes such as trees and rule patterns. The paper introduces an Open Path rule learning method that proposes right-hand tree, which represents massive KGs.",
    "A proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes in knowledge graphs. This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs as a way to describe KG patterns, validate KGs, and support new data entry. The proposed method uses inverse open path (IOP) rules, which are generated from the sampled KG. IOP rules suggest an extended validation framework for the interaction between inference rules and SHACL shapes in KGs. This product learns from IOP rules and expresses constraints on the sampled KG.",
    "The sampled knowledge graph generates IOP rules for this product, which learns SHACL shapes from itself. This paper proposes a method to learn SHACL shapes from KGs and uses existing ontologies to generate these shapes. The interaction between inference rules and SHACL shapes in KGs is crucial for validating or detecting potential violations within the graph. Furthermore, annotated IOP rules are evaluated, annotated, and filtered to produce high-quality SHACL shapes.",
    "The quality of IOP rules plays a crucial role in predicting instances of SHACL shapes from knowledge graphs. Inverse Open Path (IOP) rules, which present specific shape patterns as paths over connected entities, are used to validate or predict these shapes. The interaction between inference rules and SHACL shapes in KGs is facilitated by the representation formalism they use for their output, which is difficult to compare with IOP rules that we use in this paper. We introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes in the form of paths over connected entities present in a Knowledge Graph. The mined IOP rules are used as building blocks for discovering more complex tree shapes, which can be learned through SHACLEARNER from massive knowledge graphs like Poker KG. An embedding-based method that discovers IOP rules from a KG is also employed to address learning SHACL shapes from KGs.",
    "The task of automatically generating constraints on graph data as shapes using SHACL from knowledge graphs has been addressed. The introduction of Inverse Open Path (IOP) rules provides a building block for complex shapes like trees, along with cardinality constraints and quality measurements. These IOP rules can be syntactically rewritten as SHACL shapes, which serve to validate or predict instances of tree-shaped structures in Knowledge Graphs. Furthermore, the interaction between inference rules and SHACL shapes has been explored, highlighting the importance of basic SHACL semantics for expressing constraints on graph data.",
    "In this paper, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes as paths over connected entities in knowledge graphs. These IOP rules are used to learn SHACL shapes from KGs and address learning SHACL shapes from KGs corresponds to SHACL shapes. Our approach utilizes existing ontologies for KGs to generate SHACL shapes, which is enhanced with minimum cardinality constraints. The quality of these IOP rules has been evaluated using a set of measures evaluating the effectiveness and confidence of Inverse Open Path (IOP) rules in predicting instances of SHACL shapes from knowledge graphs. Furthermore, we propose a novel machine learning algorithm called SHACLEARNER for learning high-quality shapes from knowledge graphs.",
    "This paper introduces Inverse Open Path (IOP) rules, a predicate logic formalism that presents specific shape patterns as paths over connected entities in knowledge graphs. The SHACLEARNER algorithm generates IOP rules based on the sampled KG and discovers more complex tree shapes by aggregating these rules. We reduce the SHACL learning problem to introducing IOP rules, which are used to learn high-quality rules from Knowledge Graphs. Informative and intuitive shapes refer to specific patterns or structures learned by the SHACLEARNER method from large knowledge graphs. The main contributions of this paper propose a novel method to aggregate IOP rules into tree shapes, extending the learning of IOP rules annotated with cardinality constraints using unary predicates.",
    "The interaction between inference rules and SHACL shapes in KGs enables learning high-quality tree shapes from massive knowledge graphs. Our state-of-the-art embedding-based open path rule learner, SHACLEARNER, aggregates IOP rules for subject and object arguments of each target predicate to derive tree shapes. This paper proposes a method called SHACLEARNER that learns SHACL shapes from knowledge graphs as a way to describe KG patterns, validate KGs, and support new data entry.",
    "In this paper, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism for expressing specific shape patterns in knowledge graphs. We also propose SHACLEARNER, a novel machine learning algorithm that learns high-quality shapes from massive knowledge graphs. Our approach uses existing ontologies to generate SHACL shapes and can be used to validate a knowledge graph. The output of our method is a set of IOP rules R and Tree, which are used as constraints on graph data. We demonstrate the effectiveness of our approach by applying it to the small Poker Knowledge Graph (KG) and show that it can learn accurate shape patterns from this dataset.",
    "In this work, we focus on learning SHACL shapes from knowledge graphs (KGs). Our approach uses a novel machine learning algorithm called SHACLEARNER to learn high-quality shapes from massive KGs. The mined IOP rules are used as building blocks for discovering more complex tree shapes. We show that our SHACLEARNER can effectively learn informative and intuitive shapes from large KGs, which is essential in addressing the challenge of learning SHACL shapes from KGs.",
    "The process of combining inference rules with SHACL shapes to validate or detect potential violations within knowledge graphs (KGs) has been a long-standing challenge. To address this, we introduce Inverse Open Path (IOP) rules, which present specific shape patterns in the form of paths over connected entities present in KGs. We propose a method that learns high-quality SHACL shapes from massive real-world KGs using IOP rules and aggregating them to discover more complex tree-structured patterns. Our approach, called SHACLEARNER, is scalable so it can handle large-scale data and has been shown to be effective in learning accurate shape constraints.",
    "The interaction between inference rules and SHACL shapes in KGs enables the derivation of simple shape patterns, such as trees. This process involves combining IOP rules with SHACL constraints to validate or detect potential violations within a knowledge graph. The head of an IOP rule is a sequence of predicates that are predicted to be instantiated together. Inverse Open Path (IOP) rules and OP rules serve as building blocks for more complex shapes, such as trees and rule patterns. By learning high-quality SHACL shapes from KGs using algorithms like RESCAL in Embeddings or embedding learners, we can address the challenge of learning SHACL shapes from massive KGs. The proposed method involves generating SHACL trees by aggregating IOP rules into a tree structure. This process is facilitated through semi-automatic methods that provide an interactive interface for human users to create SHACL shapes.",
    "In knowledge graphs, IOP rules are used to express specific shape patterns as paths over connected entities. These rules can be applied to massive KGs with tens of billions of facts, such as Wikidata. To learn high-quality SHACL shapes from these KGs, a novel machine learning algorithm called SHACLEARNER is employed. This algorithm starts by mining IOP rules and then uses them to prune trees and achieve higher confidence in the learned shapes. The interaction between inference rules and SHACL shapes plays a crucial role in validating or detecting potential violations within a knowledge graph. Informative and intuitive shapes can be effectively learned using our SHACLEARNER, which is particularly useful for addressing learning SHACL shapes from KGs.",
    "In the realm of massive Knowledge Graphs (KGs), SHACL shapes play a crucial role in validating and guiding their population. Basic SHACL serves as a foundation for modeling diverse shapes, including rules and constraints. As expected, certain KGs have less stringent requirements to be met, such as Yago2s, Wikidata, and DBpedia. These real-world knowledge graph models are data structures that represent relationships between entities and concepts in everyday life. The interaction between inference rules and SHACL shapes is a fundamental concept in learning high-quality shapes from massive KGs. In fact, most modern KGs have weakly- constraining data schemes, making it essential to design an efficient learning method for discovering SHACL shapes.",
    "In this paper, we introduce Inverse Open Path (IOP) rules to learn SHACL shapes from knowledge graphs. Our approach utilizes existing ontologies for KGs to generate SHACL shapes and derive shapes that can be trivially expressed in a fragment of SHACL. We also propose learning partially instantiated shapes by combining inference rules with SHACL shapes. Moreover, we demonstrate the effectiveness of our method on real-world massive knowledge graphs (KGs) and show how it can transform these KGs into a common representation for experimentation.",
    "The SHACLearner algorithm's input dataset 'KG K' containing knowledge graph facts. Massive KGs, which are large-scale datasets that are graph databases with weak inference rules and weakly-constraining data schemes, often used to validate or edit knowledge graphs. The interaction between inference rules and SHACL shapes in KGs is the process of combining these rules with SHACL shapes to validate or detect potential violations within a knowledge graph. We introduce Inverse Open Path (IOP) rules, which are predicate logic formalisms presenting specific shape patterns as paths over connected entities present in a Knowledge Graph. These IOP rules can be used to learn high-quality SHACL shapes from massive KGs and support efficient and interpretable human validation in a depth-first manner.",
    "In this study, we discuss the results of an experimental evaluation on SHACLEARNER, a novel machine learning algorithm for learning high-quality shapes from knowledge graphs. We show that SHACLEARNER can derive complex tree-shaped patterns by combining and aggregating simple shape patterns (IOP rules) from massive real-world knowledge graphs. Moreover, our approach does not miss any rules of the highest quality as expected, given their least stringent requirements to be met in the graph. Furthermore, we demonstrate how SHACLEARNER provides an interactive interface for a human user to create SHACL shapes and validate a knowledge graph using these learned shapes.",
    "Our work uses knowledge graphs (KGs) to discover SHACL shapes without relying on external modeling artefacts. The head of the rule has a broader term, IOP rules, which are used to express specific shape patterns as paths over connected entities in KGs. These shapes correspond to SHACL shapes and can be validated against real-world KGs. We introduce Inverse Open Path (IOP) rules, which present specific shapes as paths over connected entities in knowledge graphs. The established approach for evaluating KG rule-learning methods involves measuring both the quantity and quality of distinct rules learned from these IOP rules.",
    "SHACLEARNER, with respect to IOPSC and IOPHC, learns high-quality shapes from knowledge graphs. For a small complete KG, SHACL can be presented as standard SHACL shapes. Derive shapes that can be trivially expressed in a fragment of SHACL have basic SHACL as their broader term. And provides the output in an interactive interface for a human user to create SHACL shapes has The Shapes as its broader term. The computation of IOPSC and IOPHC has IOPHC as its broader term, while the support for an IOP rule has IOP rules. Three very different KGs have Knowledge Graphs as their broader term. To assess evaluates the quality of IOP rules. SHACL shapes have Shapes as their broader term. Yago, Wikidata, and DBpedia have benchmark KGs as their broader term. Annotated IOP rules have Rules as their broader term. Basic SHACL has SHACL, the Shapes Constraint Language as its broader term. Corresponding shapes have SHACL shapes as their broader term. Rules found evaluates To find. We introduce Inverse Open Path (IOP) rules, which are a predicate logic formalism presenting specific shapes as paths over connected entities in knowledge graphs.",
    "The effectiveness of our SHACLEARNER algorithm at capturing shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs (KGs) has been proven. Our experiments demonstrate that IOP rules can be used to express specific shapes in the form of paths over connected entities present in a Knowledge Graph. The power of KGs lies in their data-first approach, allowing users to extend and modify them freely. SHACL learning problem is the task of automatically generating constraints on graph data as shapes using the Shapes Constraint Language (SHACL) from knowledge graphs. Each IOP rule or tree represents either an Inverse Open Path (IOP) rule or a tree-structured shape constructed from these rules.",
    "In this paper, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism for presenting specific shapes in the form of paths over connected entities that are present in a Knowledge Graph. These IOP rules can be induced from paths and used to validate or predict instances of SHACL trees. We also propose rule quality measures, including IOP Standard Confidence, IOP Head Coverage, and Cardinality, which augment these rules. Our approach is demonstrated on massive KGs with tens of billions of facts, such as the most recent version of Wikidata. The results show a consistent decrease in the proportion of high-quality SHACL shape learning rules as their confidence score (IOPSC) increases.",
    "The power of knowledge graphs (KGs) lies in their ability to represent relationships between entities, concepts or terms as a graph of interconnected nodes. In this context, we explore the application of SHACL shapes learned from KGs using heuristic pruning alone and briefly discuss its extension to KG completion. Large KGs are populated by automatic and semi-automatic methods, which can be prone to errors. The method of learning SHACL shapes from knowledge graphs that uses only heuristic pruning without sampling is a novel approach for populating large KGs. Furthermore, we observe a consistent decrease in the proportion of quality rules as IOPSC increases. Inverse Open Path (IOP) rules are used to describe conjunctive paths of constraints over properties and enhance SHACL shapes.",
    "Massive knowledge graphs (KGs) contain numerous entities and relationships. Our shape learning system learns high-quality SHACL shapes from such path shapes, which are sequences of predicates connected by closed intermediate variables but terminating with open variables at both ends. These uncertain shapes can be used to learn inverse open path rules and cardinality constraints. We show how to build high-quality tree shapes from the IOP rules, demonstrating a method for constructing SHACL shapes using Inverse Open Path (IOP) rules. The efficient matrix-computation is an essential component of this process.",
    "SHACLEARNER, an open path rule learner, uses the efficient matrix-computation to learn high-quality shapes from massive knowledge graphs. Currently, it focuses on deriving SHACL trees by aggregating Inverse Open Path rules into a tree structure. The power of KGs lies in their data- first approach, allowing users to extend and modify them freely. We evaluate SHACLEARNER on some real-world massive KGs, including Wikidata+UP, Yago, DBpedia, and others. Most modern KGs correspond to standard SHACL shapes, which define constraints on RDF data. The expressivity of the shapes we can discover is extended by deriving simple shape patterns from knowledge graphs using logical formalisms and motivating use cases.",
    "In this paper, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes as paths over connected entities present in knowledge graphs. We demonstrate how to derive simple shape patterns, such as trees, from these IOP rules using logical formalisms and motivating use cases. Furthermore, we show that existing ontologies for KGs can be used to generate SHACL shapes, which are constraints on graph data serving to validate a knowledge graph, facilitate manual editing tasks, and provide insights into its structure. Additionally, we discuss the importance of learning shape constraints from real-world massive knowledge graphs (KGs) like YAGO2s with 4 million facts, DBpedia 3.8 with 11 million facts, and Wikidata with 8 million facts.",
    "In this study, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes in the form of paths over connected entities that are present in knowledge graphs. These IOP rules can be learned from massive KGs and deployed to make the KG consistent. We also propose novel tree shapes for validating KGs, which provide an innovative way to extend and modify existing KGs. Furthermore, we demonstrate the power of KGs by showcasing their ability to store complex data structures like trees. Our approach is based on learning high-quality SHACL shapes from knowledge graphs using a machine learning technique called Open Path rule learning method.",
    "In this paper, we introduce Inverse Open Path (IOP) rules and propose a novel method to learn high-quality shapes from massive knowledge graphs using IOP rules. Unlike earlier work, our approach uses OPRL for active knowledge graph completion. We omit the quantifiers and focus on learning SHACL shapes from real-world massive knowledge graphs such as YAGO2s, Wikidata, and DBpedia. Our algorithm, SHACLEARNER, is a novel machine learning method that learns high-quality IOP rules bounded by a maximum length using heuristic functions. The power of KGs lies in their data-first approach, allowing users to extend and modify them freely.",
    "Our work uses the KG itself to learn SHACL shapes. This process involves providing an interactive interface for a human user to create these shapes, which are then used as building blocks for more complex structures. The learned IOP rules can be aggregated in GreedySearch to discover trees that represent frequent patterns in large Knowledge Graphs (KGs). These patterns cannot be obtained without the support of SHACL and its ability to express constraints on graph data. Furthermore, our approach employs a predicate logic formalism that presents specific shapes as paths over connected entities present in a KG. The quality measures used for OP rules are similar to those employed here.",
    "Knowledge Graphs (KGs) are large data-structures that represent relationships between entities, concepts or terms. Inverse Open Path (IOP) rules can be learned from these KGs using SHACLEARNER algorithm. Our shapes describe conjunctive paths of constraints over properties, enhanced with minimum cardinality constraints. These shapes can be used to validate a knowledge graph and facilitate manual editing tasks. The power of KGs lies in their ability to extend and modify them freely. For instance, YAGO2s, Wikidata, and DBpedia are three real-world massive knowledge graphs that include millions of facts. SHACLEARNER is a novel machine learning algorithm for learning high-quality shapes from these KGs.",
    "The IOP rules are a predicate logic formalism that presents specific shape patterns in the form of paths over connected entities present in a Knowledge Graph. These constraints are deployed to make the KG consistent, ensuring it conforms to its defined schema and constraints. The converse of OP rules corresponds to SHACL shapes, which define constraints on RDF data. IOPSC is calculated over the full KG, while frequent patterns in a KG have a broader term as (KG). SHACLEARNER discovers shapes of varying complexity and diversity with respect to length and cardinality from knowledge graphs. Original KG has a broader term as A knowledge graph (KG), which represents relationships between entities and concepts. SCALEKB uses so-called closed path (CP) rules for expressing constraints on graph data as shapes. Our pruning techniques prune away lower quality rules, ensuring scalability while maintaining high-quality rule discovery.",
    "We introduce Inverse Open Path (IOP) rules, which present specific shapes as paths over connected entities in knowledge graphs. These IOP rules are learnt by ideal, modified SHACLEARNER (-S-H) with no pruning and can be used to validate massive KGs like YAGO2s, Wikidata, and DBpedia. Our mined shapes offer readable patterns for KG documentation, demonstrating a tendency towards higher cardinalities than the other KGs. The general form of a tree is represented by P't(x, z0), which defines a path or sequence of connected entities in a Knowledge Graph. Shapes provide insights into its structure and can be used to validate massive KGs.",
    "Our proposed system, which combines Inverse Open Path rules and SHACLEARNER algorithm, offers a novel method for learning high-quality shapes from massive knowledge graphs (KGs) automatically. This simple approach reduces the size of the problem by sampling a subgraph of entities related to a target predicate. Although they are not necessarily enforced, existing ontologies serve as constraints on graph data in Knowledge Graphs and guide population of KGs. Our proposed system has been evaluated on real-world KG such as those we used in the performance evaluation, demonstrating its effectiveness in learning high-quality shapes from massive knowledge graphs.",
    "In this study, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes as paths over connected entities present in knowledge graphs. We also propose SHACLEARNER, a novel machine learning algorithm for learning high-quality shapes from massive knowledge graphs using IOP rules and state-of-the-art embedding-based open path rule learner. Our approach enables the derivation of informative and intuitive shapes that can be trivially expressed in fragments of SHACL, the Shapes Constraint Language. Furthermore, we demonstrate the power of knowledge graphs by combining inference rules with SHACL shapes to validate or detect potential violations within a graph.",
    "The Open Path rule learning method has been applied to frequent patterns in a KG, resulting in IOP rules annotated with cardinality constraints. These rules have been mined from large KGs such as YAGO2s, Wikidata, and DBpedia. The confidence of these rules is determined by the IOPSC quality measure, which represents the proportion of true instances among all possible predicate instances in the knowledge graph. As expected, some rules have the least stringent requirements to be met in the KG, making them suitable for learning SHACL shapes from massive datasets like Wikidata. Furthermore, using rules found in experiments illustrates the practical meaning of IOPSC and IOPHC qualities, which represent measures of rule confidence based on target predicate instance proportion and relevance to the target instances justified by these predicates in the graph data.",
    "In this work, we require a common representation for our method to learn SHACL shapes from massive knowledge graphs. Our approach involves mined rules that have been learned from real-world KGs and can be used to automatically predict missing facts in these graphs. We also propose an algorithm based on inverse open path (IOP) rules, which presents specific shape patterns as paths over connected entities present in a Knowledge Graph. This formalism is particularly useful for validating and editing the graph. Furthermore, we demonstrate how our method can learn SHACL shapes from KGs with higher confidence than previous approaches. Our results show that frequent patterns in a KG can be characterized by recurring relationships or structures within the graph.",
    "Our proposed system learns high-quality SHACL shapes from massive knowledge graphs (KGs) automatically, using Inverse Open Path rules and the SHACLEARNER algorithm. This approach enables description of KG patterns, validation of KGs, and support for new data entry. By aggregating IOP rules into tree shapes, we can efficiently learn and represent constraints on graph data. Our system is particularly useful when dealing with massive KGs like YAGO2s, Wikidata, and DBpedia. Furthermore, our proposed method provides a semi-automatic manner to create SHACL shapes from sampled KGs, which facilitates the acquisition of facts about entities that are entirely missing from the KG.",
    "SHACL shapes, which describe conjunctive paths of constraints over properties, can be learned from knowledge graphs using Inverse Open Path rules. The open path rule learner SHACLEARNER learns high-quality IOP rules from massive KGs and can augment these figures with type information. These shapes characterize a KG and can be used for subsequent data cleaning or ongoing data entry. Moreover, the learning of SHACL shapes is addressed by Algorithm 1, which presents the semantics of recursive SHACL shapes. The quality of learned IOP rules is measured using OPSC, ensuring that only high-quality rules are included in these figures.",
    "In this context, we consider an OP rule of the given form. This rule is part of a broader set of rules that frequent patterns in a KG can be characterized by. The SHACLEARNER algorithm, shown in Algorithm 1, learns high-quality shapes from knowledge graphs and has been designed to prove the effectiveness of our SHACLEARNER method. Path-based representations are used to express simple shape patterns, which are then mined using an adjacency matrix representation of the KG. Inverse Open Path rules induced from paths that have a straightforward interpretation as shapes can be learned through Algorithm 2. The SHACLEARNER algorithm discovers and learns these shapes, allowing for effective knowledge graph completion tasks.",
    "The massive real-world benchmark knowledge graph, Yago, has been used to develop a KG context. SCALEKB, an algorithm that uses closed-path rules for expressing constraints on graph data as shapes, was found to be effective in generating frequent patterns in a KG. SHACL shapes from KGs have been applied to define such constraints on graphs as shapes. Large-scale graph databases like YAGO2s and Wikidata were used to store massive amounts of data first, followed by weak inference rules and loosely constraining data structures. The small Poker Knowledge Graph was used as a dataset for evaluating the performance of SHACLEARNER's approach in generating trees that can be generated by SHACLearner. Inverse Open Path rules have been found to be useful in learning methodological approaches for validating and completing massive knowledge graphs.",
    "To sort and select rules based on their quality measures for constructing SHACL trees, one must first understand how to learn shapes from knowledge graphs. This process involves sorting all rules that bind the subject argument into lower-cardinality versions of that rule with the same or higher IOPSC values. The semantics of recursive SHACL shapes are presented in a study or paper that provides formal explanations and definitions of meaning and structure. Publicly accessible databases, such as Yago, Wikidata, and DBpedia, represent relationships between entities using semantic web technologies. Inference processes on these knowledge graphs infer new information by making predictions from type information to understand, validate data, and improve inference accuracy.",
    "In this context, an editor called Schimatos for manual knowledge graph completion plays a crucial role. The inverse open path support (IOPsupp) and head coverage (IOPHC) of IOP rules are essential in understanding frequent patterns in a KG. Moreover, the semantics of SHACL language and property shapes provide valuable insights into the structure of these graphs. Our algorithm learns high-quality shapes from massive knowledge graphs using Inverse Open Path rules and state-of-the-art embedding-based open path rule learners. The distribution of rules with respect to features such as IOPSC, IOPHC, cardinality, and length illustrates their quality metrics.",
    "This paper proposes an approach to generate SHACL shapes from knowledge graphs (KGs) using existing ontologies. The authors show how to build high-quality tree shapes from IOP rules, which can be used for validating and documenting KG patterns. Additionally, they demonstrate a method called SHACLEARNER that learns SHACL shapes from KGs as a way to describe KG patterns, validate KGs, and support new data entry. Furthermore, the authors highlight the importance of using public knowledge graphs like Yago and RESCAL embedding learners in their shape learning system. The proposed approach can be used for building complex tree-structured shapes that capture relationships between entities in a KG.",
    "The SHACLEARNER algorithm learns high-quality shapes from massive knowledge graphs (KGs) by discovering frequent patterns and validating KGs. It uses Inverse Open Path rules, which are quality measures for confidence and generality. The system can handle extremely large KGs with tens of billions of facts, such as the most recent version of Wikidata. By pruning variants and learning annotated IOP rules, SHACLEARNER efficiently extracts meaningful information from massive datasets.",
    "In practice, sampled knowledge graphs (KGs) are much smaller than their original counterparts. To derive shapes that can be trivially expressed in a fragment of SHACL, we compare them to real-world massive KGs like Yago and Wikidata. Mined rules with respect to their cardinalities have broader terms such as KG data. Inverse Open Path (IOP) rules are used for inferring new facts from these massive datasets. We plan to investigate this potential post-processing step further, learning high-quality shapes from knowledge graphs using SHACLEARNER algorithm and aggregating IOP rules to produce tree shapes.",
    "The SHACLEARNER algorithm, shown in Algorithm 1, learns high-quality shapes from knowledge graphs (KGs). It uses a state-of-the-art open path rule learner to discover Inverse Open Path rules. The distribution shows that most learned IOP rules have a cardinality of one, indicating the majority of discovered patterns meet minimum quality thresholds. Formal human-expert evaluation assesses the correctness and relevance of these shapes. SHACL learning is used as a method for processing or solving problems in knowledge graphs.",
    "In knowledge graphs, frequent patterns can be characterized by recurring relationships or structures. We introduce Inverse Open Path (IOP) rules, a predicate logic formalism presenting specific shapes as paths over connected entities within these graphs. These IOP rules are induced from paths that have a straightforward interpretation as shapes and can be used to learn SHACL shapes from knowledge graphs. Moreover, the majority of learned rules exhibit k-cliques in massive real-world KG models like Yago or DBpedia. Furthermore, property shapes derived from these graphs provide constraints on graph data, which can be validated using procedure-based approaches.",
    "In knowledge graphs, constraints are not necessarily enforced. One proposal suggests an extended validation framework for the interaction between inference rules and SHACL shapes. The distribution of learned SHACL shapes from massive KGs shows that mined rules with respect to their lengths can be used as building blocks for more complex shapes such as trees and rule patterns. Our approach uses Inverse Open Path (IOP) rules, which present specific SHACL shapes in the form of paths over connected entities that are present in a Knowledge Graph. We introduce IOPSC, a quality measure for IOP rules based on counting the proportion of target predicate instances for which the rule holds true in the KG. Our novel machine learning algorithm, SHACLEARNER, learns high-quality shapes from knowledge graphs and generates trees that can be used to validate and complete massive KGs.",
    "In this context, SHACLEARNER proposes to detect which SHACL shapes are proposed. The algorithm learns the head of the IOP rule and includes high-quality rules meeting minimum quality thresholds in these figures. Closed rules (CR) serve as a broader term for rules, while SHACL has a broader term existing ontologies. We follow a style of quality measures similar to those used for OP rules, which are also included under the umbrella of shapes. To discover more complex shapes, that is, trees, we use a KG context and feed The sample into an embedding learner. Original KG serves as a comprehensive knowledge representation framework or dataset, while SHACLEARNER with regard to the quality and quantity of learnt rules has a broader term Algorithm 1. Formalisms are various approaches used to express patterns and constraints on graph data (KGs), including SHACL shapes, Closed rules (CR), Functional Graph Dependency (FGD), k-cliques, trees, etc. The rule serves as a specific shape pattern expressed as paths over connected entities that are present in a Knowledge Graph (KG). These figures illustrate the number of high-quality Inverse Open Path (IOP) rules learned by SHACLEARNER from massive knowledge graphs. Open Path rule learning method is an embedding- based open path rule learner designed to learn inverse open path rules and cardinality constraints.",
    "In knowledge graphs, logical formalisms with well-defined semantics and motivating use cases are used to express constraints on graph data as shapes. The inverse open path support (OPSC) measures the number of entity pairs that satisfy both the head and body predicates in an IOP rule. Our SHACLEARNER algorithm learns high-quality tree shapes from knowledge graphs, which can be extended by contributors using their data-first approach. Furthermore, our sampling method creates customized knowledge graphs for target predicates, improving the quality of embeddings used to learn SHACL shapes. The confidence of a learned rule is determined based on counting the proportion of target predicate instances that satisfy both the body and head in the KG.",
    "The SHACLEARNER algorithm, a novel method for learning high-quality shapes from knowledge graphs, has been adapted from state-of-the-art embedding-based open path rule learners. This approach enables the discovery of complex tree-shaped structures in large datasets. The proposed system learns these shapes by aggregating Inverse Open Path rules and pruning variants to produce more accurate results. Furthermore, a method is presented for building trees out of mined rules, demonstrating their potential as building blocks for even more complex patterns. These findings have significant implications for the verification and validation of knowledge graphs.",
    "The paper introduces Inverse Open Path (IOP) rules, which are used to learn high-quality SHACL shapes from knowledge graphs. These IOP rules can be induced from paths that have a straightforward interpretation as shapes and are employed by the open path rule learner algorithm. The largest proportion of learned SHACL shapes has a cardinality of 1, indicating their simplicity and ease of use. Furthermore, several methods for automatically completing knowledge graphs were proposed to predict missing facts and traverse the graph in a breadth-first manner. Additionally, embedding learners can be used to learn high-quality SHACL shapes from these knowledge graphs.",
    "The knowledge graph's original predicates and facts, including those extended with our new ones, provide a foundation for understanding recursive SHACL shapes. Our pruning techniques help filter out low-quality rules learned from these graphs. Entities are represented as abstract groupings of people, places, or things. The quality of IOP rules is evaluated using measures that assess their effectiveness in predicting instances of SHACL shapes. We introduce Inverse Open Path (IOP) rules to present specific shape patterns over connected entities in knowledge graphs. TreeSC is a novel method for learning high-quality tree shapes from these graphs, used for validating and completing massive KGs. Such constraints on a KG as shapes define rules and patterns for populating the graph.",
    "In this context, Algorithm 1 constructs tree shapes from Inverse Open Path (IOP) rules mined from a Knowledge Graph. DBpedia and Wikidata are examples of real-world massive knowledge graphs that can be used to learn high-quality SHACL trees using the embedding- based method that discovers IOP rules from a KG. The quality of these KGs is crucial, as it affects the accuracy of the learned shapes. Treesupp generates tree shapes from annotated IOP rules and learns SHACL trees from knowledge graphs. Designing an efficient learning method for discovering SHACL shapes from massive KGs while enabling the acquisition of facts about entities that are entirely missing from the KG is a challenging task.",
    "In this study, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes as paths over connected entities in knowledge graphs. We also propose an approach for learning high-quality SHACL shapes from massive knowledge graphs using IOP rules and the Sampling() method. Our results show that our SHACLEARNER based on Algorithm 1 can learn complex tree shapes from large KGs, which is essential for human-KG-completion tasks. Furthermore, we demonstrate the validity of IOP rules in a KG by evaluating their correctness within the knowledge graph's structure, data quality, and constraints. Our findings highlight the importance of learning partially instantiated shapes to avoid an explosion in the space of possible shapes.",
    "SHACLEARNER, a novel machine learning algorithm for learning high-quality shapes from massive knowledge graphs (KGs), has been proposed to address the limitations of existing methods. The distribution refers to the graphical representation or plot showing the proportion of mined rules with respect to their cardinalities. SHACLEARNER learns IOP rules by leveraging Algorithm 2, which is a step-by-step procedure for processing or solving a problem. Furthermore, this method has been shown to be more efficient than brute-force search approaches like 'this method', which cannot handle real-world KGs due to its inefficiency and inability to scale. The proposed methodology aims to generate high-quality rules from knowledge graphs (KGs) by learning IOP rules that express simple shape patterns in the form of paths over connected entities present in a Knowledge Graph.",
    "Our state-of-the-art embedding-based open path rule learner, SHACLEARNER, has a broader term Algorithm 2. The largest proportion of rules have a cardinality of one and are constrained by property shapes that act to constrain an argument of the target predicate. Our shape learning system is based on TreeSC, which determines the confidence of a rule based on counting the proportion of target predicate instances for which the rule holds true in the knowledge graph. The semantics of SHACL language has a broader term formalisms and representation. Inverse open path rules are used to infer new facts by applying computational rules that have a broader term algorithm.",
    "The SHACLEARNER algorithm, which learns high-quality IOP rules from knowledge graphs using heuristic pruning and does not sample. This approach has been applied to learn annotated IOP rules from a sampled YAGO2s dataset. The extended validation framework enables detection of which SHACL shapes could be violated by applying inference rules in Knowledge Graphs. We report the characteristics of discovered trees, including average shape complexity, branch count, and building time. Furthermore, we show that recursive SHACL shapes can be used to validate RDF databases.",
    "In this paper, we propose a novel method for learning high-quality shapes from massive knowledge graphs. Our approach uses Inverse Open Path rules to express constraints on graph data as shapes and derives SHACL shapes through a rule learner called Schimatos. We demonstrate the potential of these kinds of shapes by building trees out of mined rules and show that our method can learn complex shape patterns from large-scale datasets like DBpedia with over 11 million facts.",
    "The DBpedia knowledge graph has a broader term of A knowledge graphs (KG). Various rule languages have a broader term of language. YAGO2s with 4 million facts, KG K', and the small Poker KG all have a broader term of Knowledge Graphs. The IOP version has a broader term of representation. Inverse Open Path rules are pruned by scoring function heuristic for OP rules to mine frequent patterns in a KG. Quality rules define acceptable levels of quality for mined rules. If there exist entities e1, ..., en-21 in the KG such that P1(e, e1), P2(e1, e2), ..., Pn(en-21, e') are facts in the KG, then we can represent these frequent patterns as paths over connected entities present in a Knowledge Graph.",
    "The embedding learners, Algorithm 2, utilize a greedy search to learn high-quality SHACL shapes from knowledge graphs. The IOP rules and representation formalism employed by these algorithms enable the discovery of specific shape patterns as paths over connected entities in the graph. In contrast, other approaches use procedural methods without logical foundations for learning SHACL shapes. Our proposed extended validation framework uses inference processes on the KG to detect which SHACL shapes could be violated, ensuring that the learned rules are accurate and scalable. Furthermore, our pruning techniques, such as Algorithm 2, help filter out low-quality SHACL shapes, allowing for efficient rule discovery in large-scale knowledge graphs.",
    "In this knowledge graph, we see various entities and their relationships. For instance, a human is a citizen of a country, which is an example of Inverse Open Path (IOP) rules. These IOP rules are used to learn SHACL shapes from knowledge graphs by aggregating mined trees. The procedure involves building trees out of mined rules, evaluating the sampled KG, and learning high-quality tree shapes for validation and completion. This method can be applied to public knowledge graphs like Wikidata with 8 million facts or DBpedia, which is a large-scale online database that extracts structured information from Wikipedia and other sources.",
    "Note that, SHACLEARNER learns high-quality shapes from massive knowledge graphs using inverse open path rules. Wikidata, a free online database of structured data, provides information on various topics and serves as an example of such real-world massive KGs. Each KG is defined by the number of target predicates for which we have at least one IOP rule. Our SHACLEARNER based on Algorithm 1 uses pruning variants to reduce the search space for Inverse Open Path rules. The representation formalism they use for their output is a procedural method used by other approaches to learn SHACL shapes from knowledge graphs, which is difficult to compare with the IOP rules presented in this paper. Furthermore, we discuss the application of such trees to KG completion and highlight that our approach Using rules provides a systematic procedure for achieving specific goals or outcomes.",
    "SHACL defines constraints, which are rules or limitations that govern behavior. IOP rules were discovered through experiments and have a broader term of predicates. K' has a broader term of A knowledge graph (KG). The support for an IOP rule is equivalent to corresponding OPSC. We handle this experiment by using the complete KG for learning rules about all target predicates, which includes heuristic pruning in SHACLEARNER. This system does not sample and uses heuristic pruning to learn high-quality IOP rules from a knowledge graph. W3C introduced SHACL as a semantic web language standard used for expressing constraints on graph data.",
    "SHACLEARNER, a state-of-the-art embedding-based open path rule learner, learns high-quality shapes from massive knowledge graphs. By annotating IOP rules with cardinalities and using our sampling method to create customised KGs for target predicates, we achieve better quality of embeddings extracted from sampled KGs. Our four benchmark KGs demonstrate the effectiveness of SHACLEARNER in learning node shapes that have a variety of lengths and cardinalities. The practical meaning of IOPSC and IOPHC qualities is illustrated through experiments on learning SHACL shapes from knowledge graphs.",
    "Our four benchmark KGs, described in Table 1, are used to evaluate SHACLEARNER's ability to learn shapes from massive datasets. Our novel algorithm, our SHACLEARNER, has a broader term of 'algorithm', which systematically solves specific problems or achieves particular goals through automated data processing and manipulation. An adjacency matrix representation of the KG is also represented as Algorithm 1, while the method for aggregating IOP rules into trees shares this same broad term. Furthermore, SHACLEARNER can be categorized under rule learners, each defined by the number of target predicates with at least one Inverse Open Path (IOP) rule, which has a broader term of 'Definition 1'. P1 and P't are specific shapes that have broader terms of 'rules' and 'tree', respectively. A novel approach or technique used to learn SHACL shapes from knowledge graphs is also represented as a method with a broader term of 'approach'. Recursive SHACL shapes, which present the semantics of recursive SHACL shapes, share this same broad term. An IOP rule has a broader term of 'entities' and represents specific paths over connected entities in knowledge graphs. Existing ontologies for KGs have a broader term of 'ontologies', while We introduce Inverse Open Path (IOP) rules shares the same broad term as well. The method for aggregating IOP rules into trees is also represented by Algorithm 1, which has a broader term of 'Algorithm'. Heuristic functions that inform the generation of IOP rules bounded by a maximum length have a broader term of 'rules', while ontology constraint patterns share this same broad term with 'pattern'. Finally, the space of possible shapes has a broader term of 'Shapes' and represents all possible shape patterns that can be learned from knowledge graphs.",
    "In this study, we explore the application of Inverse Open Path (IOP) rules to learn high-quality shapes from knowledge graphs. Our learning method uses an embedding-based open path rule learner adapted for discovering IOP rules from a Knowledge Graph. We demonstrate that our approach can effectively discover diverse and complex shape patterns, including tree-shaped structures. Furthermore, we show that the performance of SHACLEARNER (-S+H) compares favorably to the ideal rule learner (SHACLEARNER -S- H). Our results highlight the potential for IOP rules in learning high-quality shapes from KGs.",
    "The paper presents a novel method to learn high-quality rules from knowledge graphs, which involves learning Inverse Open Path (IOP) rules. The IOP rules are used to validate and edit the graph by discovering frequent patterns in a KG. To achieve this, an embedding learner is employed to learn SHACL shapes from the graph data. These learned shapes can be used for completing or filling gaps in the knowledge graph. Furthermore, pruning variants of these IOP rules are proposed to reduce the search space during learning. The approach has been demonstrated through experiments on DBpedia and other KGs.",
    "In this paper, we propose SHACLEARNER, an algorithm that learns high-quality shapes from massive knowledge graphs. To demonstrate its potential, we employ a greedy search approach and show how it can be used to build trees out of mined rules. Our method builds upon Algorithm 1 and uses the Sampling() method to compute fragments of knowledge graphs. We also discuss the importance of SHACL semantics in defining constraints on graph data as shapes. Furthermore, we highlight the role of IOP rules in aggregating candidate rules for subject and object arguments of each target predicate. The proposed algorithm is evaluated using real-world KG models such as Yago and DBpedia 3.8, which are used to benchmark knowledge graphs.",
    "Formal quality measures for IOP rules are used to evaluate the quality of Inverse Open Path (IOP) rules, which express specific shapes as paths over connected entities within a Knowledge Graph. The semantics of SHACL language define constraints on graph data as shapes using Shapes Constraint Language (SHACL). SHACL shapes represent semantic data modeling patterns or formats that define constraints on RDF data. IOP rules are used to guide the population of a KG, and ontology constraint patterns are used for generating SHACL shapes. We focus on property shapes, which express constraints on graph data as paths of connected entities present in a Knowledge Graph. The computed embedding representations of predicates and entities are used by SHACLEARNER, an algorithm that learns high-quality rules from knowledge graphs. Each target predicate has its own set of IOP rules learned using the SHACL learning problem, which is a challenge to overcome.",
    "To demonstrate, an approach for building complex tree shapes from Inverse Open Path (IOP) rules mined from a Knowledge Graph. The related OP rule has a broader term of OP rules that define paths over connected entities present in the graph. These IOP rules express simple shape patterns and are used to validate and learn high-quality shapes from massive KGs. Furthermore, SHACL shapes learned from knowledge graphs using Inverse Open Path rules can be diverse in structural features such as depth and width, and also in quality measures that indicate confidence and generality. The power of KGs lies in their data-first approach, allowing users to extend and modify them freely.",
    "In this study, we introduce Inverse Open Path rules and propose TreeSC, a novel method to learn high-quality tree shapes from knowledge graphs. Our approach builds trees out of mined rules, which are then used for expressing shape constraints in SHACL. We demonstrate that our proposed system can efficiently learn high- quality shapes from massive real-world knowledge graphs like YAGO2s, Wikidata, and DBpedia. Furthermore, we show the distribution of learned SHACL shapes with respect to their lengths, cardinalities, and qualities, highlighting the effectiveness of pruning variants in reducing search space for Inverse Open Path rules.",
    "In this study, we propose an approach to learn high-quality rules from massive knowledge graphs (KGs) using a novel method. Our learning method, SHACLEARNER, adapts a state-of-the-art embedding-based open path rule learner and demonstrates its effectiveness in discovering accurate shape constraints. The quality of the learnt embeddings used in the scoring function is crucial for evaluating the performance of learned rules. We also show that customising KGs for target predicates can improve the quality of embeddings. Furthermore, we highlight the importance of designing efficient learning methods to handle massive datasets. Our results demonstrate the potential of SHACLEARNER in discovering high-quality shapes from large-scale knowledge graphs.",
    "Massive Knowledge Graphs (KGs) contain numerous entities and relationships, including data. The Inverse Open Path (IOP) version presents specific shapes as paths over connected entities. SHACLearner learns high-quality shapes from KGs by aggregating IOP rules for subject and object arguments of each target predicate. Quality measures similar to those used for OP rules are followed to derive tree shapes, which represent hierarchical structures of connected entities with relationships in a knowledge graph. The numbers of quality rules mined demonstrate the effectiveness of SHACLEARNER's pruning methods. Open path rule learner OPRL is adapted to learn annotated IOP rules from massive KGs.",
    "The SHACLEARNER algorithm, which learns high-quality shapes from knowledge graphs using a state-of-the-art embedding-based open path rule learner. This approach can be augmented with minimum cardinality constraints and used as a building block for more complex shapes like trees. The core of SHACL defines how node and property shapes constrain data in the graph. To prune classes with few instances, we consider only unary predicates having at least one hundred facts. For example, Algorithm 1 invokes PathFinding() to generate candidate Inverse Open Path rules by leveraging predicate embeddings. Closed rules (CR) have a broader term of algorithm, which is also an approach that can be used for inferring new facts in the graph.",
    "Our work proposes a novel method to learn high-quality rules from knowledge graphs (KGs). We use the KG itself to discover shapes, without relying on external modelling artefacts. This approach leverages embedding learners that learn SHACL shapes from KGs by aggregating Inverse Open Path rules and discovering more complex tree-shaped patterns. The proposed methodology is demonstrated through three benchmarks: YAGO2s, Wikidata, and DBpedia. These benchmarks showcase the effectiveness of our method in capturing shapes with varying confidence, length, and cardinality from massive KGs.",
    "In knowledge graph learning, One proposal suggests an extended validation framework to detect which SHACL shapes could be violated. The method for aggregating IOP rules into trees has a broader term Algorithm 2. We follow a style of quality measures similar to those used for OP rules, which are also known as Inverse Open Path (IOP) rules. An adjacency matrix representation of the KG is another way to represent data structures in knowledge graphs. Our pruning techniques filter out lower-quality rules and ensure scalability while maintaining high-quality rule discovery. The effectiveness of our SHACLEARNER algorithm at learning shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs has been evaluated. Large KGs are typically populated by automatic and semi-automatic methods using non-structured sources such as Wikipedia that are prone to errors of omission and commission.",
    "In this study, we propose a predicate calculus formalism for expressing constraints on graph data as shapes. We also introduce SHACLEARNER (-S+H), an algorithm that does not sample and uses heuristic pruning to learn high-quality IOP rules from knowledge graphs. Our approach is designed to capture shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs (KGs). The goal of pruning is to improve the computational performance without missing high- quality rules. We demonstrate that our algorithm can efficiently learn SHACL trees from annotated IOP rules in order to learn high-quality tree shapes for knowledge graph validation and completion.",
    "To learn, which involves deriving SHACL trees from annotated IOP rules using Algorithm 1's greedy search, can be applied to learning. OPRL, an algorithm for learning open path rules from knowledge graphs, has a broader term in rule learners. The shapes that serve as constraints on graph data and validate a knowledge graph have a broader term in entities. Pruning variants are used to reduce the search space for Inverse Open Path rules in SHACLEARNER, which learns high- quality shapes from knowledge graphs. Such path shapes can be built based on IOP rules discovered in experiments. The practical meaning of the IOPSC and IOPHC qualities represents a measure of rule confidence based on target predicate instance proportion (IOPSC) and relevance to the target instances justified by these predicates in the graph data (IOPHC). TreeSC, a novel method for learning high-quality tree shapes from knowledge graphs, is used for validating and completing massive KGs. The system that learns SHACL trees has a broader term in an embedding learner.",
    "In a knowledge graph, SHACL shapes serve as constraints on data and validate RDF databases. These shapes can be used to describe conjunctive paths of constraints over properties, enhanced with minimum cardinality constraints. To demonstrate this concept, we propose an approach that uses IOP rules mined from the KG to build complex tree shapes. This method enables efficient and interpretable human validation in a depth-first manner. Furthermore, our proposed procedure can be used for learning rules about all target predicates without sampling or pruning, considering all facts in the KG. Additionally, this process allows acquiring facts about entities that are entirely missing from the KG while enabling the acquisition of such facts.",
    "In knowledge graph learning, SHACLEARNER uses Inverse Open Path (IOP) rules to discover high-quality shapes. These IOP rules can be augmented with minimum cardinality constraints and used as building blocks for more complex shapes like trees. The algorithm also employs a greedy search approach to iteratively add IOP rules to a tree until the TreeSC drops below a defined threshold or all possible rules are exhausted. Furthermore, SHACLEARNER uses sampling techniques to prune irrelevant data and learns high-quality shapes from knowledge graphs. Its scoring function heuristic for OP rules helps in pruning candidate IOP rules based on their similarity and plausibility.",
    "In this study, we propose an approach to learn high-quality tree shapes from knowledge graphs by building trees out of Inverse Open Path rules mined from the graph. We introduce a novel numerical confidence measure for evaluating the quality of these learned SHACL shapes. Our method can be used to augment existing shape constraints with new ones, ensuring scalability while maintaining high-quality rule discovery. For example, we demonstrate our approach on Wikidata+UP, a large knowledge graph dataset containing 8 million facts. The results show that our proposed method is effective in learning accurate and scalable SHACL shapes from large KGs.",
    "In the realm of knowledge graphs, SHACL shapes play a crucial role in defining constraints on RDF data. A novel method to learn high-quality rules from KGs has been proposed, which involves introducing cardinality constraints and tree shapes for more expressive patterns. This approach is depth-first as it works through a shape tree, ensuring that the learned rules are of the highest quality. The SHACLEARNER algorithm is designed to automate learning of shapes from large knowledge graphs, using a MapReduce implementation to handle extremely large datasets. By leveraging this system, researchers can uncover meaningful relationships between entities and their connections in a KG.",
    "In a Knowledge Graph (KG), entities are grouped together to form paths, which can be represented as open path rules. To prune classes with few instances and pointless learning, we consider only unary predicates with at least one hundred facts. Our SHACLEARNER algorithm learns high-quality shapes from KGs by adding pathfinding algorithms like Add PathFinding(K', Pt, P, A, k). The power of KGs lies in their data-first approach, allowing users to extend and modify them freely. Tree shapes simplify complex relationships between entities, while the left-hand tree in Fig. 3 represents a specific pattern or constraint. Constraints are used to validate consistency within a KG, which can be represented as schema information. IOP rules learned by ideal SHACLEARNER with no pruning serve as building blocks for more complex shapes like trees.",
    "The Sampling() method, which computes fragments of knowledge graphs (KGs), relies on a broader concept - methods. The body predicate is declared as sh:nodeShape, showcasing its connection to shapes and constraints. Unary occupation predicates are part of a larger set of predicates that define relationships in KGs. This method's efficiency can be improved by learning quality IOP rules using Algorithm 2 or open path rule learner OPRL. Dealing with uncertainty adds an extra dimension of challenge but also utility, highlighting the importance of robust systems like SHACLEARNER (-S+H). DBpedia and Wikidata are examples of large-scale online databases that store structured information about entities and their relationships in KGs. Learning paths can be designed to facilitate discovery of shapes, such as trees, which represent hierarchical structures of connected entities with specific types and relationships.",
    "The paper presents two pruning methods, namely prior Sampling and heuristic pruning, used by SHACLEARNER to reduce the search space for IOP rules. The approach provides a sample of data to an off-the-shelf graph structure learner and provides the output in an interactive interface for a human user to create SHACL shapes. Additionally, it discusses the representation formalism they use for their output, which is based on the embedding representation. Furthermore, the paper highlights the characteristics of discovered trees, including results from mined rules with respect to their lengths. The established approach for evaluating KG rule-learnings methods involves measuring both quantity and quality of distinct rules learned. Moreover, it introduces a state-of-the-art embedding-based open path rule learner that learns high-quality shapes for knowledge graphs by adapting a state-of-the-art approach to learn open path rules.",
    "In this paper, we propose an approach to learn high-quality tree shapes from knowledge graphs by building trees out of Inverse Open Path rules mined from the graph. The complexity of the sampling algorithm has a value of O(|K|). We then design an efficient learning method for aggregating IOP rules into trees using heuristic functions that inform the generation of IOP rules bounded by a maximum length. Our approach is based on existing ontologies for KGs and uses YAGO2s with 4 million facts as benchmarks. The statistical likelihood of poor quality rules can be represented through an adjacency matrix representation of the KG, which informs our algorithm to learn SHACL shapes from knowledge graphs. Furthermore, we propose a method for aggregating IOP rules into trees using OPRL, which is informed by PathFinding and subject and object argument embeddings. Our approach has been evaluated on various datasets, including YAGO2s with 4 million facts, and the results are presented in the figures.",
    "In this work, we introduce Inverse Open Path (IOP) rules and show how to build high-quality tree shapes from these IOP rules. We also present a novel method for learning SHACL trees from knowledge graphs using semi-automatic manner: it provides a sample of data to an off-the-shelf graph structure learner and provides the output in an interactive interface for a human user to create SHACL shapes. Our approach, called SHACLEARNER, is based on Algorithm 1 and uses ontology constraint patterns as well as input ontologies. We evaluate our method using Wikidata with 8 million facts and demonstrate its effectiveness in generating high-quality tree-structured data from massive knowledge graphs.",
    "In this work, we focus on learning SHACL shapes from knowledge graphs. For example, consider a scenario where we want to compute IOPSC and IOPHC quality measures for an inverse open path rule in the context of learning SHACL shapes from real-world KGs. Using rules found in experiments, we can leverage self-loop links for unary predicates as convenient syntactic sugar to keep presentations in triple format. Our approach involves building a knowledge graph (KG) using a method that is used to build a KG and then applying an embedding learner to learn high-quality SHACL shapes from the KG. This allows us to represent complex patterns, such as nested structures resembling tree-like shapes, which can be useful for tasks like pathfinding of algorithm 1. As IOPSC increases, conditions become more stringent, requiring entities associated with any predicate within distance l to meet specific criteria.",
    "Learning SHACL shapes from schema-free knowledge bases without logical foundations has been shown to be non-scalable for handling large-scale datasets. To address this challenge, we propose an approach that combines exploratory attempts at learning tree shapes with a graph structure learner. Our method involves aggregating IOP rules for the subject and object arguments of each target predicate using a greedy search. We demonstrate the effectiveness of our algorithm on five hundred poker hands and all facts related to them, achieving high OPSC scores. Furthermore, we show that our approach can be used to generate new facts by learning SHACL shapes from public knowledge graphs.",
    "The SHACLearner algorithm learns high-quality shapes from knowledge graphs, leveraging IOP rules with up to l atoms. These rules connect entities satisfying a subject predicate to those forming an object predicate along a path of predicates. The system can be further pruned by applying stricter quality thresholds later on. In its reverse form, the target predicate serves as a connection between nodes in node shapes that constrain it. SHACLearner differs from OPRL evaluation module and is part of Algorithm 2, which conducts experiments to evaluate IOP rules with up to l atoms. The closed path forms formalism represents specific shape patterns expressed as paths over connected entities present in knowledge graphs.",
    "In knowledge graphs, Inverse Open Path (IOP) rules are used to define relationships between entities. These rules can be learned from massive datasets like Wikidata and DBpedia using the SHACLEARNER algorithm. The quality of these rules is measured by their support, confidence, and head coverage. A greedy search procedure is employed to iteratively add IOP rules to a tree structure until a threshold is reached or all possible rules are exhausted. This approach has been shown to be effective in transforming knowledge graphs into common representations for experimentation.",
    "In this study, we demonstrate how to construct high-quality tree shapes from Inverse Open Path (IOP) rules. Our approach involves aggregating IOP rules into trees using a novel method that combines predicates Pt(x, y), P1(y, z), and P2(z, t). We show that our SHACLEARNER algorithm can effectively capture complex patterns with varying confidence, length, and cardinality from massive knowledge graphs (KGs) in real-world scenarios. Furthermore, we highlight the importance of learning IOP rules without sampling or pruning, considering all target predicates. The next challenge is designing an efficient learning method for discovering SHACL shapes from KGs, taking into account uncertainty in constraints and the learning process.",
    "The proposed system, an algorithm designed to automate learning of shapes from large knowledge graphs. In comparison with the ideal rule learner (SHACLEARNER (-S-H)), our SHACLEARNER learns high-quality rules by defining schema information for KGs stored as an RDF graph and computing quality measures such as OPSC. The effectiveness of our SHACLEARNER is evaluated through a small amount of data, demonstrating its ability to learn shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs. Furthermore, we define formal quality measures for IOP rules, which are inverse open path rules that present specific shape patterns in the form of paths over connected entities present in a Knowledge Graph.",
    "Our experiments demonstrate the effectiveness of SHACLEARNER at capturing shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs (KGs). The algorithm learns to express not only the necessity but also the number of different chains of facts that should exist. To achieve this, it deals with uncertainty in both constraint definitions and the learning process. SHACLEARNER uses a shape learning system to recognize patterns and relationships between entities, which are then used to complete gaps in the KG through efficient matrix computations. The algorithm's quality is measured by its ability to learn IOP rules that capture shapes of varying lengths.",
    "The DBpedia 3.8 knowledge graph dataset contains approximately 11 million facts, which can be represented as a set of entities and their relationships using Knowledge Graphs. SHACLEARNER algorithm learns open path rules from these graphs by discovering trees for each argument of target predicates. The closed path forms representation is used to validate and maintain large-scale graph databases. Entities with only type or class declarations are triggered by the shapes, which define schema information for KGs stored as RDF graphs. OPRL system uses HOLE and RESCAL algorithms to learn embeddings in knowledge graphs. Pathq represents a sequence of paths or branches in tree shape, characterizing frequent patterns that can be used for data cleaning or ongoing entry. Automatic and semi-automatic methods are employed to populate large KGs from non-structured sources like Wikipedia.",
    "The two type-like predicates from DBpedia 3.8 and one from Wikidata are used to generate unary predicates and facts for learning SHACLEARNER, a novel machine learning algorithm that learns high-quality shapes from knowledge graphs like YAGO2s with massive KGs. The proposed method efficiently evaluates IOP rules and trees by exactly computing the quality measures of each rule using fast matrix and vector operations. This system provides an interactive interface for human users to create SHACL shapes in semi-automatic manner, which can be used for subsequent data cleaning or ongoing data entry as frequent patterns that characterize a KG.",
    "In knowledge graphs, pruning methods can cause it to fail to discover more rules of lower quality. To address this issue, many cases require a broader term through an example. The predicate P1( e, e1), P2(e1, e2), ..., Pt(e(q-1)', eq) are facts in the KG that have been mined using SHACLEARNER's Add PathFinding(K', Pt, P, A, k). This approach has led to the discovery of OP rules express simple shape patterns. The effectiveness of our SHACLEARNER algorithm at learning shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs is promising. Furthermore, manual knowledge graph completion tasks can be facilitated by using mined shapes. We propose a predicate calculus formalism as an alternative to traditional approaches.",
    "The open path rule learner OPRL uses an approach to learn rules from knowledge graphs. The proposed system utilizes a method that involves learning shapes and rules from existing KGs. Inverse Open Path (IOP) rules are used, which have a body and head satisfied in the Knowledge Graph. This may be because using the complete KG for learning rules about all target predicates could harm the quality of learnt embeddings used in the scoring function of SHACLEARNER. The efficient matrix-computation is used to calculate the quality measures IOPSC and IOPHC. PathFinding of Algorithm 1 reduces the search space for Inverse Open Path rules using prior sampling and heuristic pruning.",
    "The Sampling() method, which computes a fragment of a knowledge graph (KG) K', relies on procedure. Node shapes are constrained by rules that govern behavior and usage. The core of SHACL defines how node and property shapes constrain data in KGs. Entities serve as the building block for expressing various types of shape constraints. Exploratory attempts to learn high-quality shapes from KGs involve graph structure learners, such as Algorithm 2. IOP rules are used to represent predicates that express relationships between entities. The Shapes Constraint Language (SHACL) ensures a knowledge graph conforms to its defined schema and constraints.",
    "In this paper, we illustrate the computation of IOPSC and IOPHC for evaluating SHACLearner's learned rules from knowledge graphs. We demonstrate how to prune candidate rules using automatic and semi-automatic methods without missing high-quality rules. Furthermore, we show that learning tree shapes in SHACL can be used to define support and standard confidence measures for trees, which is essential for discovering high- quality rules from extremely massive KGs with tens of billions of facts, such as the most recent version of Wikidata.",
    "In this study, we propose an efficient method to evaluate IOP rules and trees by exactly computing the quality measures of each rule using fast matrix and vector operations. This approach can be applied to learn partially instantiated shapes from knowledge graphs (KGs), which are represented as sets of square adjacency matrices. The proposed method is particularly useful for learning complex patterns, such as those with varying complexity and diversity in terms of length and cardinality. Furthermore, we show that the support for an IOP rule is equivalent to its corresponding OPSC measure, indicating a strong connection between these two concepts.",
    "In this study, we show how to build high-quality tree shapes from inverse open path rules. We demonstrate a procedure for constructing these shapes using IOP rules and illustrate its application on many knowledge graphs. The head of each rule instantiates an instantiation of the body in the KG. Our approach uses self-loop links for unary predicates as convenient syntactic sugar, keeping presentations in triple format. SHACLEARNER learns high-quality rules meeting minimum quality thresholds from massive knowledge graphs. This completion strategy is depth-first and works through a shape tree. An editor called Schimatos manually completes information gaps in the KG using learned SHACL shapes.",
    "The proposed system, which utilizes an approach to automate learning of shapes from large knowledge graphs, relies on a set of rules that satisfy specific conditions. These rules are quality measures such as IOP Standard Confidence, IOP Head Coverage, and Cardinality, which augment the rules. The first rule indicates that music entities belong to albums with specific record labels. Inverse Open Path Support (IOPSC) is used to validate knowledge graphs, while type predicates for experiments enable learning of shapes from real-world data. An embedding learner learns high-quality SHACL shapes by using embeddings and heuristic functions. Definition 6 defines tree shapes in SHACL that quantify support and standard confidence measures for trees. Unifying variables across multiple branches simplifies the structure of learned tree shapes. The row index of elements with value >= Car of V^1(A(P1)*A(P2)*...*A(Pm)) represents entities satisfying both conditions: being an argument in a rule's body and also satisfying its head, given a minimum cardinality threshold (Car). Their data-first approach enables contributors to extend knowledge graphs. Now, the procedure for deriving SHACL trees from annotated Inverse Open Path rules begins. The methodology involves validating KGs using CP rules.",
    "The branching structure of learned tree shapes, represented by the proportion of IOP rules aggregated into trees, has a broader term as data. The depth-first strategy for human-KG-completion works through shape trees using an algorithm that learns ideal SHACLEARNER (-S-H) with no pruning. A human expert helps categorize or represent these trees based on their physical forms or structures, known as Tree shapes. The predicates P1, P2, and Pt define specific rules used to validate and edit large-scale data-first graph databases. These rules are part of a broader set of constraints expressed in the Shapes Constraint Language (SHACL), which is itself a standard language for defining constraints on graph data as shapes. Existing ontologies for KGs provide syntax guidelines that govern the structure of this language, allowing it to be used effectively by algorithms like RESCAL and Algorithm 2. The size of the problem posed by learning SHACL shapes from large-scale knowledge graphs can be addressed using learned rules, which are acquired through experience or training.",
    "The process of computing quality measures (IOPSC, IOPHC) for a given SHACLearner rule from the adjacency matrices representing knowledge graphs. This approach involves learning shapes with different complexities, including nested patterns that are similar to trees. The largest proportion of rules has a cardinality of one, and weak inference rules can be used to draw conclusions about entities in public knowledge graphs. Moreover, mined rules can be extracted or discovered using algorithms like HOLE and RESCAL from schema-free databases such as KGs.",
    "PathFinding() leverages embedding representations of predicates to generate candidate Inverse Open Path (IOP) rules. These two pruning methods, namely prior Sampling and heuristic pruning, are used by SHACLEARNER to reduce the search space for IOP rules. A branch represents a path or sequence of predicates and nodes in a tree shape, characterizing specific patterns within knowledge graphs. As frequent patterns, shapes can be used for data cleaning or ongoing entry. Table 2 contains numerical information about rules, quality metrics, and running times. The number of rules increases as the space of possible rules grows. SHACLEARNER is an algorithm that learns high-quality shapes from knowledge graphs in a semi-automatic manner. It provides a sample of data to an off-the-shelf graph structure learner and offers an interactive interface for creating SHACL shapes.",
    "The RESCAL algorithm learns embeddings from knowledge graphs. The process of validating these knowledge graphs involves data type validation and human-kg-completion, which requires entities to be represented as vectors in a high-dimensional space. These unary occupation predicates are connected by the predicate 'in' or 'learn', indicating relationships between entities. SHACLEARNER, Ev is responsible for evaluating IOP rules and filtering potential ones based on their quality measures such as OPSC and IOPSC. The challenge lies in formalising approaches to shape mining without essential shape quality measures. A knowledge graph (KG) represents a collection of facts, which can be aggregated into trees using the method for aggregating IOP rules. Experiments have discovered high-quality IOP rules that describe specific shape patterns present in KGs.",
    "In this paper, we introduce Inverse Open Path (IOP) rules as a predicate logic formalism presenting specific shapes as paths over connected entities in knowledge graphs. We demonstrate that IOP rules can be used to learn SHACL shapes from massive knowledge graphs using Algorithm 1 and prior sampling techniques. Our evaluation shows the effectiveness of our approach on two benchmarks, YAGO2s with 4 million facts and DBpedia 3.8 with 11 million facts. Furthermore, we show how OPRL can be applied to validate and maintain large-scale graph databases by representing specific shape patterns as closed path forms.",
    "In this experiment, we used predicate logic formalism to present specific shapes as paths over connected entities in a Knowledge Graph. The reference r1 represents an Inverse Open Path (IOP) rule that expresses a shape pattern for completing knowledge graphs. We also discovered IOP rules and validated the accuracy of these rules using SHACLEARNER, which is a novel machine learning algorithm for learning high-quality shapes from knowledge graphs. Furthermore, we applied efficient matrix-computation to calculate quality measures such as open path head coverage and inverse open path support. Our approach involves representing entities in multidimensional relational database schemes and inferring candidate shapes based on these representations.",
    "In knowledge graphs, discovering shapes that capture complex relationships between entities is crucial. To achieve this, we propose an Inverse Open Path (IOP) rule language that allows us to mine rules with open variables. This approach enables learning of SHACL shapes from knowledge graphs, which can be used for populating large knowledge graphs and validating data quality. Furthermore, we introduce a graph structure learner algorithm that learns tree-shaped patterns in knowledge graphs, allowing for the discovery of complex relationships between entities. The expressivity of these learned shapes is evaluated using various quality measures, including IOPHC, confidence TreeSCorig, and OP. Our approach demonstrates the potential to automatically generate high-quality SHACL shapes from large-scale knowledge graphs.",
    "In knowledge graphs, OP rules are used to define entities and their relationships. For instance, we can observe an expected decrease with greater cardinalities as they demand tighter restrictions to be satisfied. Another work proposes a framework for defining SHACL shapes with varying levels of complexity, featuring nested patterns comparable to the tree structures used in this work. Algorithm 1 evaluates candidate rules using HOLE and RESCAL methods. The confidence measure of a tree shape learned by SHACLEARNER indicates its quality and generality in predicting instances from knowledge graphs. Quality measures are essential for evaluating IOP rules and trees, which can be computed efficiently using fast matrix and vector operations. In addition, unary predicates that can act as class or type constraints play a crucial role in defining shapes and patterns. Furthermore, SHACL advanced features provide sophisticated capabilities to enhance the standard features of Shapes Constraint Language (SHACL).",
    "In predicate logic formalism, inverse open path rules (IOP) express specific shapes as paths over connected entities present in knowledge graphs. We define formal quality measures for IOP rules to evaluate their effectiveness. To reduce complexity and prune irrelevant data, we employ sampling techniques like Algorithm 2. The pairs of entities satisfying the head are connected by a path P1, P2, ..., Pm, which can be represented as shapes or patterns in knowledge graphs. These shapes can take various forms, such as trees or paths with specific variable binding patterns. By aggregating mined IOP rules using GreedySearch, we can learn and combine previously extracted inference optimization problem (IOP) rules to achieve better rule quality. The standard confidence level for inverse open path rules is measured by Inverse Open Path Standard Confidence (IOPSC). Furthermore, we use self-loop links for unary predicates as a convenient syntactic sugar to keep the presentation in triple format.",
    "The proposed system learns SHACL shapes from knowledge graphs using IOP rules, which are pruned by a scoring function heuristic for OP rules. The learned shapes can be used to complete gaps in the graph and improve its quality. This approach has been shown to be effective in learning novel numerical confidence measures that quantify the reliability of learned SHACL shapes. Furthermore, it is sensitive to the maximum length of rules and uses inverse open path head coverage (IOPHC) as a measure of rule relevance. The system can also handle large-scale knowledge graphs like DBpedia and Wikidata.",
    "In a knowledge graph, constraints govern behavior and usage. The ideal rule learner (SHACLEARNER (-S-H)) generates rules without sampling or pruning. YAGO2s serves as benchmarks for evaluating the quality of these rules. SHACL Core features define node and property shapes, while rules and constraints shape learning. Rule quality is measured by Inverse Open Path Head Coverage (IOPHC), which assesses rule relevance to target predicates. The sequence of paths forms a tree structure in SHACL. Entities less relevant to targets are pruned from the graph. IOPsupp measures support for inverse open path rules, and approach determines how to construct these trees. Minimum quality thresholds ensure acceptable data. A knowledge graph (A KG) serves as schema-free database, while entities represent abstract concepts or groups. CP rules facilitate computational processing and inference. These novel quality measures evaluate candidate rules on a small sampled knowledge graph.",
    "The rule-based learning system, RLvLR, uses representation to organize and describe knowledge. It can also be used with predicates like P1(z0, z1) that present specific shapes over connected entities in a Knowledge Graph. The process of completing missing facts for a target predicate is known as KG-completion, which involves unifying some variables that occur in multiple branches. This process can be achieved through Algorithm 2 or ideal, modified SHACLEARNER (-S-H). Inverse Open Path rules (IOPs) are used to complete missing facts and have quality measures like IOPSC and IOPHC. The distribution of mined rules is a collection of extracted and organized principles that dictate specific behavior. These rules can be transformed from OP to CP through straightforward variable unification, demonstrating the power of representation in knowledge graphs.",
    "The massive knowledge graph, Wikidata with 8 million facts, contains information about various entities and their relationships. A tree represents a hierarchical structure of connected entities with specific types and relationships. The size of the problem significantly affects learning partially instantiated shapes from KG data. For each predicate P_k, there are rules that express constraints on graph data and validate knowledge graphs. RESCAL is an algorithm used to learn embeddings for SHACL shapes. Learning partially instantiated shapes involves aggregating IOP rules for subject and object arguments using a greedy search. The maximum length of rules affects the complexity of learning SHACL shapes from KG data. Human knowledge can be represented as KG data, which contains structured information about entities and their relationships. PathFinding is a method that generates candidate Inverse Open Path (IOP) rules by leveraging embedding representations of predicates involved in each rule.",
    "In a knowledge graph (KG), entities are connected by predicates, forming relationships that can be represented as embeddings. The RESCAL algorithm learns these embeddings to capture properties and relationships between entities. SHACLEARNER uses this approach to learn high-quality rules from massive KGs, pruning methods like prior Sampling and heuristic pruning help reduce the search space for IOP rules. To quantify the quality of a KG, measures such as IOPSC and IOPHC are used. The head of a tree shape in SHACL represents the sequence of connected entities and predicates that form its structure.",
    "In knowledge discovery and artificial intelligence applications, mined rules are used to extract or learn patterns from data. These rules can be instantiated as specific entities within a knowledge graph (KG), such as an instantiation of the body of the rule in the KG. The quality of these rules is measured by Inverse Open Path Standard Confidence (IOPSC) values and other quality measures like IOP Head Coverage and Cardinality, which augment the rules. By aggregating mined IOP rules using a state-of-the-art embedding-based open path rule learner, we can produce annotated IOP rules that complete missing facts for target predicates and related predicates in the KG. This approach is useful not only for traditional knowledge graph completion but also for more advanced applications like completing missing data or acquiring new information about entities with incomplete or absent data.",
    "The established approach has a broader term 'approach', which refers to a systematic method or procedure for achieving a goal. A knowledge graph (KG) also falls under this category, being defined as a data structure that represents relationships between entities, concepts, or terms as a graph of interconnected nodes. Rule quality measures IOP Standard Confidence, IOP Head Coverage, and Cardinality augment the rules by quantifying their confidence, head coverage, and cardinality. Every entity-instantiation of a CP rule is also an entity-instantiation of the related OP rule, but not vice versa. The system without sampling, but with the heuristic rule learning module learns potential rules from a knowledge graph. These learned rules are evaluated based on various feature values such as IOPSC, IOPHC, cardinality and length, which provide insights into their quality metrics.",
    "In this study, we propose novel tree shapes for representing Inverse Open Path (IOP) rules. Our approach leverages SHACL trees to capture high-quality shapes from IOP rules. We demonstrate how to build these shapes using Algorithm 1 and validate a knowledge graph with SHACL advanced features. Furthermore, our experiments show that this method can be used for KG-completion under the guidance of human experts. The results highlight the effectiveness of our approach in capturing diverse tree structures from real-world massive knowledge graphs.",
    "Closed-path rules are a type of algorithm that can be used to learn novel tree shapes from knowledge graphs. These rules consist of head and body, forming a closed path or single unbroken loop of links between variables. The quality of these rules can be evaluated based on their ability to generalise well and explain many facts. Inverse Open Path (IOP) rules are another type of algorithm that can be used more traditionally to complete missing facts for target predicates. These IOP rules have been shown to learn novel tree shapes, including those with constraints defined by Feature Generalization Diagrams (FGDs). The quantifiers in these rules refer to a set of logical operators used to express constraints on graph data as shapes. Furthermore, the representation of types and classes as unary predicates allows for learning fully abstracted shapes instead of partially instantiated ones.",
    "The target predicate defines specific shapes as paths over connected entities in knowledge graphs. Most rules have a cardinality of one, indicating that they are highly constrained and precise. Algorithm 1 uses Tree: Treesupp, TreeSC to support subtree and node operations. Experiments assess the effectiveness of SHACLEARNER at capturing shapes with varying confidence, length, and cardinality from massive knowledge graphs. The formalism of constraints governs the acquisition and representation of knowledge in schema-free knowledge bases. In Embeddings computes predicate, subject, and object argument embeddings using RESCAL algorithm. Parameters define boundaries for knowledge graph storage as RDF graphs. Definition 3 outlines a specific idea or principle. Candidate rules are guidelines used to determine outcomes or make decisions. Algorithm 2 is a step-by-step procedure for processing or solving problems. To assess evaluates the effectiveness of Inverse Open Path (IOP) rules. The example illustrates computing quality measures such as IOPSC and IOPHC over knowledge graphs.",
    "In a knowledge graph, entities are connected by relationships represented as triples. For instance, subject and object argument embeddings have a broader term of object argument embeddings, indicating their semantic relationship. Atomic bodies can be seen as single entities that satisfy conditions specified by Inverse Open Path (IOP) rules, which correspond to SHACL shapes. The minimum threshold value for Tree Standard Confidence (TreeSCMIN) is used in IOP rule construction. This simple approach reduces the complexity of learning SHACL shapes from knowledge graphs by sampling a subgraph related to a target predicate. A schema-free database like DBpedia 3.8 contains approximately 11 million facts, which can be represented as trees or hierarchical structures of connected entities with specific types and relationships.",
    "In this knowledge graph, we see various entities and their relationships. The 'variables' entity is connected to 'KG data', indicating that it represents values within a collection of structured information. Similarly, 'Shapes' are related to 'schema information', suggesting that they define the structure or form of something. We also find connections between 'closed path rules' and 'entities', implying constraints on relationships between entities. Furthermore, we see 'YAGO2s' as target predicates for 'binary predicates', indicating a type of logical operator used in knowledge graphs. The graph also reveals relationships between 'Path2' and 'branch', suggesting that Path2 represents a sequence of predicates and entities satisfying certain conditions.",
    "SHACLEARNER, an algorithm for learning open path rules from knowledge graphs, has been shown to discover complex shapes that generalise well and explain many facts. The quality measures learned by SHACLearner indicate confidence and generality in the discovered trees. Inverse Open Path (IOP) rules are used to connect entities satisfying specific subject and object arguments through a chain of predicates. Closed-path (CP) rules, on the other hand, cannot handle unary predicates. RESCAL is employed for embedding each entity into a vector space and each predicate into a matrix. The process of identifying unique entities that satisfy a given rule or pattern involves sampling from knowledge graphs to construct SHACL trees.",
    "In this study, we learn shapes and representations. The Inverse Open Path Support (IOPSC) measures the quality of IOP rules. We also define schema information for knowledge graphs stored as RDF graphs. Our algorithm learns shape constraints from these graphs. It uses a greedy search to iteratively add IOP rules until the TreeSC drops below a defined threshold or all possible rules are exhausted. The results show that most learned SHACL shapes have a cardinality of 1, indicating high-quality rules. We also demonstrate the relationship between logical statements and embedding spaces.",
    "In knowledge graphs, paths are sequences of predicates connected by closed intermediate variables and terminating with open variables at both ends. These path shapes can be used to represent various features such as representation, feature values, or entities. Learning methods like Algorithm 1 and RESCAL can learn embeddings from these graph structures. For example, a procedure called HOLE and RESCAL combines hierarchical representation learning with relational embedding constrained linearizations. The quality of learned rules is evaluated using metrics such as OPHC, which measures the proportion of entities that satisfy both the body and head of an open path rule.",
    "In knowledge graph completion tasks, HOLE and Tree shapes are used to represent entities. A fact can be represented as a rule that states a person has at least one father. OP rules imply the existence of facts like spouse(x,y) -> child(x,z). SHACLEARNER uses two pruning methods to reduce search space for IOP rules. The confidence measure, TreeSCorig, is used to evaluate tree shapes in knowledge graphs. To compute subject and object argument embeddings of a predicate P_k involves aggregating entity representations from the graph. Certain properties in the embedding space are related to logical statements of rules. Wikidata provides information on various topics including geometric forms like an example shape. Rules learned with respect to features IOPSC, IOPHC, cardinality, length, and also IOPSC vs length can be used for inference. Unlike CP rules, OP rules do not necessarily form a loop.",
    "This paper investigates the application of SHACL shapes to validate RDF databases. RESCAL, a machine learning algorithm, embeds each predicate P_k into a matrix P_k in R^(d x d). Its advanced features include rules and constraints for modeling diverse shapes. To assess the effectiveness of Inverse Open Path (IOP) rules, we compute their embeddings after sampling from knowledge graphs. The IOP Standard Confidence measure quantifies the quality of these rules by dividing their support by the number of entities that can instantiate the body part. We also represent K as a set of square n x n adjacency matrices and define schema information for KGs stored as an RDF graph. Furthermore, we investigate the application of SHACL shapes to validate RDF databases using novel numerical confidence measures. The results show that GreedySearch is effective in finding high-quality shapes from knowledge graphs.",
    "The Cardinality of an IOP rule, Car, represents the number or quantity of inputs and outputs allowed for a specific process. HC measures the explanatory power of rules, while Algorithm 2 provides a step-by-step procedure to solve problems. This property ensures that SHACLEARNER does not miss any high-quality IOP rules, improving computational performance without compromising rule quality. The learnt rules are acquired through experience or learning and represent guidelines for decision-making processes. A sequence is used to organize entities in computer science. We sum each column and transpose to obtain the vector V^2(Pt), which calculates the proportion of entities that can instantiate a rule's body to satisfy its head, known as inverse open path standard confidence. Parameters define the boundaries or limits within which systems operate. All facts related to them are statements asserting information about specific objects or concepts. IOPHC is a quality measure for IOP rules, while entities represent abstract representations of people, places, things, or concepts. PathFinding provides a computational method for finding paths between points or locations. The experiments evaluate SHACLEARNER on massive knowledge graphs such as YAGO2s and DBpedia 3.8. One presentation presents the semantics of recursive SHACL shapes. Breadth-first manner traverses data structures by visiting all nodes at a given level before moving to the next level. Using self-loop links maintains consistency in triple format, while rules and shapes govern geometric entity formation.",
    "The use of Open Path (OP) rules implies the existence of facts, such as spouse(x, y) \u2192 \u2203z child(x, z). Additional constraints are imposed on systems to modify their behavior. Rules and algorithms govern decision-making processes. The formalism of constraints determines the scope of information that can be learned from schema-free knowledge bases. We define quality measures for Inverse Open Path (IOP) rules, which involve scoring functions. Learning paths facilitate skill acquisition. Additional head predicates are included in IOP rules to affect their support and standard confidence measures. Our novel tree shapes represent a visual representation of graphs as shapes.",
    "The concept of k-cliques and entities serves as a foundation for understanding complex relationships between knowledge graphs. The existential quantifier 'exists(zi, ..., z(n-1), y)' represents a path of connected entities, while universal quantification enables expression of constraints on graph data. Rule learning methods, such as Algorithm 1, can be used to acquire rules from massive knowledge graphs like YAGO2s and DBpedia. The experiments conducted demonstrate the effectiveness of these approaches in discovering new facts. Furthermore, Horn shapes are used to validate and edit the graph, while closed rules (CRs) infer new facts based on patterns found in the data. Inverse Open Path Support (IOPSC) measures the quality of IOP rules, which can be used traditionally to complete missing facts for target predicates. A good learning algorithm is essential for efficiently acquiring knowledge from large datasets.",
    "In this knowledge graph, we can see that 'a tree' has a broader term of 'tree', which represents its hierarchical structure. The sampled KG contains entities with certain properties, including variables and embeddings. We call SHACLEARNER to learn high-quality shapes from these graphs. The support for an IOP rule measures the quality of this learning process. After sampling, we compute predicate embeddings, subject and object argument embeddings for all predicates in a sampled knowledge graph. Schema constraints ensure data validation and consistency. This feature not only grows exponentially with the maximum number of predicates but also has reassuring properties that improve computational performance without compromising rule quality. Furthermore, this completion strategy is depth-first as it works through a shape tree to achieve its goal.",
    "The RESCAL algorithm embeds entities and predicates into vector spaces, allowing for efficient computation of relationships between them. This approach has a broader term in syntax, which defines the rules governing structured data paths. The OP rule provides guidelines for decision-making, while pruning methods reduce unnecessary computations. Constraints govern behavior, and trees support subtree operations. In Embeddings, procedure is used to compute predicate embeddings from knowledge graphs. Standard confidence measures satisfaction of inferred head instances. The whole path represents a composite predicate formed by chaining predicates along a path. Definition 3 outlines the proportion of entity pairs that satisfy rule bodies in the KG. SHACL shapes require songs to have at least one producer, and candidate rules are used for decision-making.",
    "In knowledge graph processing, finding path shapes and Closed-path rules are crucial steps. To achieve this, one can employ an approach that involves representation of entities as paths connected by predicates. This process can be facilitated through algorithmic methods such as association rule mining or RESCAL to embed each entity into a vector space. Furthermore, the performance of SHACLEARNER (-S+H) and IOP Head Coverage metrics are essential in evaluating the quality of Inverse Open Path rules. Through concrete examples, one can illustrate how existential quantifiers like exists(z1, ..., z(n-1), y) capture constraints on graph data as shapes.",
    "In this knowledge graph, we have entities such as adjacency matrices representing predicate P1, non-recursive rules for processing data, and logical statements that connect entities satisfying subject predicates to those forming object predicates. The RESCAL algorithm embeds each entity into a vector space and each predicate into a matrix representation. We also see quality rules defining acceptable levels of quality, pruning methods reducing unnecessary parts of the system, and inverse open path head coverage (IOPHC) measuring IOP rule quality. Furthermore, we have entities satisfying both conditions, quantifiers expressing constraints on graph data as shapes, and predicates connected by closed intermediate variables.",
    "The entities satisfying both conditions are those that meet specific requirements, as calculated through matrix operations on an adjacency representation of a knowledge graph. Association rule mining and rule mining processes involve discovering patterns and relationships between variables by identifying rules with significant predictive power. The condition for a pair of entities (e, e') to satisfy the body of an open path rule r is when there exist entities e1, ..., en-1 in the KG such that P1(e, e1), P2(e1, e2), ..., Pn(en-1, e') are facts in the KG. The procedure for calculating quality measures like IOPSC and IOPHC involves considering recursive SHACL shapes and their advanced features, including self-loop links and input ontologies. Furthermore, RESCAL is used to embed each entity e_i to a vector E_i in R^d and each predicate P_k to a matrix P_k in R^(d x d), providing insights into the representation of entities and relationships.",
    "In knowledge graph rule learning, inverse open path (IOP) rules play a crucial role. These rules are defined by logical statements that govern the existence or non-existence of entities. The quality of these IOP rules can be measured using various metrics such as cover and rule quality measures. To evaluate the plausibility of an entity-free IOP rule, we define formal quality measures for IOP rules. In this context, schema constraints are essential to ensure data validation and consistency. Description Logic ontologies provide a framework for representing knowledge using logical operators. The ideal approach is to generate all possible rules up to the maximum length parameter without sampling or heuristic pruning. This experiment demonstrates how annotated IOP rules can be mined from DBpedia and evaluated based on their quality measures.",
    "In this study, we select 50 target predicates to learn SHACL shapes from knowledge graphs. A rule is considered closed when all variables have occurred at least twice. We employ a learning method, Algorithm 2, which uses RESCAL for embedding representations of entities and rules. The open path support (OPsupp) measures the number of pairs of entities that satisfy both the body and head of an IOP rule. Our approach involves sampling methods to learn rules from knowledge graphs. A matrix representation is used to describe predicate Pt in the graph, indicating a fact P(t)(e3, e2). We also consider learned rules as algorithms for solving specific problems. The shape of Fig. 2 represents the geometric form of diagrammatic representations. Our scoring function assigns scores based on entity relationships and properties. Along a path of entities that satisfy a chain of predicates in an IOP rule, we use predicates to express relationships between entities. We assess the effectiveness of our approach by evaluating its ability to cover facts satisfying specific conditions.",
    "The self-loops of method can be seen as a type of edge or connection within a graph that connects an element to itself. The running time for aggregating IOP rules into trees is significantly lower than the initial IOP mining time by a factor greater than 10, resulting in results that are more efficient and effective. A completion strategy like Algorithm 1 can be used to achieve this efficiency gain. In addition, predicates such as unary predicates that can act as class or type constraints play an important role in shaping our understanding of the world. OP rules, for instance, satisfy certain conditions represented by mathematical expressions like (e', e). Furthermore, measures like HC and representation are crucial in evaluating the quality of these shapes. Moreover, schema constraints can be applied to correct errors that may arise from applying IOP- shaped constraint. The DBpedia database contains a vast amount of predicates that can be used for knowledge graph completion. In conclusion, by combining methods such as GreedySearch with algorithms like Algorithm 1 and using measures like quality measures, we can efficiently detect which shapes could be violated by applying rules.",
    "The knowledge graph contains various entities, including Sampling(), Embeddings(), and Algorithm 1. These entities are connected by predicates such as has a broader term, compute, and arises from. For example, Embeddings() computes argument embeddings, while P3 represents a fact in the knowledge graph. The predicate is used to connect entities satisfying its subject predicate to entities forming an object predicate along a path of predicates. CP rules provide a set of logical rules for computational processing and inference. KG documentation serves as a written or digital record of knowledge that can be shared with others. HC assesses the explanatory power of concepts, rules, or theories. The adjacency matrices represent a knowledge graph, where each matrix corresponds to a predicate and indicates whether there exists an edge between entities. All entities in the sampled KG are either directly related to the target predicate or close neighbours of directly related entities.",
    "The This sampling method computes fragments of knowledge graphs, consisting of bounded entities related to target predicates. Building blocks for more complex trees are approaches that use features as important since learning partially instantiated Shapes. Inverse open path standard confidence (IOPSC) measures the proportion of target predicate instances held true in a knowledge graph. PathFinding algorithms like Algorithm 2 traverse paths breadth-first manner, pruning techniques reducing complexity. A path is a sequence of predicates connected by closed intermediate variables terminating with open variables at both ends. Entities satisfying conditions are identified through inference rules and matrix operations on adjacency representations. Knowledge panels summarize feature values for entities.",
    "In this paper, we propose a novel method to learn high-quality rules from Knowledge Graphs (KGs). The limitation that SHACLearner does not perform any kind of data type validation refers to its inability to validate or check the types of data used in a KG. Our novel tree shapes are designed to capture complex relationships between entities and predicates, allowing for more accurate rule discovery. By pruning less relevant entities and predicates from the sampled graph, we can improve the quality of our learned rules. Furthermore, by defining constraints on graph data, such as the type of entities involved or the number of connections between them, we can ensure that our rules are well-defined and meaningful. Our approach has been shown to produce high-quality rules with good open path head coverage (OPHC) measures.",
    "The paper reports on the discovery of closed-path rules, which impose constraints like entity types and predicate ranges. These rules are learned by rule learners using Algorithm 2. The quality measures for these rules include HC measures, open path head coverage (OPHC), and support. Inverse Open Path (IOP) rules have a maximum cardinality MCar. Embeddings represent entities as vectors in high-dimensional space. DBpedia provides benchmarks for evaluating the performance of algorithms like A good learning algorithm. The paper also discusses Wikidata's role in providing structured data.",
    "In knowledge graphs, predicates play a crucial role in connecting entities and forming relationships. For each predicate P_k, there exists a broader term, The predicate, which represents a logical statement or expression that connects entities satisfying a subject predicate to entities forming an object predicate along a path of predicates. This concept is further expanded by the notion of all unary predicates being occupations such as singer or entrepreneur, resulting in all entities with these types being classified as people. Additionally, there exist various algorithms and procedures for processing data, including Sampling() method and Algorithm 2. Furthermore, logical statements can be used to represent rules and relationships between entities, while embeddings are mathematical representations that capture semantic relationships between entities. Pruning techniques can also be employed to reduce or eliminate parts of a system or model. The proportion of mined rules having various feature values is presented as results. Finally, the concept of representation is further explored through existing ontologies and definitions such as Definition 2.",
    "In this context, -S+H is an algorithm that uses both sampling and heuristic pruning to learn SHACL shapes from knowledge graphs. This approach can be seen as a broader term for self-loops, which are used in various procedures such as To find or TreeSCnew. The subject (respectively object) argument embeddings of predicates P_k play a crucial role in this process, along with structural features like depth and width. Furthermore, learning paths and scoring functions are essential components of the system. We define formal quality measures for IOP rules to ensure that these shapes can be effectively applied.",
    "The association rule mining method discovers patterns and relationships between variables by identifying rules with significant predictive power. Every entity-instantiation is an instance of a Conceptual Path (CP) rule, where every such instance is also an instance of the related Open Path (OP) rule. Predicate embeddings are used to represent predicates' meanings or concepts as vectors in high-dimensional spaces. The performance of SHACLEARNER (-S+H), without sampling but with heuristic pruning, compares favorably to the ideal rule learner (SHACLEARNER -S- H). Inverse Open Path rules connect entities satisfying a subject predicate to entities forming an object argument along a path of predicates. Logical formalisms provide well-defined semantics and motivating use cases for deriving SHACL shapes. The performance of Algorithm 2, which uses Sampling, is comparable to that of the ideal rule learner.",
    "In knowledge graph sampling, This sampling approach learns partially instantiated shapes that can cause an explosion in possible shapes. To address this issue, we focus on such path shapes and use rule learning algorithms to mine these shapes from data. Our mined shapes are embedded using RESCAL, which maps entities to vectors and predicates to matrices. These embeddings enable us to reason about the relationships between entities and predicates, allowing for more efficient knowledge graph processing.",
    "The study explores various formalisms, including predicate logic and scoring function heuristics for OP rules. It highlights the importance of similar argument embeddings for predicates with the same variable in a rule. The methodology involves using rules found in experiments to extend knowledge graphs in an arbitrary manner. Additionally, it discusses the concept of cover used for rule learning and certain properties that define IOPSC's non-increasing nature with length. Furthermore, it touches on parameters such as maximum allowed atoms in an IOP rule and approaches like their method. The study also references Wikipedia as a source of non-structured information.",
    "The presentation of closed path rules and open path rules highlights several methods for automatically completing knowledge graphs by predicting missing facts. These approaches, such as Algorithm 1, utilize various measures like HC measures to quantify the explanatory power of a rule. The atomic body represents a single entity that satisfies certain conditions and patterns expressed through Inverse Open Path (IOP) rules. Furthermore, RESCAL is used for learning embeddings in this experiment. Additionally, several methods are proposed for procedure-based knowledge graph completion after sampling.",
    "In knowledge graphs, predicates with similar variables should have analogous argument embeddings. Formal quality measures are defined for IOP rules to ensure their effectiveness. A depth-first questioning strategy can be employed to extract information from these graphs as shapes. The target predicate represents a specific path in the graph that satisfies certain conditions. Rule learning systems and existential rules play crucial roles in understanding the relationships between entities. Presenting class and type information is essential for methodological approaches. Body_ r, or the support of closed-path rules, counts instances where both body and head are satisfied. IOP Head Coverage measures the coverage of a set of rules or entities. Algorithm 1 provides a computational framework to solve specific problems. The embedding representation captures the essence of concepts and objects in high-dimensional spaces.",
    "The relationships between entities reveal insights into their connections and properties. For instance, each argument of a target predicate has an associated broader term, which can be seen as a characteristic or attribute that defines it. Similarly, K' represents a collection of facts or knowledge, while quality rules outline guidelines for acceptable levels of quality. The pairs satisfying the head are connected by paths P1 to Pm, illustrating how entities relate to each other through these sequences of predicates. Furthermore, A path is a sequence of predicates terminating with open variables at both ends, highlighting its role in shaping our understanding of knowledge graphs.",
    "In this section, we explore various concepts and techniques for processing knowledge graphs. We consider a mathematical concept or notation representing a modified version of an initial object processing (IOP) rule, which can be used to validate, edit, or gain insight into these graphs. Additionally, we discuss the importance of embedding entities in vector spaces using methods like RESCAL, as well as evaluating their quality through measures such as IOP Head Coverage and inverse open path standard confidence. Furthermore, we highlight the significance of self-loops and certain properties that are typically incomplete. Overall, this section provides a comprehensive overview of various approaches for processing knowledge graphs.",
    "The quality rules are built upon quality measures, which define acceptable levels of quality for a particular process or product. The representation of input data through argument embeddings captures semantic relationships and contextual information. In addition to these representations, we also have cardinality constraints that govern the quantity of entities within a set. Furthermore, pruning techniques can be used to reduce parts of a system or dataset. We conducted experiments to assess the effectiveness of SHACLEARNER in capturing shapes with varying confidence, length, and cardinality from various real-world massive knowledge graphs.",
    "The method for comparison purposes, Algorithm 2, relies on closed paths of variables to represent sequences of entities. This formalisation uses vectors and representation techniques to model data structures like TreeSCMIN, which defines a threshold value for rule dismissal during tree construction. The approach involves learning rules from new unary facts and applying them to the knowledge graph, considering cardinality constraints and structural features such as depth and width. Rule learners use algorithms like Algorithm 1 to generate shapes that satisfy certain properties, including confidence measures like TreeSCorig. HC evaluates the explanatory power of these concepts.",
    "The concept of citizen-of relationships between entities based on their location and residence information is governed by rules, which can be learned through machine-based methods. These rules are used to highlight errors for correction by applying schema constraints. The embedding space captures certain properties related to predicate embeddings, subject and object argument embeddings, and the support of closed-path rules. A good learning algorithm treats all binary predicates as plain predicates without distinction. Definition 1 represents a tree shape learned from IOP rules, where entities are connected by paths P1, P2, ..., Pm. The sampling algorithm selects specific data or information from a larger dataset.",
    "In knowledge graphs, embeddings play a crucial role in representing entities and their relationships. A representation is a conceptual structure used to organize or describe something, while confidence measures are essential for Inverse Open Path (IOP) rules. Partially instantiated shapes can cause an explosion in the space of possible shapes, highlighting the importance of presenting class and type information as unary predicates. IOP rules connect entities satisfying specific subject and object arguments through a chain of predicates. Pruning methods help reduce or eliminate parts of a system to improve performance. Embeddings are mathematical representations that capture semantic relationships between entities, used in natural language processing and machine learning applications.",
    "In this knowledge graph, we can see various entities and their relationships. Ontologies provide a framework for representing knowledge using classes, types, and relationships. Features are characteristics or attributes that define something, while representations are conceptual structures used to organize data. Type predicates describe properties of things, and predicate embeddings capture the meaning of predicates as vectors in high-dimensional space. Rules govern how entities interact with each other, such as predicting missing facts based on patterns learned from a knowledge graph. The RESCAL algorithm embeds entities into vector spaces for efficient computation. Inverse Open Path rules connect entities along paths of predicates to predict new relationships.",
    "The scoring function, which assigns scores to entities based on their relationships and properties, has been applied to Algorithm 2. The performance of SHACLEARNER (-S+H) was evaluated using this algorithm, demonstrating its effectiveness in processing large-scale shape mining tasks. Shape quality measures are essential for ensuring the accuracy of these processes. RESCAL embeddings were used to represent entities and predicates in a high-dimensional space, allowing for efficient computation of predicate embeddings. The first rule indicates that x should belong to an album with y as record label, while the fact Pk(ei, ej) represents specific instances of this relationship. Entities e2 and e3 satisfy the head of the rule by instantiating their second argument classes. Universal quantification was used to express statements about all members of a particular class without specifying individual instances. Open path standard confidence (OPSC) measures were employed to evaluate the quality of open-path rules, which rely on predicate embeddings for inference. The computed embedding representations of predicates and entities provide a numerical framework for representing concepts in knowledge graphs.",
    "In this knowledge graph, we see various entities and their relationships. The Embeddings function generates vector representations of input data for natural language processing tasks. An entity can be a concept representing an object or person that serves as an identifier. There are also definitions such as A path being a sequence of predicates connected by closed intermediate variables but terminating with open variables at both ends, which is used to validate manual editing tasks. Furthermore, we have quality measures like the Open Path Standard Confidence (OPSC) and representation concepts like embedding spaces and target predicates that define constraints on data. Additionally, there are algorithms such as Algorithm 1 and Algorithm 2 for solving specific problems or achieving particular goals.",
    "The scoring function heuristic for OP rules uses a method to prune candidate IOP rules based on their similarity and plausibility, considering properties of predicate arguments and composite predicates in an embedding space. This approach ensures that entities with similar variable embeddings are treated similarly. The third rule requires a song (x) to have at least two producers, which is more constraining than the second rule due to its lower confidence. New predicates from facts can be generated by transforming binary fact patterns into new predicate forms, each corresponding to a class or type in a knowledge graph. These predicates are used to model types and classes in ontologies.",
    "In this knowledge graph, we observe relationships between entities such as increasing IOPHC and phenomenon. The triple format provides a standardized structure for representing subject-predicate-object relationships. Rules are used to dictate specific behaviors or decision-making processes, with max rule length l serving as an upper bound. Sampling techniques like Algorithm 2 help prune irrelevant data. Learning methods and approaches can be applied to acquire knowledge from experience or data. Predicates compute argument embeddings, which capture essential characteristics of concepts. Classes and types provide a framework for categorizing entities. Definition 5 outlines the cardinality of Independent Operator Pattern rules. We find that some major approaches to rule learning involve measures such as support, head coverage, and standard confidence. The last row in Algorithm 1 represents an unmodified version of SHACLEARNER. A song has at least two producers according to a given fact. Native language is associated with each entity x satisfying human(x). Definition 1 defines the open path measure (OPsupp, OPSC, OPHC), which assesses rule quality. We also discover that TreeSCnew represents the confidence of pruned tree shapes.",
    "In knowledge graphs, relationships between entities are crucial for understanding and inference. The necessity of a chain of facts highlights the importance of these connections. Pruning methods can be used to reduce unnecessary information, while completion strategies help fill in gaps. Measures such as standard confidence and head coverage evaluate the quality of closed-path rules. A path is a sequence of predicates connected by intermediate variables, terminating with open variables at both ends. Definition 1 formalizes this concept. GreedySearch optimizes processes by making decisions based on available information. Three versions of rule r (r1, r2, and r3) can be used to achieve different cardinality constraints. Matrix P_k in R^(d x d) represents the mapping between entities and predicates. Thresholds like TreeSCMIN determine when an IOP rule should be dismissed during tree construction. CRs are used for inferring new facts, while trees represent structured data.",
    "The evaluation framework, Ev, utilizes an algorithm to generate embedding representations of concepts. These embeddings are used to define new predicates from facts about albums, which are then constrained by FGDs that impose rules on relationships between entities. The quality measures for open path standard confidence assess the predictive power of these rules. Additionally, alternative formalisms provide a framework for expressing mathematical concepts and relationships. Existing ontologies serve as a foundation for understanding classes and types of entities, while subject and object argument embeddings capture semantic relationships between them. Furthermore, it treats all properties as plain predicates without distinction, allowing for feature extraction from data. Finally, these thresholds mark the minimum quality standards for IOP rules to be considered valid.",
    "In knowledge graphs, entities such as 'e' and 'entities' are connected by predicates like 'has a broader term', indicating relationships between concepts. For instance, PathFinding has a broader term of approach, while These thresholds have a broader term of threshold. The two type-like predicates from DBpedia 3.8 and the one from Wikidata also share a broader term with The predicate. Then standard confidence is related to SC(r), which measures how frequently closed-path rules are true. We employ an abstract model using Description Logic ontologies, where TreeSCnew represents novel numerical confidence measure. A song has at least two producers as a fact in the knowledge graph. Car indicates a satisfying e and counts non-zero elements. Entities with these types turn out to be persons even though there is no explicit person type in our KG. Pruning techniques are algorithms used for comparison purposes, while Definition 4 represents a conceptual framework or notation. Let r has broader term variables, which can take on different states or values. A path is a sequence of predicates connected by closed intermediate variables but terminating with open variables at both ends, representing an entity's whole path. The necessity of a chain of facts requires certain relationships between entities to be present in the knowledge graph. Argument embeddings are used to capture semantic relationships and contextual information in embedding spaces.",
    "Algorithm 2 shows the sampling method, first introduced. Rule learning systems has a broader term entities. The adjacency matrix representation A(Pt) = [0 0 0; 0 0 0; 0 1 1] has a broader term through an example. Algorithm 1 has a broader term procedure. z0 has a broader term An entity. Prior Sampling has a broader term Sampling() method. V^1 has a broader term vectors. Entities that satisfy the second argument of the body have a broader term argument. The class and type information as unary predicates has a broader term information. The pairs (e, e') satisfying the head are connected by the path P1, P2, ..., Pm. has a broader term Definition 3. Algorithm 2 has a broader term approach. Cover has a broader term representation. Target predicate has a broader term predicates. The performance of the SHACLEARNER (-S+H) has a broader term system. Maximum number of predicates has a broader term Parameters. Each non-zero element indicates a satisfying e has a broader term certain properties. They have a broader term entities. Class has a broader term representation. Target predicate has a broader term conditions. Prior Sampling has a broader term pruning methods.",
    "In this knowledge graph, we see that OP rules do not necessarily form a loop. Representation and predicate embeddings are key concepts used to organize entities into logical formalisms. Formalisation provides an approach for describing these relationships. The embedding space represents the semantic meaning of predicates and entities as vectors in high-dimensional space. Algorithm 2 is used to sample entities, while predicates and entities are represented as vectors enabling plausibility scores for facts. We obtain simpler trees by removing branches from complex tree shapes. Variables instantiated by entities capture certain properties in the embedding space. Presenting class and type information requires a procedure that represents data as unary predicates. The Sampling() method is an approach used to extract specific data. Data can be represented as information, which is evaluated using statistical measures like SC. Formalisms provide standardized notations for expressing patterns and relationships.",
    "In a knowledge graph, we can represent complex relationships between entities using rules and predicates. A path of entities that satisfy a chain of predicates in a rule connects distinct e's by summing each row of this product to obtain the vector V^1(A(P1)*A(P2)*...*A(Pm)). This process satisfies the head of r, which is defined as the set of predicate facts in a tree shape. The body of r represents a sequence of paths or branches that can be used to validate an instance in the knowledge graph. To compute this representation, we employ an abstract model using logical statements and predicates. Additionally, pruning methods are employed to reduce parts of the system, improving performance and efficiency.",
    "In knowledge graphs, entities can have different cardinalities despite sharing the same head and body. Potential rules are a collection of guidelines that haven't been formally established yet. A father-child relationship between x and z2 satisfies human(z2). Paths connect entities in a sequence. The subject argument embeddings of predicates P_k represent relationships between entities. Constraints govern behavior, while formalisation provides structured representation. Existential variables appear at the end of an inverse open path rule. An instance of the head is a specific shape pattern that can be augmented with minimum cardinality constraints. ShEx standardises data shapes. RESCAL embeds entities and predicates into vector spaces and matrices. Scoring functions assign values to entities based on their relationships. TreeSCnew measures confidence in pruned tree shapes. We model types and classes as unary predicates, representing certain properties. Relational databases employ schema-based constraints like not-null. Roles are named binary predicates that connect entities with certain properties.",
    "In knowledge graphs, consisting of bounded entities can be related to facts about them. For instance, a Car has parameters that define its features and properties. Sampling approaches are used to select subsets from larger populations, ensuring high-quality IOP rules. Then standard confidence measures how frequently these rules are true by counting the proportion of entity pairs that satisfy both body and head predicates. Interestingly, the number of missing rules increases as quality decreases in real applications. Errors can be highlighted for correction using schema constraints. For example, tree shape learning involves modeling types and classes with unary predicates. An illustration is provided to demonstrate this concept. The same head and body may have different cardinalities due to certain properties. A pair of entities satisfies a given open path rule by having predicates as facts in the knowledge graph.",
    "In this knowledge graph, we see various entities and their relationships. For instance, e2 has a broader term of matrix, indicating that it's a type of mathematical object or value used in computation. The set P1(\u2026), Pt(e(q-1)', eq) is related to the concept of fact, which represents a statement or assertion of information. Additionally, facts related to them are categorized under data, suggesting a collection of information about various aspects. Furthermore, we have entities like r', empty, and lemma that represent specific concepts in this framework.",
    "In knowledge graphs, shape quality measures are used to evaluate and quantify characteristics or attributes of shapes. Target predicates define constraints on data, while entities represent abstract representations or groupings of people, places, things, or concepts. The tree support and standard confidence of a rule measure how well it explains its conclusions. Ideal learners serve as standards for excellence in learning algorithms that solve specific problems through automated manipulation and processing of data. Pairs of connected entities satisfying the head predicate are represented by paths of predicates. Some major approaches to rule learning use measures such as support, head coverage, and standard confidence.",
    "The figures, which are graphical representations or illustrations, convey information and data. Shape quality measures evaluate the characteristics of shapes, while predicates express relationships between entities. The explanatory power of a rule assesses its ability to explain conclusions based on certain properties in an embedding space. Additionally, we compute argument embeddings as part of our procedure for aggregating entity representations from knowledge graphs (KGs). Furthermore, RDF databases store and manage data using Resource Description Framework format.",
    "The relationships between entities reveal that parameters are a broader term for y, and applications can be applied in practice. Sampling methods approach problems by considering conditions and formalisms. Variables r and l have different cardinalities, while matrix representations like (Pt) capture features of entities. The logical statement of rules is connected to the embedding space's properties. Open path support measures entity pairs satisfying body and head predicates. Unary predicates represent relationships between entities, and P_k variables are used in rule instantiations. Whole paths consist of multiple predicates, and Pt targets specific predicate representations.",
    "In this knowledge graph, we see various entities and their relationships. The concept of predicates plays a crucial role, as it represents statements that express relationships between one or more entities. These predicates can be further categorized into broader terms such as 'arguments' which describe conceptual representations used to describe relationships. Additionally, there are conditions like the IOP rule with cardinality constraint Car=2, where only e2 satisfies the rule. Furthermore, we have representation concepts like confidence widely used in association rule mining and embedding representations that capture essential characteristics of entities. The graph also highlights schema information which defines data structures and their organization. Moreover, it showcases logical statements such as sequences of entities satisfying head predicates, prior sampling techniques for pruning datasets, and algorithmic procedures like Embeddings() to generate vector representations.",
    "In the realm of predicate embeddings, predicates serve as fundamental building blocks for expressing relationships between entities. PathFinding and procedure are two approaches used to navigate these complex networks. Computer scientists have developed class-based representations that categorize entities into distinct groups. Meanwhile, existing ontologies provide a framework for describing individuals with identifying characteristics like name, birth date, and place of birth. Trees can be seen as abstract representations or groupings of people, places, things, or concepts. The concept of predicate P_k represents logical expressions or relations used in knowledge representation frameworks. Sample data is often used to test or train models, while support measures evaluate the effectiveness of these systems. Each binary target predicate has a broader term that satisfies certain conditions, and Then IOPSC(r) <= IOPSC(r') outlines an inequality stating that inverse open path supports do not exceed their counterparts' support. Out of thirty-five predicates, unary predicates may represent class assertions expressed in RDF triples as (e, rdf:type, P), where P is a class or datatype. The standard confidence approach measures the frequency with which closed-path rules are true. A( Pt) represents an adjacency matrix for predicate Pt in entity set E and fact set F. SC stands for statistical concepts used to evaluate frequencies or proportions of entities that satisfy certain conditions. Finally, constraints govern behavior, usage, or interpretation, while figures represent numerical data used to describe statistical information.",
    "The approach used by their method for comparison purposes involves considering entities forming the object argument of the last predicate, P2. This process relies on a broader term, which can be an entity-fee rule or rules that connect classes and types to entities. The fact that Algorithm 2 uses subject and object argument embeddings in its representation highlights the importance of embedding space for capturing complex relationships between entities like Car. Furthermore, the scoring function used by P0 is based on a logical statement that assigns scores to entities according to their properties. In addition, head coverage measures how well a model captures relevant information about each element.",
    "In this knowledge graph, we see various entities and their relationships. A pair of entities satisfies a specific condition defined by 'the body of r'. The label of a link in the graph has a broader term that describes its feature. Type information provides additional context about data, which is itself an entity. FGDs define constraints on predicates and entities, while relational databases impose schema-based constraints to ensure data consistency. Measures such as head coverage quantify how well models capture relevant information. The computed embedding representations of predicates and entities are used in procedures like the Sampling() method. Facts are statements that provide specific information about data or entities. Variables can be closed or open, depending on their context. Conditions define rules for satisfying certain criteria. Representation is a conceptual structure used to organize and describe something.",
    "In this knowledge graph, we see various entities and their relationships. For instance, an example of a shape from Wikidata has a broader term through an example. The predicate's components P*(x, t) also have a broader term in terms of its logical statement representation. We can observe that certain predicates like r have cardinality annotation for Car(1), while others like An OP rule has two open variables, y and x are connected to the concept of variables. Additionally, pruning methods are categorized under procedure, cultural bias, and resource constraints fall under resource constraints, and so on. The graph also includes entities with type information, data representations, and subject and object argument embeddings that capture semantic relationships between entities.",
    "In our knowledge graph, we model types and classes with unary predicates. We choose to represent linguistic relationships between entities using a binary predicate. The tree support and standard confidence of rules are measures that assess their quality. Definition 3 outlines specific concepts and principles used in our framework. Rules like citizenOf(x,y) <- livesIn(x,z)^locatedIn(z,y) infer citizenship based on residence and location. We also consider the proportion of facts satisfying a rule's head that can be inferred from its body. Our approach involves modeling entities with song as their type, using shapes to represent paths over connected songs. The semantics of SHACL language governs our interpretation of statements. Through sampling techniques, we select subsets of data for analysis and evaluation.",
    "In knowledge graphs, composite predicates are formed by chaining predicates along paths of entities that satisfy certain conditions. DBpedia contains predicates where the second arguments are types or classes, which can be used to define relationships between entities and their representations. Unlike CP rules, OP rules do not necessarily form a loop, allowing for more flexible representation of human knowledge. The predicate embeddings method represents entities as vectors in high-dimensional space, enabling similarity measures and distance calculations. Furthermore, the syntax of RDF triples allows us to represent facts such as (e, P, e') where 'P' is a predicate connecting two entities 'e'. New unary facts can be produced based on new predicates and related facts, facilitating learning fully abstracted shapes instead of partially instantiated ones.",
    "In the context of knowledge graph validation and data type checking, several entities play crucial roles. The mathematical entity z1 represents a binding variable in an inverse open path rule, while All head branches denote the sequence of paths or branches terminating at each node in a tree shape. Parameters define the boundaries within which a system operates, whereas distinct e's by summing each row of this product to obtain the vector V^1(A(P1)*A(P2)*...*A(Pm)) represents the set of entities that satisfy the head of an IOP rule. The predicate P connects two entities, representing an association between them. Furthermore, rules dictate specific behaviors or decision-making processes, and search space encompasses all possible options or possibilities within a domain. In addition, SC measures the frequency or proportion of entities that satisfy certain conditions.",
    "The composite predicate, which is composed of multiple predicates or propositions connected by a logical operator. This approach has been shown to be effective through Sampling, as demonstrated in Algorithm 2. The quality measures for this rule (r1) are defined according to Definition 4, with IOPHC(r1) = 2/2 and IOPSC(r1) = 2/2. Furthermore, the row index of elements with value >= Car of V^1(A(P1)*A(P2)*...*A(Pm)) is determined by P2. This method can be applied in a breadth-first manner to traverse the graph efficiently. Additionally, resource constraints must be considered when applying this approach. The embedding representation captures essential characteristics and properties of entities, which are represented as vectors E_i in R^d for each entity e_i. These representations can be used to analyze or generate natural language text.",
    "In knowledge graph construction, SHACLEARNER faces limitations regarding cardinality expressions, only handling minimum cardinality constraints. Feature extraction and predicate representation are crucial steps in this process. P2 represents a mathematical function that takes one input and produces another as its output. Both kinds of errors can occur during knowledge graph construction, including errors of omission and commission. Identifiers for objects such as places or people serve to represent entities like e. To compute refers to aggregating and combining entity representations from a knowledge graph. Our novel tree shapes yield results that demonstrate the effectiveness of this approach. P1( y, z) represents a logical operator connecting two entities based on their relationships with other entities. Cardinality measures describe the size or magnitude of sets in our representation. Learning involves acquiring knowledge through various approaches and procedures. For example, we can illustrate complex concepts using concrete examples like fullHouse predicates that represent poker hands. Definition 5 defines an entity as a unique label used to represent locations, individuals, or things. Unary predicates may express class assertions about entities' nature or classification. We learn from cardinality constraints by aggregating and combining entity representations. The row index of elements with value >= Car of V^1(A(P1)*A(P2)*...*A(Pm)) represents a set of entities that satisfy both conditions: being an argument in the body of a rule and also satisfying its head.",
    "In this knowledge graph, we see various entities and their relationships. Each entity e_i has a vector representation E_i in R^d, which captures its essential characteristics. The embedding represents a predicate or entity's meaning, computed using the RESCAL algorithm to capture their relationships and properties. We also have unary predicates that represent class assertions expressed in an RDF triple as (e, rdf:type, P) where P is a class or a datatype. Furthermore, we see entities like 'From Definition 4, IOPHC(r1) = 2/2 and IOPSC(r1) = 2/2' which represents the quality measures for a specific rule (r1). Additionally, there are concepts like 'what proportion of the facts satisfying the head of the rule could be inferred by satisfying the rule body?' that assesses the explanatory power of closed-path rules. The graph also contains entities representing mathematical expressions and logical frameworks, such as '(e', e)' which represents a pair or tuple of values.",
    "In knowledge graphs, prior sampling serves as an approach for selecting data subsets. This method was first introduced through the statistical technique of sampling methods. The set E1 represents entities related to a target predicate Pt, which can be either Pt(e,e') or Pt(e',e). Each entity e_i is represented by a vector E_i in R^d, capturing certain properties in the embedding space. DBpedia contains predicates where the second arguments are types or classes. These predicates connect entities satisfying subject predicates to those forming object predicates along paths of predicates.",
    "In this work, we explore the concept of open path rules and their application to knowledge graphs. Specifically, we consider entities such as albums, record labels, and songs that satisfy certain constraints or predicates. For instance, an album should belong to a record label with which it has a specific relationship. We also examine various measures of rule quality, including confidence widely used in association rule mining. Furthermore, we discuss the importance of distinguishing between entities and predicates using a common representation for this work. Additionally, we model concept or class membership as entity instances of binary facts, highlighting the significance of variables such as Pt and zi that represent predicate components.",
    "The unary predicate P1(e, e1) represents an RDF triple (e, P, e') where 'e' is an identifier for an object such as a place or person and 'P' is a binary predicate. The class and type information can be used to classify entities into categories like classes and types. Unary predicates like human in Fig. 2 constrain entities as 'human' and specify three paths: citizenOf(x, z1) ^ country(z1), father(x, z2) ^ human(z2), and nativeLanguage(x, z3) ^ language(z3). The predicate P'i is the inverse open path rule 'P'i. Entities that satisfy target predicates can be used to determine confidence widely used in association rule mining. OPRL evaluation module evaluates or assesses something using a system of rules. Further research on future work proposes new ideas and approaches.",
    "The predicate of the head relates to representation, which refers to conceptual structures or models used to organize and describe something. E1 represents a set of entities related through predicates Pt(e,e') or Pt(e',e). The fact that e has a headr(e) indicates its role in these relationships. Furthermore, facts are statements describing information, often represented by subject-predicate-object triples. Our unary predicates and facts demonstrate the importance of confidence widely used in association rule mining to evaluate the effectiveness of rules. Additionally, entities which have types such as singer or entrepreneur exist within Wikidata+UP knowledge graph. Lastly, TreeSCnew represents a pruned tree shape with guaranteed confidence greater than or equal to the original.",
    "The concept of representation plays a crucial role in understanding poker hands, as it encompasses various combinations of five cards. This idea can be further represented by unary predicates like P1(e, e1), which describe specific facts about entities that satisfy certain conditions. The process of generating new unary facts based on existing predicates and related facts is an essential aspect of this representation. In the context of future work, these representations will likely involve constraints such as minimum cardinality restrictions to ensure meaningful relationships between entities like cars or songs with multiple producers. Furthermore, understanding semantics and variables can help us better grasp how these representations are constructed.",
    "The predicates where the second arguments are types or classes relate to predicates, results, and Definition 2. These entities with only a type or class declaration can be classified as data, Car values, One proposal, Section 6, this section, and representation. Our unary predicates and facts represent class assertions expressed in RDF triples, which may also include classic versions of related work, exploratory attempts, approaches, and logical statements about birthPlace, input ontologies, poker hands, Sampling procedures, conditions, and entities like e2.",
    "The occupations category has a broader term of classes and types. Car indicates a satisfying e also falls under conditions, while target predicate is related to The predicate. Language( z3, z3) represents entities, with z2 being equivalent to t. Based on new predicates and facts, we can generate unary facts from existing predicates and their instances. Each entity e_i is represented as a vector E_i in R^d, which falls under the broader term of representation. The head of the rule refers to the topmost part of an expression or construct, while predicates and entities are connected by The predicate. Variables such as existential variables represent unknown values used for quantification over domains. Rule quality is evaluated based on entity representations. All unary predicates like occupations such as singer or entrepreneur fall under a broader category. We observe expected decreases in rule quality with greater cardinalities demanding tighter restrictions to be satisfied, resulting in the distribution of mined rules by their cardinalities. NativeLanguage(x,z3) constrains an argument of human(x), expressing that each entity x satisfying human(x) should have a native language specified by z3. Learning is a process or method for acquiring knowledge and skills through procedure.",
    "The predicates express certain properties that define objects, concepts or systems. Quality measures are used to assess and evaluate performance. P represents a binary relation connecting entities. Car(r) has constraints governing its behavior. The trees consist of individual tree structures with stems, leaves, and roots. Uncertainty arises from representation models. Standard confidence is a measure evaluating the quality of rules. Binary predicates take two arguments returning truth values. Once in the straight form, object arguments connect heads forming predicate paths. P1(e, e1), ..., Pt(e(q-1)', eq) satisfy The predicate's head. CapitalOf represents government seats for countries within existing ontologies. Procedural methods achieve specific goals through approaches given. Each argument of each target predicate serves as an entity connected by path predicates forming a tree-like structure. A song has at least 2 producers, and One approach is used to accomplish tasks via method definitions.",
    "The concept of Car has a broader term, which refers to (1). For example, this can be illustrated through an example. The mathematical representation Pt also has a broader term, Definition 3. Similarly, suit and maximum number of predicates have broader terms classes and types and variables respectively. Furthermore, persons even though there is no explicit person type in our KG and {e1, e2, e3, e4, e5} both have broader terms entities. The variant representation also has a broader term, which is the concept of representation. Additionally, Pt(x,y) has a broader term predicates, while each entity e_i to a vector E_i in R^d and certain properties in the embedding space both have broader terms predicate embeddings and certain properties respectively. Moreover, Car has another broader term Cardinality, indicating that it can be categorized under this concept. Similarly, y^q has a broader term variables, P1(x,z0) has a broader term logical statement, Pi has a broader term representation, and Cardinailty also has a broader term rule quality measures. Finally, consisting of a bounded number of entities has a broader term number, indicating that it can be categorized under this concept. Lastly, occupations have a broader term class.",
    "In this knowledge graph, we have various entities and predicates that describe their relationships. For example, a pair of entities (E,F) has a broader term 'entities', which is an abstract representation or grouping of people, places, things, or concepts. The unary predicate human in Fig. 2 constrains entities as 'human' and specifies three paths: citizenOf(x,z1), father(x,z2)^human(z2), and nativeLanguage(x,z3) ^ language(z3). A tree is an entity that represents a perennial plant with a single stem or trunk and branches that grow from it. The satisfaction of a less-constrained shape has conditions, which are rules or limitations that must be met in order to satisfy a particular standard, requirement, or expectation. We use methods, such as the sampling method first introduced, to achieve specific goals or solve problems. A pair of entities satisfies an open path rule r by having predicates P1(e,e1), ..., Pn(en-1,e') as facts in the knowledge graph and Pt(e',e) being a fact. The target predicate Pt is a concept that represents an abstract idea or property, which has certain properties.",
    "The native language of an individual x, constrained by z3, can be described as a feature. Cardinality measures the size or magnitude of entities, which are abstract representations or groupings of people, places, things, or concepts. An approach to achieving a goal is often structured and systematic, like a procedure. Generality refers to the scope or breadth of certain properties that define an object, concept, or system. Unary predicates represent class assertions expressed in RDF triples as (e, rdf:type, P), where P is a class or datatype. A pair of entities e and e' satisfies a closed-path rule's body, indicating a relationship between them. The predicate connects entities satisfying subject predicates to those forming object predicates along a path of predicates. Measures quantify indicators or standards for evaluating the quality of rules generated during learning.",
    "The concept of entities and their relationships is fundamental to understanding knowledge graphs. Entities, such as e' and entities, can be related via binary predicates like has a broader term or lacks logical foundations. These relations are represented by RDF triples, which indicate that subject entity e is related to object entity e' via the predicate P. The embedding representation of vector E_i in R^d captures certain properties in the space. Procedural methods and rule quality can be evaluated based on their cardinality, which expresses a lower bound on head paths satisfied for every instantiation linking body to head. Furthermore, entities like Mary's birthplace or Canberra's location are represented as part of existing ontologies. The converse concept is also related to entities, while variables A and certain properties in the embedding space can be categorized under features.",
    "The representation of an entity serves as an identifier, with Car(r) being a type of vehicle that has predicates. Language( z3,z3 ) represents linguistic relationships between entities, while variables can take on different states or values in mathematical models. The predicate P0(e1,e2) describes associations between two entities, and measures support quantifiable indicators for evaluation. Each entity is represented by subject and object argument embeddings, with computer scientists having different depths of detail for similarly accomplished scientists. Pt(x,z0) takes two arguments to return a value, while cardinality c1 represents the minimum or maximum number of instances allowed for an entity. The predicate P1(y,z) connects entities based on their relationships with other entities, and rdf: type categorizes classifications. Finally, whole path forms a unified concept by connecting related events or actions.",
    "According to This result, IOPSC values are non-increasing with length. The lemma states that if we discard an IOP rule due to its low IOPSC value, we do not need to check versions of the rule extended with additional head atoms. Furthermore, each predicate P_k is represented as a square matrix in R^(d x d), mapping entities to vectors in high-dimensional space. Additionally, predicates and entities are closely related concepts that represent both relationships between things (predicates) and individuals or objects involved in those relationships (entities). The support of closed-path rules counts the number of entity pairs that satisfy both body and head of the rule. In terms of generality, a measure is used to evaluate the effectiveness of a rule. Moreover, presenting information involves introducing, displaying, or showcasing something. Future work includes planning research agendas, project plans, or academic goals that have not yet been undertaken.",
    "In this knowledge graph, we have various entities and predicates that describe relationships between them. For instance, 'concept or class membership' has a broader term of 'class', indicating its categorization within the concept hierarchy. Similarly, each binary target predicate has a broader term of 'The predicate', signifying its role in defining logical constructs. We also see unary predicates like 'Cardinality' and 'l' that describe properties of entities. Furthermore, we have predicates like 'a song has at least 2 producers' that specify constraints on the relationships between songs. The graph illustrates how these entities and predicates interact to form a complex network of knowledge.",
    "The presentation of entities that satisfy target predicates involves presenting, which is an act or process of introducing, displaying, or showcasing something. These entities have only a type or class declaration and are connected by closed intermediate variables. The unary predicate human constrains these entities as 'human' and specifies three paths: citizenOf(x, z1) ^ country(z1), father(x, z2) ^ human(z2), and nativeLanguage(x, z3) ^ language(z3). A pair of entities is considered together or related to each other. The meaning that the subject entity e is related to an object entity e' via the binary predicate (also known as a property), P., indicates a fact or link in a knowledge graph, represented by an RDF triple (e, P, e'), indicating that subject entity e is related to object entity e' via binary predicate P. Each entity e_i is represented as a d-dimensional vector E_i in R^d.",
    "In this knowledge graph, we see various entities and predicates that describe their relationships. Definition 1 satisfies a standard or guideline for fulfilling requirements. The 'z*' entity exists as part of the 'y*s' set, which represents pairs of entities satisfying head rules. Procedural methods are broader terms encompassing procedures used to achieve specific goals. Unary predicates represent class assertions and may be expressed in RDF triples. Pi is a mathematical function representing the ratio of circumference to diameter. Cardinality measures the size or magnitude of sets. P2(z1, z2) takes two inputs and returns values. MinTreeSC is a parameter that characterizes minimum tree structures. Strategy and approach are plans for achieving goals. Feature values describe measurable characteristics. Free variables are universally quantified at the outside. Presentation conveys information through formal talks or documents. Person refers to individuals with human characteristics. Representation models organize, categorize, or describe concepts. Related work is previous research on a topic. The number of entities that satisfy rule bodies represents total quantities. A common representation distinguishes between entities and predicates. Entity e_i represents an object in the knowledge graph. Presentation conveys information through formal talks or documents.",
    "The concept of head_$(e, e')$ relates to predicates that express relationships between entities. One proposal suggests that these predicates each have a class as their second argument, which can be categorized under type predicates. In an example, we see that distinct $e'$s are counted by summing each row of the product A(P1)*A(P2)*...*A(Pm) to obtain the vector V^1(A(PT)). This process is used to identify entities related to a target predicate PT through either PT(e,e') or PT(e',e). The results show that minimum cardinality constraints are imposed on these predicates, which can be classified under Cardinality. Furthermore, we find that Car has song as its type and genre, while Presenting involves an approach. Lastly, distinct $e'$s by summing each row of this product to obtain the vector V^1(A(PT)) is used to identify entities related to a target predicate PT through either PT(e,e') or PT(e',e).",
    "In this context, we choose to model types and classes with unary predicates. Car=1 can be seen as a vehicle designed primarily for personal transportation. The approach of modeling types and classes as unary predicates allows us to relate entities that are connected by target predicate Pt. Predicates like P1(x,z1) express relationships between one or more inputs and produce an output, while quantifiers help indicate quantity or scope in mathematical contexts. Furthermore, the relationship between predicates and entities is crucial for understanding how they interact with each other. For instance, once we have a straight form where the object argument of the predicate is the common variable to connect the head, we can establish connections between different entities like pop stars that belong to specific classes or types.",
    "The concept of uncertainty relates to entities, which can be characterized by features such as cardinality. The predicate 'father(x, z2)' represents a family relationship between an individual x and their child or children z2. In addition, there are various predicates like High Card that rank cards from highest to lowest. This paper explores the concept of composite predicates, where entities satisfy target predicates along a path of predicates. Furthermore, unary predicates may represent class assertions about entities, while our unary predicates and facts describe specific occupations such as singer or entrepreneur. The process of learning each target predicate involves producing new unary facts based on related facts. In this context, conditions are set for the satisfaction of certain cardinalities.",
    "In this knowledge graph, we see various entities and their relationships. The entity variables {x, zi, y} are used to represent different types of variables. P2 represents a mathematical function that takes one input and produces another as its output. We can also define new predicates from facts in the form where x is the name of an album. Additionally, we have unary predicates such as fullHouse which describes a poker hand with three cards of one rank and two cards of another rank. Furthermore, we see logical formalisms being used to represent classification data that can be used in logical statements. The class and type information are represented as unary predicates, allowing us to reason about the nature or classification of entities.",
    "In this knowledge graph, we can see that unary predicates represent class assertions about entities. Type predicates describe the properties or characteristics of something. A song has at least two producers and is classified as a musical composition with specific genre. The object argument of the last predicate refers to an entity satisfying certain conditions. We find entities that satisfy the head of the rule, which represents the outcome of investigating these relationships. This graph also shows how each entity can be represented as a vector in R^d, capturing its essential characteristics.",
    "According to the graph, problems have a broader term of challenges. Mathematical notations like K = (E, F) are defined as entities with a broader term of An entity. Phenomena can be observed and categorized into trees or classes. Pop stars represent famous musicians who belong to certain properties. The capitalOf representation is used to describe the city that serves as the seat of government for a country. Entities join heads of rules to their bodies, forming logical statements like Pt(x, y). Roles are also called properties that relate entities in specific ways. Procedures are employed to achieve specific goals or outcomes. Humans belong to the species Homo sapiens and can be represented by certain measures. The expected decrease phenomenon is observed in the distribution of mined rules with respect to their cardinalities.",
    "In logic, unary predicates represent class assertions expressed as RDF triples. These predicates can be used to model types and classes, such as entities, poker hands, language relationships, human representations, car values, proportion measures, and song classifications. Additionally, existential rules in literature are modeled using these predicates. Furthermore, variables like e4 and A fact (also known as a link) are also represented through unary predicates.",
    "In this knowledge graph, we see various entities and their relationships. The set of distinct e's has a broader term 'variables', which are values that can take on different states or values in a mathematical model or system. A song has an album with a record label, producers, genre, and artist, all represented through examples. Mathematical expressions like n have a broader term '(1)', while predicates P  = {P1,...,Pm} be the set of all predicates in F., which is related to Definition 2. Roles or properties are characteristics that relate two entities in a specific way. The maximum allowed length l for rules has a broader term 'length', and variants have a broader term 'variant'. A variable z0 occurs in at least two distinct predicate terms, such as here, and otherwise it is open, which relates to the predicate of the head. Definition 3 outlines a concept description that satisfies both P1(x,z0) and P2(z0,z1)^P3(z1,z2). The entity e3 refers to a specific instance of an entity that can instantiate z0 and satisfy both P1(x,z0) and P2(z0,z1)^P3(z1,z2). Presenting is the act or process of introducing, displaying, or showcasing something. Based on new predicates and related facts, we generate unary facts from existing predicates and their corresponding instances.",
    "The dataset of five hundred poker hands is comprised of unique configurations, each featuring five playing cards with attributes such as suit and rank. This collection can be related to logical statements like Pt(x, z0), which takes two arguments and returns a value. Furthermore, entities satisfying the head predicate are connected through predicates where the second argument is a type or class. In this context, unary predicates play a crucial role in defining relationships between variables. For instance, country(z1, z1) can be seen as a representation of sovereign nations with their own governments. Similarly, song and class illustrate how entities can instantiate specific types or categories. The experiment highlights the importance of predicate terms and their connections to broader concepts like predicates and logical statements.",
    "The concept of an entity, such as a place or person, has a broader definition that outlines its specific understanding. A process P1' transforms input values into new outputs, which can be connected to other processes like P2'. The object argument of this predicate connects entities forming the last predicate's head. For instance, Car is related to t and human is linked to predicates. Then, a set of pairs satisfies the head of r denoted by headr(e). Definition 1 provides a formal statement explaining these concepts. Additionally, unary predicates represent class assertions expressed in RDF triples as (e, rdf:type, P) where P is a class or datatype. The number of entities that join the head of the rule to its body can be counted. It occurs in at least two distinct predicate terms like z0 here and otherwise it's open. Binary predicates take two arguments and return truth values. Once in the straight form, the object argument connects the head. Human(x) is a member of Homo sapiens.",
    "In this context, entities refer to abstract representations or groupings of people, places, things, or concepts. A person in a specific use case has a broader term that encompasses all entities. The predicate Pt(x, y) relates two variables and can be instantiated by P1(x, z0). Measures are quantifiable indicators used to quantify confidence levels. Experimental evaluations involve systematic investigations designed to test hypotheses or evaluate phenomena. Variables represent values that can take on different states in mathematical models. Each card is described using two attributes: suit and rank. The predicate of the head connects entities satisfying a subject predicate to entities forming an object predicate along a path of predicates.",
    "The experimental evaluation of this experiment involves applying P2(z0, z1) to determine whether a logical statement satisfies headr(e). Let Definition 1 provide the context for understanding what constitutes a pair of entities. It's essential to recognize that these pairs can comprise various occupations such as singer or entrepreneur. The target predicate Pt is crucial in defining how variables like e3 and e5 relate to each other through an example, illustrating fullHouse(x) where x is a specific poker hand. Furthermore, the procedure for evaluating next steps involves considering entities with only a type or class declaration, ultimately leading to a logical statement that can be applied to father(x, z2).",
    "In this knowledge graph, we see various entities and predicates that describe relationships between them. For instance, 'Free variables' has a broader term of 'variables', indicating its connection to mathematical concepts. Similarly, 'human( z2 )' relates to 'predicates', while 'Car(r)' is connected to 'The predicate'. We also observe the role of 'it occurs in at least two distinct predicate terms...' as an argument for both 'The predicate arguments' and 'The predicate'. Furthermore, we see connections between entities such as 'an identifier for an object...' being related to 'Definition 3', while 'a rule are closed' is connected to 'country'. The graph also highlights the relationships between predicates like 'capitalOf' and 'entities', as well as definitions like 'P = { P1, ..., Pm } be the set of all predicates in F.' which relates to 'Definition 1'. Lastly, we see connections between mathematical concepts such as 'Cardinality' being related to 't', while 'human(x)' is connected to 'entities'.",
    "In the realm of poker, a full house represents a specific type of hand. This concept has a broader term, which is The predicate - a logical statement that connects entities satisfying subject predicates to entities forming object predicates along a path of predicates. A deck with thirteen ranks and four suits serves as the foundation for this game, where each card has its own rank. Humans are part of the set of entities that can satisfy target predicates. For example, Mary lives in Canberra, Australia, which is an instance of a tree - a hierarchical structure representing connected entities with specific types and relationships. A song's metadata includes information about its album, record label, producers, genre, and artist. When we consider entity x as a song, it must be part of an album y that has a record label z, forming a logical statement. The set P = {P1,...,Pm} represents the collection of all predicates in F. Variables like x and z_i are used to represent entities in these predicate logic expressions. A mathematical function P1(z0,x) is another example of such an expression. Some songs require at least two producers, which highlights the importance of this group of individuals responsible for creating or overseeing creative content. Definition 3 outlines a specific concept description that can be applied to various domains. Each card in a deck has its own suit and rank attributes. Entities that satisfy target predicates are those that meet certain criteria or conditions. Presenting something involves introducing, displaying, or showcasing it. P2_1 refers to a predicate argument with the same variable as another predicate argument, satisfying specific similarities and properties. An identifier for an object like a place or person serves as a unique label used to represent locations, individuals, or things. When all variables in an open path rule have been instantiated, making it impossible to further instantiate any remaining free variables, then the rule is closed - a situation that can occur when dealing with countries and their governments.",
    "The set of entities {e2, e3} with count 2 has a broader term 'count'. Opera singers are classified under 'classes and types', while Ei represents variables. The unary predicate P(e) satisfies Definition 1, which outlines the head of relation r denoted as headr(e). Each entity consists of its body at the tail. Unary predicates take one argument or entity, such as Pi representing the ratio of a circle's circumference to its diameter. Country is an example of entities that can be classified under 'entities'. An identifier for an object like a place or person serves as an explanation for Definition 2. We conclude in Section 6.",
    "The concept of certain properties defines a set of characteristics that satisfy the head of a relation, such as citizenOf(x, z1). A song has an album with a record label and two producers. The unary predicate human constrains entities to be 'human' and specifies three paths: father(x, z2) ^ human(z2), nativeLanguage(x, z3) ^ language(z3), and country(z1, z1). Predicates and entities are connected by the target predicate Pt., which relates a song's metadata. The composite predicate P1(x, z1) takes one or more inputs to produce an output. Full House is a type of poker hand that satisfies certain properties.",
    "The concept of having the same head and body shares a common characteristic with features. A song that requires at least two producers has a broader term, which is a specific type of musical composition. The set of numbers from 1 to 28 can be considered as a feature or property. Unary predicates take a single argument and return a truth value, similar to the predicate that connects entities satisfying subject predicates to object predicates along a path. A deck with thirteen ranks and four suits has distinct designs or patterns used in card games like Poker. An entity is an identifier for objects such as places or people. Manual mode refers to performing actions without relying on automated systems. Human beings are members of the species Homo sapiens, which can be considered as features or properties. The head of a relation that satisfies a set of entity pairs has a broader term, similar to unary predicates. A poker hand is drawn from a standard deck with thirteen ranks and four suits. An identifier for an object such as a place or person serves as a definition. Definition 3 outlines specific ideas or principles. Mathematical expressions like P1(z0,x) have broader terms that represent logical statements.",
    "In this knowledge graph, we see various entities and their relationships. For instance, 'human(2)' has certain properties that define it as a member of the species Homo sapiens. A song can have multiple producers, an album with a record label, genre, and artist who is a musical artist. The Intel Xeon CPU E5-2650 v4 processor runs at 2.20 GHz and has 66 GB RAM. We also see entities like 'e' which serves as an identifier for objects or people, and predicates that describe their relationships. Furthermore, we have concepts such as type, feature, genre, deck, High Card, Each hand, human expert, birth date, An entity, unary predicates, The predicate, country(1), and Definition 3.",
    "In this context, uncertainty refers to a state or condition characterized by lack of clarity. Phenomena can be observed and studied, but they may also exhibit unpredictability due to their inherent complexity. A song typically has at least two producers, which highlights the importance of collaboration in creative endeavors. The entity e3 is an instance that satisfies both predicates P1(x,z0) and P2(z0,z1)^P3(z1,z2). Human beings are capable of logical thought and can express themselves through music, as seen in album y. A song's metadata includes its record label, producers, genre, and artist, which provides context for understanding the creative process. The predicate 'citizenOf(x,z1)' highlights the importance of nationality or citizenship in defining one's identity.",
    "Mary, a person who lives in Canberra and Australia, has occupations such as singing or entrepreneurship. She also has an album with a record label, two producers, and a genre. The song she sings about her life features at least one producer. Additionally, Mary's place of birth is recorded along with other facts about her. Furthermore, we allow unary predicates to describe various aspects of her life.",
    "Similarly accomplished scientists from different countries have achieved remarkable levels of accomplishment. Some notable examples include Mary, who resides in Canberra and has a fascinating fact about her life. Additionally, there are opera singers who are professional vocalists specializing in classical music compositions. Furthermore, songs often come with albums featuring record labels, producers, genres, and artists. Interestingly, some countries like Australia have their own unique characteristics."
  ],
  "times": [
    2516.3965258598328
  ]
}