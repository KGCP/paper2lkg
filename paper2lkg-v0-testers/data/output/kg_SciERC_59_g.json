{
  "iri": "Paper-59",
  "title": "P05-1046",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-59-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-59-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-1",
              "text": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-2",
              "text": "We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-3",
              "text": "Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-4",
              "text": "However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions ."
            },
            {
              "iri": "Paper-59-Section-1-Paragraph-1-Sentence-5",
              "text": "In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "The applicability of many current information extraction techniques is severely limited by the need for supervised training data . We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion . Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions . In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .",
  "kg2text": [
    "General unsupervised HMM learning improves the learned structure, which in turn enhances the performance of semi-supervised methods. This learning approach has a broader term known as unsupervised methods and is also related to hidden Markov models. By exploiting simple prior knowledge, the learned structure can be significantly improved. Semi-supervised methods make use of small amounts of labeled data, which can lead to better learning outcomes. Additionally, small amounts of prior knowledge can be utilized to learn effective models, particularly in field structured extraction tasks. The concept of labeled examples encompasses 50 labeled examples, which are a subset of labeled data, while 400 unlabeled examples fall under the broader category of unlabeled examples. Hidden Markov models provide a generative model that represents the underlying structure of data, and the quality of the learned structure can be enhanced through the exploitation of simple prior knowledge.",
    "Unsupervised methods are capable of attaining accuracies that can be comparable to those achieved by supervised methods, despite many current information extraction techniques being limited by the need for supervised training data. In this context, small amounts of labeled data and labeled examples are both broader terms that fall under labeled data, which itself encompasses supervised training data. Furthermore, simple prior knowledge plays a crucial role in exploiting desired solutions, enhancing the effectiveness of unsupervised methods. The quality of the learned structure, which is a broader term for quality, can significantly improve when prior knowledge is utilized. Field structured extraction tasks, such as classified advertisements and bibliographic citations, represent specific challenges within our domains, which are encompassed by the broader term domains. Overall, the relationship between these entities highlights the interplay between unsupervised and supervised methods in the realm of information extraction."
  ],
  "times": [
    7.564733505249023
  ]
}