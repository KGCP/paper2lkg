{
  "iri": "Paper-HDGI_A_Human_Device_Gesture_Interaction_Ontology_for_the_Internet_of_Things",
  "title": "HDGI: A Human Device Gesture Interaction Ontology for the Internet of Things",
  "authors": [
    "Madhawa Perera",
    "Armin Haller",
    "Sergio J. Rodr\u0131\u0301guez M\u00e9ndez",
    "and Matt Adcock"
  ],
  "keywords": [
    "ontology",
    "gesture",
    "semantic web",
    "Internet of Things",
    "gesture interfaces"
  ],
  "sections": [
    {
      "iri": "Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-1-Sentence-1",
              "text": "Gesture-controlled interfaces are becoming increasingly popular with the growing use of Internet of Things (IoT) systems."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-2",
              "text": "In particular, in automobiles, smart homes, computer games, and Augmented Reality (AR) / Virtual Reality (VR) applications, gestures have become prevalent due to their accessibility to everyone."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-3",
              "text": "Designers, producers, and vendors integrating gesture interfaces into their products have also increased in numbers, giving rise to a greater variation of standards in utilizing them."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-4",
              "text": "This variety can confuse a user who is accustomed to a set of conventional controls and has their own preferences."
            },
            {
              "iri": "Section-1-Paragraph-1-Sentence-5",
              "text": "The only option for a user is to adjust to the system even when the provided gestures are not intuitive and contrary to a user\u2019s expectations."
            }
          ]
        },
        {
          "iri": "Section-1-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-1-Paragraph-2-Sentence-1",
              "text": "This paper addresses the problem of the absence of a systematic analysis and description of gestures and develops an ontology which formally describes gestures used in Human Device Interactions (HDI)."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-2",
              "text": "The presented ontology is based on Semantic Web standards (RDF, RDFS, and OWL2)."
            },
            {
              "iri": "Section-1-Paragraph-2-Sentence-3",
              "text": "It is capable of describing a human gesture semantically, along with relevant mappings to affordances and user/device contexts, in an extensible way."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-2",
      "subtitle": "Introduction",
      "paragraphs": [
        {
          "iri": "Section-2-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-1-Sentence-1",
              "text": "Gesture-based systems are becoming widely available and explored as methods for controlling interactive systems."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-2",
              "text": "Especially in modern automobiles, smart homes, computer games, and Augmented Reality (AR) and Virtual Reality (VR) applications, gestures have become prevalent due to their accessibility to everyone."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-3",
              "text": "Most of these gesture interactions consist of physical movements of the face, limbs, or body and allow users to express their interaction intentions and send out corresponding interactive information to a device or a system."
            },
            {
              "iri": "Section-2-Paragraph-1-Sentence-4",
              "text": "However, most of the gestural interfaces are built based on a manufacturer\u2019s design decision."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-2-Sentence-1",
              "text": "Introducing the concept of 'guessability of a system' in 2005, Wobbrock et al. emphasize that a user\u2019s initial attempts at performing gestures, typing commands, or using buttons or menu items must be met with success despite the user\u2019s lack of knowledge of the relevant symbols."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-2",
              "text": "Their study enables the collection of end users\u2019 preferences for symbolic input and is considered the introduction of Gesture Elicitation Studies (GES)."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-3",
              "text": "Since then, many researchers have attempted to define multiple gesture vocabularies."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-4",
              "text": "However, a majority of them are limited in their scope and specific uses."
            },
            {
              "iri": "Section-2-Paragraph-2-Sentence-5",
              "text": "As a result, an impressive amount of knowledge has resulted from these GES, but it is currently cluttered."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-3-Sentence-1",
              "text": "There are multiple studies that show 'best gestures' for the same referent, where the referent is the effect of a gesture or the desired effect of an action which the gestural sign refers to."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-2",
              "text": "Hence, there are redundant gesture vocabularies."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-3",
              "text": "If all the knowledge of GES is properly linked, researchers could find gesture vocabularies that are defined for similar referents."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-4",
              "text": "However, a lack of linked data in this area has resulted in researchers conducting new GES whenever they need a particular gesture-referent mapping instead of using existing knowledge."
            },
            {
              "iri": "Section-2-Paragraph-3-Sentence-5",
              "text": "Hence, we see the necessity of a gesture ontology that can describe gestures with their related referents and facilitate automated reasoning."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-4-Sentence-1",
              "text": "Further, there currently exist several sensors, such as Microsoft Kinect, allowing out-of-the-box posture or movement recognition, which allows developers to define and capture mid-air gestures and use them in various applications."
            },
            {
              "iri": "Section-2-Paragraph-4-Sentence-2",
              "text": "With the advancements in AR and VR, the use of gestural interfaces has increased as these immersive technologies tend to use more intuitive Human-Computer Interaction (HCI) techniques."
            },
            {
              "iri": "Section-2-Paragraph-4-Sentence-3",
              "text": "All these systems have the capability to detect rich gestural inputs."
            },
            {
              "iri": "Section-2-Paragraph-4-Sentence-4",
              "text": "This has resulted in designers, developers, producers, and vendors integrating gesture interfaces into their products, contributing to a surge in their numbers and causing greater variation in ways of utilizing them."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-5-Sentence-1",
              "text": "Riener et al. also show that, most of the time, system designers define gestures based on their own preferences, evaluate them in small-scale user studies, apply modifications, and teach end users how to employ certain gestures."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-2",
              "text": "Further, they state that this is problematic because people have different expectations of how to interact with an interface to perform a certain task."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-3",
              "text": "This could confuse users who are accustomed to a set of conventional controls."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-4",
              "text": "Most of the time, these systems have either binary or a few choices when it comes to gesture selection."
            },
            {
              "iri": "Section-2-Paragraph-5-Sentence-5",
              "text": "Therefore, users do not have much of a choice even though the manufacturer-defined gestures are undesirable or counter-intuitive."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-6",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-6-Sentence-1",
              "text": "For example, if we take Microsoft HoloLens, its first version has a 'bloom' gesture to open its 'start' menu."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-2",
              "text": "In contrast, in HoloLens 2, a user has to pinch their thumb and index finger together while looking at the start icon that appears near a user\u2019s wrist when they hold out their hand with their palm facing up, to open the start menu."
            },
            {
              "iri": "Section-2-Paragraph-6-Sentence-3",
              "text": "Optionally, they can also tap the icon that appears near the wrist using their other hand."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-7",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-7-Sentence-1",
              "text": "BMW\u2019s iDrive infotainment system expects users to point a finger to the BMW iDrive touchscreen to accept a call, whereas Mercedes-Benz\u2019 User Experience (MBUX) multimedia infotainment system uses the same gesture to select an icon on their touchscreen."
            },
            {
              "iri": "Section-2-Paragraph-7-Sentence-2",
              "text": "Further, online search engines currently do not provide sufficient information for gesture-related semantics."
            },
            {
              "iri": "Section-2-Paragraph-7-Sentence-3",
              "text": "For example, a search query to retrieve gestures to answer a call in a car would not provide relevant gesture vocabularies supported by different vendors."
            },
            {
              "iri": "Section-2-Paragraph-7-Sentence-4",
              "text": "Designers and developers have to find individual studies separately and read or learn necessary data manually."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-8",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-8-Sentence-1",
              "text": "Being able to retrieve semantics and refer to a central location that maps all the available gestures to the affordance of answering a call in a car would be convenient for designers and developers in such situations."
            },
            {
              "iri": "Section-2-Paragraph-8-Sentence-2",
              "text": "Additionally, understanding the semantics of these gestures and inter-mapping them will help to bring interoperability among interfaces, increasing User Experience (UX)."
            },
            {
              "iri": "Section-2-Paragraph-8-Sentence-3",
              "text": "The problem is how to do this mapping."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-9",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-9-Sentence-1",
              "text": "Our approach is to design an ontology to map existing and increasingly prolific gesture vocabularies and their relationships to systems with the intention of providing the ability to understand and interpret user gestures."
            },
            {
              "iri": "Section-2-Paragraph-9-Sentence-2",
              "text": "Henceforth, users are individually shown the desired effect of an action, called a referent, to their preferred gestures."
            },
            {
              "iri": "Section-2-Paragraph-9-Sentence-3",
              "text": "Villarreal-Narvaez et al.'s most recent survey paper shows that a majority of gestures are performed using the upper limbs of the human body, i.e., hands."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-10",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-10-Sentence-1",
              "text": "Thereby keeping extensibility in mind, we designed a Human Device Gesture Interaction (HDGI) ontology to describe and map existing and upcoming upper limb related gestures along with relevant device affordances."
            },
            {
              "iri": "Section-2-Paragraph-10-Sentence-2",
              "text": "This allows systems to query the ontology after recognizing the gesture to understand its referents without having to be pre-programmed."
            },
            {
              "iri": "Section-2-Paragraph-10-Sentence-3",
              "text": "This further helps the personalization of gestures for particular sets of users."
            },
            {
              "iri": "Section-2-Paragraph-10-Sentence-4",
              "text": "As such, a user does not have to memorize a particular gesture for each different system, which improves system reliability."
            }
          ]
        },
        {
          "iri": "Section-2-Paragraph-11",
          "sentences": [
            {
              "iri": "Section-2-Paragraph-11-Sentence-1",
              "text": "This paper describes the HDGI ontology and its sample usage and state of the art in this area."
            },
            {
              "iri": "Section-2-Paragraph-11-Sentence-2",
              "text": "First, in Section 2, we discuss existing approaches to address the problem of ubiquitousness in human-device gesture interactions."
            },
            {
              "iri": "Section-2-Paragraph-11-Sentence-3",
              "text": "In Section 3, we describe the syntax, semantics, design, and formalization of HDGI v0.1, and the rationale behind such a design."
            },
            {
              "iri": "Section-2-Paragraph-11-Sentence-4",
              "text": "In Section 4, we illustrate tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices."
            },
            {
              "iri": "Section-2-Paragraph-11-Sentence-5",
              "text": "This serves as an evaluation of the expressive power of our ontology and provides developers and designers with a tool on how to integrate the HDGI ontology in their development."
            },
            {
              "iri": "Section-2-Paragraph-11-Sentence-6",
              "text": "We conclude and discuss future work in Section 5."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-3",
      "subtitle": "Related Work",
      "paragraphs": [
        {
          "iri": "Section-3-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-1-Sentence-1",
              "text": "A large number of studies can be found dealing with the problem of hand gesture recognition and its incorporation into the design and development of gestural interfaces."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-2",
              "text": "In most of these cases, gestures are predefined with their meaning and actions."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-3",
              "text": "Yet, the studies do seem to explore the capability of identifying the relationship beyond predefined mappings of a gesture."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-4",
              "text": "Thus, we see very few studies that have attempted to define and formalise the relationship between each gesture."
            },
            {
              "iri": "Section-3-Paragraph-1-Sentence-5",
              "text": "A review conducted by Villarreal Narvaez et al. in 2020 shows that gesture recognition has not yet reached its peak, which indicates that there will be many more gesture-related vocabularies in the future, consequently increasing the need to have interoperability between them."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-2-Sentence-1",
              "text": "One approach that has been adopted by researchers is to define taxonomies, enabling designers and manufacturers to use standard definitions when defining gesture vocabularies."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-2",
              "text": "Following this path, Scoditti et al. proposed a gestural interaction taxonomy in order to guide designers and researchers, who need an overall systematic structure that helps them to reason, compare, elicit, and create the appropriate techniques for the problem at hand."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-3",
              "text": "Their intention is to introduce system-wide consistent languages with specific attention for gestures."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-4",
              "text": "However, those authors do not map existing gesture vocabularies with semantical relationships."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-5",
              "text": "Following this, Choi et al. developed a 3D hand gesture taxonomy and notation method."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-6",
              "text": "The results of this study can be used as a guideline to organize hand gestures for enhancing the usability of gesture-based interfaces."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-7",
              "text": "This again follows a similar approach to Scoditti et al."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-8",
              "text": "However, this research is restricted to 6 commands (43 gestures) of a TV and blinds that were used in the experiment."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-9",
              "text": "Therefore, further experiments with an increased number of commands are necessary to see the capability and adaptability of the proposed taxonomy and notation method."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-10",
              "text": "Also, this notation uses numeric terminology which is not easily readable unless designers strictly follow a reference guide that is provided."
            },
            {
              "iri": "Section-3-Paragraph-2-Sentence-11",
              "text": "In addition, they mention that the size or speed of hand gestures have not been considered in their approach."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-3-Sentence-1",
              "text": "Moving beyond taxonomies, there is also existing research using ontologies."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-2",
              "text": "Osumar et al. have modelled a gesture ontology based on a Microsoft Kinect-based skeleton which aims to describe mid-air gestures of the human body."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-3",
              "text": "Their ontology mainly focuses on capturing the holistic posture of the human body, hence misses details like the finger pose or movements and a detailed representation of the hand."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-4",
              "text": "In addition, the ontology is not openly shared, hence it prevents use and extensibility."
            },
            {
              "iri": "Section-3-Paragraph-3-Sentence-5",
              "text": "Their main contribution is to have a sensor-independent ontology of body-based contextual gestures, with intrinsic and extrinsic properties, where mapping different gestures with their semantic relationships to affordances is not considered."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-4-Sentence-1",
              "text": "Khairunizam et al. have conducted a similar study with the intention of addressing the challenge of how to increase the knowledge level of computational systems to recognize gestural information with regard to arm movements."
            },
            {
              "iri": "Section-3-Paragraph-4-Sentence-2",
              "text": "In their research, they have tried to describe knowledge of the arm gestures and attempted to recognize it with a higher accuracy."
            },
            {
              "iri": "Section-3-Paragraph-4-Sentence-3",
              "text": "This can be identified as an interesting study where the authors have used Qualisys motion capture to capture the movement of the user\u2019s right arm when they perform an arm gesture."
            },
            {
              "iri": "Section-3-Paragraph-4-Sentence-4",
              "text": "However, their focus was mainly on recognizing geometrical gestures and the gesture set was limited to 5 geometrical shapes."
            },
            {
              "iri": "Section-3-Paragraph-4-Sentence-5",
              "text": "Again, their ontological framework does not consider the mapping of other gestures that carry similar referents."
            }
          ]
        },
        {
          "iri": "Section-3-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-3-Paragraph-5-Sentence-1",
              "text": "Overall, the attempts above have a different scope compared to our ontology."
            },
            {
              "iri": "Section-3-Paragraph-5-Sentence-2",
              "text": "Our focus is not on modelling the infinite set of concepts, features, attributes, and relationships attached to arm-based gestures."
            },
            {
              "iri": "Section-3-Paragraph-5-Sentence-3",
              "text": "We do not consider gestures that do not carry a referent to a particular affordance of a device."
            },
            {
              "iri": "Section-3-Paragraph-5-Sentence-4",
              "text": "Nonetheless, our ontology is extensible to allow the addition of emerging gestures with a referent to an affordance or to be extended to other body parts, i.e., extending the gestures beyond the upper limbs of the human body."
            },
            {
              "iri": "Section-3-Paragraph-5-Sentence-5",
              "text": "As a best practice, we have used existing ontologies whenever they fit and provided mappings to concepts and properties in these ontologies."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-4",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontology",
      "paragraphs": [
        {
          "iri": "Section-4-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-1-Sentence-1",
              "text": "The HDGI ontology models the pose and movement of human upper limbs that are used to interact with devices."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-2",
              "text": "This ontology describes gestures related to device interactions and which are performed using a human's upper limb region."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-3",
              "text": "It maps affordances and human gestures to facilitate devices and automated systems to understand different gestures that humans perform to interact with the same affordances."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-4",
              "text": "Additionally, it acts as a dictionary for manufacturers, designers, and developers to search and identify the commonly used gestures for certain affordances, and to understand the shape and dynamics of a certain gesture."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-5",
              "text": "The ontology is developed with a strong focus on flexibility and extensibility, allowing device manufacturers, designers, and users to introduce new gestures and map their relations to necessary affordances."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-6",
              "text": "Most importantly, this does not enforce designers and manufacturers to follow a standard but maps the ubiquitousness in gesture vocabularies by linking them appropriately."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-7",
              "text": "The aim of this study is to define a semantic model of gestures combined with its associated knowledge."
            },
            {
              "iri": "Section-4-Paragraph-1-Sentence-8",
              "text": "As such, GES becomes more permissive, which opens up the opportunity to introduce a shareable and reusable gesture representation that can be mapped according to the relationships introduced in HDGI."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-2-Sentence-1",
              "text": "We defined a new namespace https://w3id.org/hdgi with the prefix hdgi for all the classes used in the ontology to be independent of external ontologies."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-2",
              "text": "However, we have provided relevant mappings to external ontologies where appropriate."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-3",
              "text": "We are using w3id.org as the permanent URL service."
            },
            {
              "iri": "Section-4-Paragraph-2-Sentence-4",
              "text": "Furthermore, the relevant code, data, and ontology are made available for the community via GitHub, allowing anyone interested to join as a contributor."
            }
          ]
        },
        {
          "iri": "Section-4-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-4-Paragraph-3-Sentence-1",
              "text": "Design Rationale"
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-2",
              "text": "We have arranged the classes and properties of the HDGI ontology to represent human upper limb region gestures with their associated affordances and context."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-3",
              "text": "The ontology is designed around a core that consists of seven main classes: hdgi:Gesture, hdgi:BodyPart, hdgi:Pose, hdgi:Movement, hdgi:Affordance, hdgi:Device, and hdgi:Human, establishing the basic relationships between those along with hdgi:Observer and hdgi:Context classes."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-4",
              "text": "This core ontology design pattern will be registered in the ontology design pattern initiative."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-5",
              "text": "Please note that the ontology introduces all classes and relationships in its own namespace, but for illustration purposes, we use their equivalent classes and properties from external ontologies when appropriate."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-6",
              "text": "All the classes and properties are expressed in OWL2 and we use Turtle syntax throughout our modelling."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-7",
              "text": "We use global domain and range restrictions on properties sparingly, but as much as possible, we use guarded local restrictions instead, i.e., universal and existential class restrictions for a specific property such that only for instances of that property with that class as the subject, the range of the property is asserted."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-8",
              "text": "This helps in the alignment of the ontology with other external ontologies, particularly if they also use guarded local restrictions."
            },
            {
              "iri": "Section-4-Paragraph-3-Sentence-9",
              "text": "We provide alignments to these ontologies as separate ontology files in GitHub."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-5",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontolog: Gesture",
      "paragraphs": [
        {
          "iri": "Section-5-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-1-Sentence-1",
              "text": "Gesture A hdgi:Gesture is defined in such a way that it distinguishes two atomic types of gestures, namely static and dynamic gestures."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-2",
              "text": "A dynamic gesture consists of exactly one start hdgi:Pose at a given time, exactly one end hdgi:Pose at a given time, an atomic hdgi:Movement, and involves a single hdgi:BodyPart at a time."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-3",
              "text": "However, since a gesture can have multiple poses and movements of multiple body parts, we provide a means to define a sequence of gestures."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-4",
              "text": "Since the ontology is designed in a way that it can capture and describe individual body parts separately, a gesture that involves multiple movements and poses of body parts can be described using the object property hdgi:includesGesture that aggregates hdgi:Gesture and through their mapping to Allen time puts them in sequence or concurrent."
            },
            {
              "iri": "Section-5-Paragraph-1-Sentence-5",
              "text": "That is, a gesture can contain one or more gestures."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-2-Sentence-1",
              "text": "To give a concrete example of the modeling of a dynamic gesture, we use a 'swipe gesture' performed with the right hand (named 'right hand swipe left') illustrated below in Listing 1.1 and Figure 2."
            },
            {
              "iri": "Section-5-Paragraph-2-Sentence-2",
              "text": "As per the description above, 'right hand swipe left' consists of eight atomic gestures."
            },
            {
              "iri": "Section-5-Paragraph-2-Sentence-3",
              "text": "Only some of these atomic gestures are shown in Figure 2 and listed in Listing 1.1 and each of those include a single body part, a start pose and an end pose, with a movement."
            }
          ]
        },
        {
          "iri": "Section-5-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-5-Paragraph-3-Sentence-1",
              "text": "For extensibility, we added several possible gesture subclasses such as hdgi:HandGesture, hdgi:ForearmGesture, hdgi:FacialGesture, hdgi:LegGesture, hdgi:UpperArmGesture etc."
            },
            {
              "iri": "Section-5-Paragraph-3-Sentence-2",
              "text": "However, at this moment only hand, forearm, and upper arm gestures are modeled in detail in HDGI."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-6",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontolog: BodyPart",
      "paragraphs": [
        {
          "iri": "Section-6-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-6-Paragraph-1-Sentence-1",
              "text": "For modeling body parts, we reuse and extend concepts and classes in the Foundational Model of Anatomy (FMA) ontology."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-2",
              "text": "Again, though we focus only on a human's upper limb region, the hdgi:BodyPart class is defined in an extensible way with the motive of allowing representation of further body parts to describe new poses in the future."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-3",
              "text": "We are not modeling all the biological concepts that are described in FMA, but only the relevant classes for HDI."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-4",
              "text": "While preserving FMA class definitions and structures, we define hdgi:UpperArm, hdgi:Forearm, hdgi:Palm, and hdgi:Finger as the basic building blocks of the 'upper limb region'."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-5",
              "text": "The hdgi:UpperArm class is an equivalent class to the Arm class in FMA."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-6",
              "text": "The hdgi:Finger class is further divided to represent each individual finger as hdgi:Thumb, hdgi:IndexFinger, hdgi:MiddleFinger, hdgi:RingFinger, and hdgi:LittleFinger and are mapped to the respective subclasses of a 'Region of hand' in FMA."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-7",
              "text": "These fingers are further divided into left and right entities."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-8",
              "text": "Figure 3 depicts each of these sections of the 'upper limb region'."
            },
            {
              "iri": "Section-6-Paragraph-1-Sentence-9",
              "text": "Thus, we define a gesture as a combination of one or more poses and-or movements involved by one or more of these eight sections of upper limb region."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-7",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontolog: Post",
      "paragraphs": [
        {
          "iri": "Section-7-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-7-Paragraph-1-Sentence-1",
              "text": "Each body part can be involved in a pose."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-2",
              "text": "In other words, a Pose must hdgi:involves one hdgi:BodyPart at a point in time."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-3",
              "text": "For each body part there is a corresponding, potential pose."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-4",
              "text": "Stepping down a layer of abstraction, the hdgi:Pose class describes the exact placement of a pose in a 3D space, by modeling the 'position' and 'rotation' of a pose."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-5",
              "text": "The hdgi:hasPosition and hdgi:hasRotation relationships are used for this mapping; e.g. hdgi:ThumbCurled -> hdgi:hasPosition -> xPosition."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-6",
              "text": "In order to avoid the problem of having different origin points based on the gesture recognition device configurations, the HDGI ontology always considers relative positions."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-7",
              "text": "That is, upper arm positions are always relative to the shoulder joint (Refer to Figure 3 - point A)."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-8",
              "text": "The position of a hdgi:ForearmPose is always relative to the elbow joint (cf. Figure 3 - point B)."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-9",
              "text": "Palm and finger positions are always relative to the wrist (cf. Figure 3 - point C)."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-10",
              "text": "Further, the hdgi:Position class must describe the local coordinate system that its hdgi:xPosition, hdgi:yPosition, and zPosition values are based on."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-11",
              "text": "Thus, every hdgi:Position must have a hdgi:hasLocalCoordinateSystem object property with a hdgi:LocalCoordinateSystem as its range."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-12",
              "text": "This is to avoid problems, such as different SDKs-systems using slightly different coordinate systems."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-13",
              "text": "For example, Unity3D11 is using a left-hand rule coordinate system where the Z-axis always points outwards from the users."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-14",
              "text": "In contrast, the leap-motion SDK uses a right-hand rule where the Z-axis is pointed inwards."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-15",
              "text": "In order to allow either type of modeling and to avoid unnecessary conversions steps, we separately model the hdgi:LocalCoordinateSystem class and hdgi:Position class relationship."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-16",
              "text": "The rotation of a pose can be represented in two different ways."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-17",
              "text": "Some systems use yaw (angle with y-axis), pitch (angle with x-axis), and roll (angle with z-axis) angles to describe the rotation of a 3D rigid body, whereas some systems use quaternions."
            },
            {
              "iri": "Section-7-Paragraph-1-Sentence-18",
              "text": "By allowing support for both of these representations (yet one at a time), we keep our model flexible and able to model data received from different manufacturers-devices."
            }
          ]
        },
        {
          "iri": "Section-7-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-7-Paragraph-2-Sentence-1",
              "text": "Further, a hdgi:Pose represents a static gesture."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-2",
              "text": "Thus, similar to the hdgi:Gesture class, a hdgi:Pose can contain one or more poses within itself."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-3",
              "text": "A hdgi:Pose always has a time stamp and involves a single body part at a time (thus, individual body parts can be modeled separately)."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-4",
              "text": "Again, for extensibility, we added several possible poses as subclasses such as hdgi:LegPose, hdgi:FootPose, etc."
            },
            {
              "iri": "Section-7-Paragraph-2-Sentence-5",
              "text": "However, at the moment we only model hdgi:UpperArm, hdgi:Forearm, hdgi:Palm, and each individual hdgi:Finger poses."
            }
          ]
        },
        {
          "iri": "Section-7-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-7-Paragraph-3-Sentence-1",
              "text": "Listing 1.2 provides an example of a pose modeling related to the gesture 'Right Hand Swipe Left'."
            },
            {
              "iri": "Section-7-Paragraph-3-Sentence-2",
              "text": "The example models the start pose and the end pose of the right forearm and the right palm."
            },
            {
              "iri": "Section-7-Paragraph-3-Sentence-3",
              "text": "As per the description above, each hdgi:Pose hdgi:used only hdgi:BodyPart and has exactly one hdgi:timestamp with a maximum of one hdgi:Position and a hdgi:Rotation (hdgi:Rotation could be modeled either using Euler angles (hdgi:xRotation (roll), hdgi:yRotation (pitch), hdgi:zRotation (yaw)) or hdgi:Quaternion based on received data)."
            },
            {
              "iri": "Section-7-Paragraph-3-Sentence-4",
              "text": "Listing 1.3 further explains the hdgi:Position and hasLocalCoordinateSystem mappings."
            },
            {
              "iri": "Section-7-Paragraph-3-Sentence-5",
              "text": "Each hdgi:Position has a maximum of one hdgi:xPosition, hdgi:yPosition, and hdgi:zPosition and exactly one hdgi:LocalCoordinateSystem."
            },
            {
              "iri": "Section-7-Paragraph-3-Sentence-6",
              "text": "Notice in hdgi:LocalCoordinateSystem, each axis direction is pre-known (enum), hence for hdgi:xAxisDirection it is either 'leftward' or 'rightward', for hdgi:yAxisDirection it is either 'upward' or 'downward', and for hdgi:zAxisDirection it is either 'outward' or 'inward'."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-8",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontolog: Movement",
      "paragraphs": [
        {
          "iri": "Section-8-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-8-Paragraph-1-Sentence-1",
              "text": "The hdgi:Movement class only relates to dynamic gestures and has no relationship to a hdgi:Pose."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-2",
              "text": "A hdgi:Movement consists of a predefined set of movements that we identified as sufficient to describe the movements of hdgi:UpperArm, hdgi:Forearm, hdgi:Palm, and hdgi:Finger."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-3",
              "text": "This is extensible for designers and developers to include their own new movements."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-4",
              "text": "As this is not tightly-coupled with other classes such as hdgi:Gesture, hdgi:Pose, and hdgi:BodyPart, the flexibility is there for customizations."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-5",
              "text": "Each hdgi:Movement is atomic (that is related to only one position change or one rotation change) and must have exactly a single hdgi:Duration."
            },
            {
              "iri": "Section-8-Paragraph-1-Sentence-6",
              "text": "This can be derived from hdgi:timestamp difference between start hdgi:Pose and end hdgi:Pose."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-9",
      "subtitle": "Human Device Gesture Interaction (HDGI) Ontolog: Affordances",
      "paragraphs": [
        {
          "iri": "Section-9-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-1-Sentence-1",
              "text": "According to Norman the term affordance refers to the perceived and actual properties of the thing that determines just how the thing could possibly be used."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-2",
              "text": "Later on, this view has become standard in Human Computer Interaction and Design."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-3",
              "text": "Further, Maier et al. define affordances to be potential uses of a device."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-4",
              "text": "This implies that the human is able to do something using the device."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-5",
              "text": "Hence affordances of a device can be stated as the set of all potential human behaviors that the device might allow."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-6",
              "text": "Therefore, Brown et al. conclude that affordances are context dependent action or manipulation possibilities from the point of view of a particular actor."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-7",
              "text": "This highlighted the necessity for us to model both an hdgi:Affordance and a hdgi:Context (both hdgi:UserContext and hdgi:DeviceContext) class when modeling Human Device Gesture Interactions."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-8",
              "text": "As a user's choice of gestures is heavily based on their context, to understand the correct intent it is important that HDGI can map both the context and affordance."
            },
            {
              "iri": "Section-9-Paragraph-1-Sentence-9",
              "text": "This helps systems to understand user specific gesture semantics and behave accordingly."
            }
          ]
        },
        {
          "iri": "Section-9-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-2-Sentence-1",
              "text": "In gesture interactions, necessary affordances are communicated by the user to a device via a gesture that is supported by the device."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-2",
              "text": "If there is an openly accessible gesture affordance mapping with automated reasoning, we could integrate multiple gesture recognition systems to cater for user needs, and thereby increase user experience (UX)."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-3",
              "text": "For example, assume that Device A has an affordance X and Device B has affordance Y."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-4",
              "text": "If a user performs a gesture which can only be detected by Device B but the user's intent is to interact with affordance X, by using the mappings in hdgi-ontology and the use of automated reasoning, Device B would be able to understand the user intent and communicate that to Device A accordingly."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-5",
              "text": "This further implies that it is the affordance that should be mapped to a gesture rather than the device."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-6",
              "text": "This is modeled as hdgi:Affordance -> hdgi:supportsGesture -> hdgi:Gesture, where an affordance can have none to many supported gestures."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-7",
              "text": "A hdgi:Device can be a host to multiple affordances and the same affordance can be hosted by multiple devices."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-8",
              "text": "Hence, hdgi:Affordance -> hdgi:affordedBy -> hdgi:Device has cardinality of many to many."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-9",
              "text": "Here, hdgi:Device is a sub class of sosa:Platform."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-10",
              "text": "SOSA (Sensor, Observation, Sample, and Actuator) is a lightweight but self-contained core ontology which itself is the core of the new Semantic Sensor Network (SSN) ontology."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-11",
              "text": "The SSN ontology describes sensors and their observations, involved procedures, studied features of interest, samples, and observed properties, as well as actuators."
            },
            {
              "iri": "Section-9-Paragraph-2-Sentence-12",
              "text": "We further reuse sosa:Sensor and sosa:Actuator and hdgi:ActuatableAffordance and hdgi:ObservableAffordance are subclasses of sosa:ActuatableProperty and sosa:ObservableProperty."
            }
          ]
        },
        {
          "iri": "Section-9-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-3-Sentence-1",
              "text": "In addition, the HDGI ontology models the relationship between hdgi:Device and a hdgi:DeviceManufacturer as there can be the same gesture mapped to different affordances by different vendors or the same affordance can be mapped to different gestures (refer to the BMW and Mercedes-Benz example in Section 1)."
            },
            {
              "iri": "Section-9-Paragraph-3-Sentence-2",
              "text": "We model this in HDGI through the hdgi:Device hdgi:manufacturedBy a hdgi:DeviceManufacturer relationship where a device must have just one manufacturer."
            }
          ]
        },
        {
          "iri": "Section-9-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-9-Paragraph-4-Sentence-1",
              "text": "Listing 1.4 provides an example modeling of hdgi:Affordance, hdgi:Device and hdgi:Context relationships corresponding to the description above."
            },
            {
              "iri": "Section-9-Paragraph-4-Sentence-2",
              "text": "Most importantly, this is one of the major contributions in this ontology and when correctly modeled, this will help systems to automatically identify the semantics of a user's gesture and perform the necessary affordance mapping through an interconnected knowledge base instead of predefined one to one mappings."
            },
            {
              "iri": "Section-9-Paragraph-4-Sentence-3",
              "text": "This allows gesture recognition systems to run gesture recognition, detection, mappings, and communication separately in independent layers."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-10",
      "subtitle": "Device Mappings to HDGI",
      "paragraphs": [
        {
          "iri": "Section-10-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-1-Sentence-1",
              "text": "In addition to ontology building and annotating, it is equally important to consider its integration and documentation as a part of ontology engineering."
            },
            {
              "iri": "Section-10-Paragraph-1-Sentence-2",
              "text": "Figure 4 illustrates a proof-of-concept implementation of the HDGI ontology."
            },
            {
              "iri": "Section-10-Paragraph-1-Sentence-3",
              "text": "Here we wrapped a set of predefined SPARQL endpoints with RESTful APIs, in order to make the integration with third party Software Development Kits and Services easier and faster."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-2-Sentence-1",
              "text": "The HDGI-Mapping Service is a fully API-driven RESTful web service, where designers, device manufacturers, and developers can refer to one place - the HDGI-gesture repository - to find currently available and contemporary gestures and their relevant mappings to device affordances."
            },
            {
              "iri": "Section-10-Paragraph-2-Sentence-2",
              "text": "In addition, APIs further allow them to define their own gesture vocabularies and map them and upload them to the gesture repository."
            },
            {
              "iri": "Section-10-Paragraph-2-Sentence-3",
              "text": "This means that their gesture vocabularies will be easily accessible to the research community."
            },
            {
              "iri": "Section-10-Paragraph-2-Sentence-4",
              "text": "We anticipate that this will help to reduce the redundant gesture vocabularies and increase the reuse of existing ones, eventually helping to reduce the ubiquitousness currently prominent in gestural interfaces."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-3-Sentence-1",
              "text": "In our study, we looked at the gesture vocabularies in the current literature and tried to map them into the ontology as a starting point."
            },
            {
              "iri": "Section-10-Paragraph-3-Sentence-2",
              "text": "This allows using the HDGI-service endpoints to query about available gesture vocabularies."
            },
            {
              "iri": "Section-10-Paragraph-3-Sentence-3",
              "text": "As we have made this an OpenSource project under Apache 2.0 license, anyone can contribute to the open GitHub code repository for further improvements."
            },
            {
              "iri": "Section-10-Paragraph-3-Sentence-4",
              "text": "In addition, they can deploy this service in their own private cloud, if necessary."
            },
            {
              "iri": "Section-10-Paragraph-3-Sentence-5",
              "text": "Either way, adhering to the HDGI ontology mappings will allow universal integration of gesture data instead of having a universal gesture standard that is not yet available and may never emerge."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-4-Sentence-1",
              "text": "Further information on HDGI mappings can be explored."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-2",
              "text": "Sample mapping service (the web application) code is available to anyone to download locally, and continue the integration with their gesture recognition software tools."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-3",
              "text": "The prerequisites to run the web application are Java version 1.9 or higher and an Apache Tomcat server."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-4",
              "text": "A 'how-to' documentation is provided."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-5",
              "text": "We have further added an API and architecture documentation which helps if someone needs to customize the web application itself, if they want to make customized SPARQL endpoints and to define new RESTful endpoints to suit any customizable needs, and to run as a private service."
            },
            {
              "iri": "Section-10-Paragraph-4-Sentence-6",
              "text": "A complete API documentation can also be found, and we are currently working on integrating the Swagger UI and Swagger codegen capabilities to the HDGI web app."
            }
          ]
        },
        {
          "iri": "Section-10-Paragraph-5",
          "sentences": [
            {
              "iri": "Section-10-Paragraph-5-Sentence-1",
              "text": "Thus, users can get a comprehensive view of the API, understand endpoint structures and try it online in real-time."
            },
            {
              "iri": "Section-10-Paragraph-5-Sentence-2",
              "text": "Further, with the integration of Swagger codegen, we will allow instant generation of API client stubs (client SDKs for APIs) from different languages including JavaScript, Java, Python, Swift, Android, etc., which will make the integration of the APIs into different gesture recognition software/services even faster and easier."
            }
          ]
        }
      ]
    },
    {
      "iri": "Section-11",
      "subtitle": "Conclusion",
      "paragraphs": [
        {
          "iri": "Section-11-Paragraph-1",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-1-Sentence-1",
              "text": "This work presents the Human Device Gesture Interaction (HDGI) ontology, a model of human device gesture interactions that describes gestures related to human device interactions and maps them with corresponding affordances."
            },
            {
              "iri": "Section-11-Paragraph-1-Sentence-2",
              "text": "This is an initial step towards building a comprehensive human device gesture interaction knowledge base with the ultimate purpose of bringing better user experience."
            },
            {
              "iri": "Section-11-Paragraph-1-Sentence-3",
              "text": "The HDGI ontology can assist gesture recognition systems, designers, manufacturers, and developers to formally express gestures and to carry automated reasoning tasks based on relationships between gestures and device affordances."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-2",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-2-Sentence-1",
              "text": "While developing the ontology, we extracted elements observed from existing gesture vocabularies defined in previous studies."
            },
            {
              "iri": "Section-11-Paragraph-2-Sentence-2",
              "text": "We also present a Web service interface, the HDGI Mapping service, that can be integrated with existing gesture recognition systems."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-3",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-3-Sentence-1",
              "text": "The intention and scope of the HDGI ontology can be summarized as follows: first, to describe gestures related to human device interaction performed using the human upper-limb region; second, to map the relationship between affordances and a particular gesture based on the user context, allowing devices to understand different gestures that humans perform to interact with the same affordances; and third, to act as a dictionary and a repository for manufacturers, developers, and designers to identify commonly used gestures for certain affordances, specify formally what a certain gesture means, and introduce new gestures if necessary."
            }
          ]
        },
        {
          "iri": "Section-11-Paragraph-4",
          "sentences": [
            {
              "iri": "Section-11-Paragraph-4-Sentence-1",
              "text": "As future work, there are several possible extensions that can be made to the ontology by incorporating more gesture types such as facial gestures and head gestures."
            },
            {
              "iri": "Section-11-Paragraph-4-Sentence-2",
              "text": "Furthermore, we are planning to release and deploy the HDGI RESTful service in the Cloud and release API clients to leading hand-gesture supported systems such as Microsoft HoloLens 2, Microsoft Kinect, and Soli."
            },
            {
              "iri": "Section-11-Paragraph-4-Sentence-3",
              "text": "Since gesture interactions in Mixed Reality are becoming increasingly popular, we plan to conduct several gesture elicitation studies using Microsoft HoloLens 2, especially to map gesture interactions in Mixed Reality to the HDGI ontology."
            }
          ]
        }
      ]
    }
  ],
  "summary": "Gesture-controlled interfaces are becoming increasingly popular with the growing use of Internet of Things (IoT) systems. In particular, in automobiles, smart homes, computer games, and Augmented Reality (AR) / Virtual Reality (VR) applications, gestures have become prevalent due to their accessibility to everyone. Designers, producers, and vendors integrating gesture interfaces into their products have also increased in numbers, giving rise to a greater variation of standards in utilizing them. This variety can confuse a user who is accustomed to a set of conventional controls and has their own preferences. The only option for a user is to adjust to the system even when the provided gestures are not intuitive and contrary to a user\u2019s expectations.\n\nThis paper addresses the problem of the absence of a systematic analysis and description of gestures and develops an ontology which formally describes gestures used in Human Device Interactions (HDI). The presented ontology is based on Semantic Web standards (RDF, RDFS, and OWL2). It is capable of describing a human gesture semantically, along with relevant mappings to affordances and user/device contexts, in an extensible way.\n\nGesture-based systems are increasingly used in various applications like automobiles, smart homes, and AR/VR due to their accessibility. These systems rely on physical movements to convey user intentions, but most are designed based on manufacturer preferences, leading to a lack of user-centered design. Wobbrock et al. introduced the concept of 'guessability' in 2005, highlighting the need for users to succeed in gesture interactions without prior knowledge. Despite numerous Gesture Elicitation Studies (GES), redundant gesture vocabularies exist, necessitating a gesture ontology to link related gestures and their referents. Current sensors like Microsoft Kinect enable gesture recognition, but designers often impose their preferences, which can confuse users. For instance, different systems like BMW's iDrive and Mercedes-Benz's MBUX use similar gestures for different functions. A centralized mapping of gestures would enhance interoperability and user experience. This paper proposes a Human Device Gesture Interaction (HDGI) ontology to map upper limb gestures to device affordances, allowing systems to interpret gestures without pre-programming. The paper details the HDGI ontology's design, syntax, and tools for integration with devices like Leap Motion and Oculus Quest, aiming to improve gesture personalization and system reliability.\n\nNumerous studies focus on hand gesture recognition and its integration into gestural interfaces, primarily using predefined gestures. However, few have formalized the relationships between gestures. A 2020 review by Villarreal Narvaez et al. indicates that gesture recognition is still evolving, necessitating interoperability among gesture vocabularies. Researchers have proposed taxonomies, such as Scoditti et al.'s gestural interaction taxonomy, to standardize gesture definitions, but these do not map existing vocabularies semantically. Choi et al. developed a 3D hand gesture taxonomy, limited to 6 commands, highlighting the need for broader experimentation. Other studies, like Osumar et al.'s gesture ontology, focus on mid-air gestures but lack detail and accessibility. Khairunizam et al. aimed to improve recognition of arm gestures using motion capture, yet their framework is limited to geometrical shapes. In contrast, our ontology aims to model gestures with specific affordances, allowing for extensibility to new gestures and body parts, while incorporating existing ontologies where applicable.\n\nThe HDGI ontology models human upper limb gestures for device interaction, mapping gestures to affordances to help devices understand human interactions. It serves as a resource for manufacturers and developers to identify common gestures and their dynamics, promoting flexibility and extensibility without enforcing standards. A new namespace, https://w3id.org/hdgi, has been created for the ontology, which is available on GitHub for community contributions. The ontology consists of seven main classes, including Gesture, BodyPart, and Device, establishing key relationships. It uses OWL2 and Turtle syntax, with a focus on local restrictions to align with external ontologies, which are also provided in separate files on GitHub.\n\nGesture is categorized into static and dynamic types. A dynamic gesture includes one start pose, one end pose, an atomic movement, and involves a single body part. Multiple gestures can be defined in sequence using the object property hdgi:includesGesture. For example, the 'right hand swipe left' consists of eight atomic gestures, each with a body part, start pose, end pose, and movement. The ontology allows for extensibility with subclasses like hdgi:HandGesture and hdgi:ForearmGesture, though only hand, forearm, and upper arm gestures are detailed in HDGI.\n\nWe utilize and expand upon the Foundational Model of Anatomy (FMA) ontology to model human upper limb parts. The hdgi:BodyPart class is designed to be extensible for future body part representations. We focus on relevant FMA classes, defining hdgi:UpperArm, hdgi:Forearm, hdgi:Palm, and hdgi:Finger as key components of the upper limb. The hdgi:UpperArm corresponds to the Arm class in FMA, while the hdgi:Finger class is subdivided into individual fingers: hdgi:Thumb, hdgi:IndexFinger, hdgi:MiddleFinger, hdgi:RingFinger, and hdgi:LittleFinger, which are also categorized by left and right. A gesture is defined as a combination of poses and movements involving these upper limb sections.\n\nEach pose involves one body part and is defined in 3D space by its position and rotation, using relationships like hdgi:hasPosition and hdgi:hasRotation. The HDGI ontology standardizes relative positions to avoid inconsistencies across devices, with positions based on joints (e.g., upper arm relative to the shoulder). The hdgi:Position class describes local coordinate systems to address variations in SDKs, such as Unity3D and leap-motion. Rotation can be represented using either Euler angles or quaternions for flexibility. A hdgi:Pose represents a static gesture with a timestamp and can contain multiple poses, currently modeling hdgi:UpperArm, hdgi:Forearm, Palm, and individual Fingers. Examples illustrate the modeling of poses like 'Right Hand Swipe Left', detailing the use of body parts, positions, and rotations.\n\nThe hdgi:Movement class focuses on dynamic gestures, independent of hdgi:Pose. It includes a predefined set of movements for hdgi:UpperArm, hdgi:Forearm, Palm, and Finger, allowing for extensibility by designers and developers. The class is flexible, not tightly coupled with hdgi:Gesture, hdgi:Pose, or hdgi:BodyPart. Each hdgi:Movement is atomic, relating to a single position or rotation change, and has a specific hdgi:Duration derived from the timestamp difference between start and end hdgi:Pose.\n\nAffordance refers to the perceived and actual properties of a device that determine its potential uses, as defined by Norman and later adopted in Human Computer Interaction. Maier et al. describe affordances as the set of possible human behaviors a device allows, while Brown et al. emphasize their context-dependent nature. This necessitates modeling both affordance and context in Human Device Gesture Interactions (HDGI) to understand user intent. Users communicate necessary affordances through gestures, and an effective mapping of gestures to affordances can enhance user experience. For instance, if a user gestures towards Device B but intends to interact with Device A's affordance, automated reasoning can bridge the gap. The HDGI ontology models relationships between devices, affordances, and manufacturers, allowing for multiple gestures and affordances across devices. This modeling aids systems in identifying gesture semantics and performing affordance mapping through a knowledge base, enabling independent layers for gesture recognition and communication.\n\nOntology engineering involves not only building and annotating but also integrating and documenting ontologies. The HDGI ontology implementation features a RESTful API-driven HDGI-Mapping Service, allowing designers and developers to access a gesture repository for contemporary gestures and their mappings. Users can define and upload their own gesture vocabularies, promoting reuse and reducing redundancy in gestural interfaces. The project is open-source under the Apache 2.0 license, enabling contributions and private cloud deployment. The service includes a web application with Java and Apache Tomcat prerequisites, along with comprehensive API and architecture documentation. Future enhancements include integrating Swagger UI for real-time API exploration and Swagger codegen for generating client SDKs in various programming languages, facilitating easier integration into gesture recognition software.\n\nThis work presents the Human Device Gesture Interaction (HDGI) ontology, a model of human device gesture interactions that describes gestures related to human device interactions and maps them with corresponding affordances. This is an initial step towards building a comprehensive human device gesture interaction knowledge base with the ultimate purpose of bringing better user experience. The HDGI ontology can assist gesture recognition systems, designers, manufacturers, and developers to formally express gestures and to carry automated reasoning tasks based on relationships between gestures and device affordances.\n\nWhile developing the ontology, we extracted elements observed from existing gesture vocabularies defined in previous studies. We also present a Web service interface, the HDGI Mapping service, that can be integrated with existing gesture recognition systems.\n\nThe intention and scope of the HDGI ontology can be summarized as follows: first, to describe gestures related to human device interaction performed using the human upper-limb region; second, to map the relationship between affordances and a particular gesture based on the user context, allowing devices to understand different gestures that humans perform to interact with the same affordances; and third, to act as a dictionary and a repository for manufacturers, developers, and designers to identify commonly used gestures for certain affordances, specify formally what a certain gesture means, and introduce new gestures if necessary.\n\nAs future work, there are several possible extensions that can be made to the ontology by incorporating more gesture types such as facial gestures and head gestures. Furthermore, we are planning to release and deploy the HDGI RESTful service in the Cloud and release API clients to leading hand-gesture supported systems such as Microsoft HoloLens 2, Microsoft Kinect, and Soli. Since gesture interactions in Mixed Reality are becoming increasingly popular, we plan to conduct several gesture elicitation studies using Microsoft HoloLens 2, especially to map gesture interactions in Mixed Reality to the HDGI ontology.",
  "kg2text": [
    "The Human Device Gesture Interaction (HDGI) ontology describes and maps gestures used in Human Device Interactions (HDI), providing a formal framework for understanding user gestures in various interactive systems. This ontology is represented by the HDGI, which also presents a systematic analysis of these gestures. In addition, the HDGI ontology, as a formal framework, describes and maps gestures in Human Device Interactions, further enhancing the understanding of gesture-controlled interfaces. This paper highlights the significance of the HDGI ontology and presents it as a major contribution to the field. Furthermore, HDGI v0.1 is a specific version of the ontology that also describes and maps gestures used in HDI. The relationships between gestures and their corresponding device affordances are systematically analyzed, with the gestures used in HDI mapping to the HDGI ontology mappings, facilitating improved user interaction across various devices.",
    "The presented ontology describes and is formally represented by gestures used in Human Device Interactions (HDI). This ontology facilitates the integration and interoperability of gesture data across various devices and applications through HDGI ontology mappings. HDGI v0.1 is a version of both the Human Device Gesture Interaction (HDGI) and the HDGI ontology, which defines these mappings. Gesture representation maps gestures used in HDI and facilitates the mapping of gestures to affordances within the HDGI ontology. This paper presents HDGI v0.1 and HDGI ontology mappings, highlighting the systematic analysis of gestures used in HDI. Furthermore, the presented ontology is a specific instance of this ontology, which describes gestures used in Human Device Interactions and has a broader term known as Human Device Gesture Interaction.",
    "Gestures used in Human Device Interactions (HDI) include the upper limb movements defined within the hdgi:Human class, which represents the human entity involved in these interactions. The gesture representation facilitates the mapping of these gestures to affordances as outlined in the HDGI ontology. Our ontology systematically describes gestures used in HDI, while gesture representation is modeled within this ontology, enhancing the understanding of gesture-based interactions. The Human Device Gesture Interaction (HDGI) ontology plays a crucial role in facilitating HDGI mappings, which are structured connections between human gestures and their corresponding device affordances. Furthermore, gestures used in HDI are a subset of gestures related to device interactions, indicating their specific relevance in controlling devices. This paper presents a systematic analysis and ontology for gesture representation, contributing to the overall framework. The HDGI ontology not only describes Human Device Gesture Interaction but also maps gestures to device affordances through HDGI mappings. Our approach is designed to facilitate the understanding of Human Device Gesture Interaction, which is a specific ontology that encompasses these interactions. The formalization of HDGI v0.1 describes and maps gestures used in HDI, ensuring interoperability in gesture-controlled systems. Additionally, the gesture ontology describes gestures used in HDI, further enriching the framework for gesture-based interactions.",
    "The HDGI ontology is a formal framework that includes the class hdgi:Human, which represents the human entity involved in gesture-based interactions. Within this framework, the Human Device Gesture Interaction (HDGI) describes gestures related to device interactions and facilitates the mapping of these gestures to their corresponding device affordances. HDGI v0.1 is a specific version of the HDGI ontology, which systematically describes and maps upper limb gestures. This paper presents HDGI mappings and discusses the Human Device Gesture Interaction, along with our approach that describes this ontology as a specific implementation of the HDGI ontology. Furthermore, the formalization of HDGI v0.1 serves as a structured representation of the Human Device Gesture Interaction, while the presented ontology acts as a formal representation of HDGI. The HDGI ontology also defines the mapping of gestures related to device interactions, enhancing the understanding of how these gestures are utilized in device interactions.",
    "The Human Device Gesture Interaction (HDGI) facilitates the understanding and interpretation of user gestures in various interactive systems, as defined by the gesture ontology. This ontology describes gestures related to device interactions, which are systematically linked through this mapping. The HDGI ontology serves as a formal representation of the formalization of HDGI v0.1, and it describes and maps device interactions effectively. Furthermore, the HDGI ontology is a specific implementation of the gesture ontology, enhancing the interoperability of gesture data. This paper presents our ontology, which includes a systematic analysis of gestures related to device interactions and describes this mapping in detail. Additionally, it presents the formalization of HDGI v0.1, which is represented by this ontology. The HDGI ontology maps the relationships between gestures and device affordances, thereby facilitating the mapping of gestures to affordances within the HDGI ontology through gesture representation. Notably, gestures used in Human Device Interactions (HDI) have a broader term, Gesture, which encompasses various physical movements employed in device interactions. Lastly, HDGI v0.1 is recognized as a version of the HDGI ontology mappings, contributing to the structured understanding of gesture-controlled interfaces.",
    "The HDGI facilitates the mapping of gestures to affordances through HDGI mappings, which are essential for understanding human-device interactions. HDGI v0.1 is a version of the presented ontology, which itself facilitates the integration and interoperability of gesture data across various devices and applications through HDGI ontology mappings. The Human Device Gesture Interaction serves as an ontology for HDGI, while our approach describes HDGI in detail. Within this framework, HDGI represents the human entity, hdgi:Human, and helps to express the concept of Gesture. Furthermore, HDGI is a specific implementation of our ontology, which aims to enhance the understanding of gesture interactions. It also facilitates the mapping of existing gesture vocabularies to specific system affordances, referred to as this mapping. The ontology is defined in terms of gestures related to device interactions, and HDGI v0.1 is a component of gesture representation, which facilitates the mapping of gestures to affordances through HDGI ontology mappings. The HDGI ontology describes this area of research, while this ontology introduces new gestures that can be utilized in interactions. The formalization of HDGI v0.1 serves as a formal representation of HDGI, and the presented ontology is modeled as gesture representation. Additionally, HDGI describes device interactions and is a specific implementation of the gesture ontology, which serves to enhance the expressive power of our ontology.",
    "HDGI v0.1 is a version of HDGI mappings, which are structured connections that facilitate the integration and interoperability of gesture data across various devices and applications in Human Device Interaction. The relationships introduced in HDGI have a broader term, which is HDGI itself. An ontology describes gestures used in Human Device Interactions (HDI), and this ontology allows for the representation of Devices. Human Device Gesture Interaction is also a version of HDGI v0.1, and our approach is similarly a version of HDGI v0.1. Gestures used in Human Device Interactions (HDI) have a broader term known as a gesture. HDGI ontology mappings facilitate Human Device Gesture Interaction and enable the mapping of gesture vocabularies through HDGI ontology mappings. The presented ontology facilitates the mapping of gestures to affordances within HDGI mappings. Additionally, HDGI v0.1 describes this mapping, which links existing gesture vocabularies to specific system affordances. The class hdgi:Human is represented by The presented ontology, which describes Human Device Gesture Interaction. Our ontology is a version of HDGI v0.1 and facilitates the integration of HDGI ontology mappings with our ontology.",
    "HDGI v0.1 describes and maps gestures related to device interactions, which are integral to the understanding of user gestures in Human Device Interactions. The HDGI ontology mappings facilitate this mapping, establishing structured connections between gestures and their corresponding device affordances. Our ontology serves as a formal representation of The presented ontology, which enables the mapping of gestures related to device interactions. Furthermore, the formalization of HDGI v0.1 is a structured representation that defines HDGI ontology mappings and is part of the broader HDGI v0.1 framework. This ontology also describes and maps device interactions, enhancing the integration of gesture data across various applications. Gesture representation plays a crucial role by facilitating the mapping of gestures to affordances within the HDGI ontology, thereby supporting Human Device Gesture Interaction. Our approach is modeled as gesture representation, which also models the human entity involved in these interactions, represented by hdgi:Human. Additionally, possible extensions can be made to this ontology to incorporate new gesture types, further broadening its applicability in gesture recognition systems.",
    "This paper develops an ontology that systematically describes device interactions, which are the gestures performed by humans to interact with devices. The presented ontology serves as a formal representation of the gesture ontology, enabling gesture recognition systems to better interpret user inputs. Our ontology is modeled as gesture representation, which facilitates the mapping of gestures to affordances within the context of Human Device Gesture Interaction. This mapping is crucial as it connects existing gesture vocabularies to specific system affordances, enhancing the understanding of user gestures. Furthermore, HDGI mappings, which are structured connections between human gestures and device affordances, are facilitated by our approach, allowing for a comprehensive understanding of gestures related to device interactions. The formalization of HDGI v0.1 describes gesture representation, which in turn maps to device interactions, ensuring that the relationships between gestures and their meanings are clearly defined. Overall, the ontology and its mappings play a significant role in improving gesture recognition and user experience in gesture-controlled interfaces.",
    "The HDGI ontology provides a framework for developers and designers to enhance user interaction through gesture-controlled interfaces. Within this ontology, the class hdgi:Gesture has a broader term known as gesture representation, which systematically models human gestures used in device interactions. Our ontology describes the concept of Human Device Gesture Interaction, which enables the mapping of existing gesture vocabularies to specific system affordances. Our approach facilitates this mapping by detailing how gestures related to device interactions are represented and understood. Specifically, our ontology is a component of hdgi:Human, which is performed by gestures related to device interactions. The formalization of HDGI v0.1 describes and maps HDGI mappings, which in turn map device interactions to their corresponding gestures. Additionally, the gesture ontology maps these HDGI mappings, ensuring a structured representation of gestures. Overall, our ontology describes the process of this mapping and the gestures related to device interactions, while the formalization of HDGI v0.1 also encompasses the description of Human Device Gesture Interaction and hdgi:Human.",
    "Human Device Gesture Interaction is a crucial component of device interactions, which are defined by the gestures performed by humans using their upper limbs to interact with devices. Our approach effectively maps these device interactions, relying on the foundational framework provided by the gesture ontology. This ontology facilitates the understanding of Human Device Gesture Interaction by describing the human entity involved, represented as hdgi:Human, which is integral to these interactions. Furthermore, our ontology serves as a formal representation of the formalization of HDGI v0.1, which not only describes device interactions but also enables the mapping of gestures related to these interactions. The formalization of HDGI v0.1 enhances this mapping process and systematically describes gestures related to device interactions. Additionally, the gesture ontology provides a comprehensive description of this mapping and the gestures involved, while also detailing sample usage scenarios. Gesture representation, which encompasses various gesture types, has a broader term known as Gesture, and this ontology can be extended by incorporating new gesture types to adapt to evolving interaction paradigms.",
    "The gesture ontology maps to device interactions, providing a structured framework for understanding how gestures facilitate communication with devices. Within this context, the HDGI ontology models the pose and movement of human upper limbs, which are essential for effective Human Device Interactions (HDI). Furthermore, gestures used in HDI have a broader term known as 'the gesture', which encompasses various forms of physical movements. Gesture interactions in Mixed Reality are particularly noteworthy as they map to the HDGI ontology, illustrating the integration of real and virtual elements in user interactions. Additionally, both Gesture and Gesture-controlled interfaces fall under the broader category of Human Device Gesture Interaction, emphasizing their significance in designing intuitive user experiences. The ontology also aids in mapping gestures related to device interactions, which are further classified under the term 'Gesture'. Our study has contributed to this ontology by mapping existing gesture vocabularies, enhancing the understanding of gesture representation, which itself is a broader term for a gesture. This ontology not only assists systems in querying and utilizing gesture data but also supports Designers in creating effective gesture-based interfaces. Tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices illustrate the practical applications of the HDGI ontology, showcasing its relevance in modern interactive systems.",
    "The relationships between gestures and device affordances are based on the concept of Gesture, which serves as a broader term encompassing various aspects of human-device interactions. This systematic analysis and description of gestures also falls under the broader category of Gesture, highlighting its significance in understanding user interactions. In this study, the aim is to define a semantic model of gestures that will enhance the mapping of user gestures to device affordances. The user's gesture, which is a specific instance of Gesture, plays a crucial role in this context. The HDGI ontology introduces all classes and relationships, providing a structured framework for understanding these interactions. Furthermore, the Human Device Gesture Interaction requires the modeling of hdgi:Context to effectively capture the nuances of gesture-based communication. The HDGI mappings, which are essential for linking gestures to their corresponding affordances, also fall under the broader term of mapping. Gestures used in Human Device Interactions (HDI) can be categorized into various types, including an arm gesture and the broader concept of Human Device Interactions. This ontology is recognized as one of the major contributions in the field, facilitating a deeper understanding of the connections between gestures, affordances, and contexts. Additionally, the design and development of gestural interfaces is a critical area that encompasses the broader term of gestural interfaces, which are integral to the implementation of these concepts in practical applications. New gestures, as innovative forms of interaction, also relate back to the overarching category of Gesture, while the hdgi:Pose class contributes to the gesture representation framework within the ontology. Overall, the HDGI ontology and its associated standards provide a comprehensive approach to modeling and understanding human-device interactions.",
    "The HDGI ontology acts as a comprehensive dictionary for manufacturers, designers, and developers, facilitating the understanding of gestures in human-device interactions. Within this framework, the gesture ontology encompasses various types of gestures, including a gesture, which serves as a broader term for specific actions like hdgi:Gesture and hdgi:UpperArmGesture. Figure 4 illustrates the HDGI ontology, showcasing its integration with gesture recognition technologies. Gestures are supported by devices, highlighting the interaction between user actions and device functionalities. Manufacturers, developers, and designers play a crucial role in introducing new gestures, which are systematically analyzed and described within the ontology. The relationships between gestures and device affordances further enhance the understanding of how gestures relate to device capabilities. The HDGI ontology is specifically designed to describe upper limb related gestures, ensuring a structured approach to gesture recognition. Additionally, the namespace https://w3id.org/hdgi is defined as a new namespace for the ontology, ensuring clarity and independence in gesture categorization. The presented ontology includes mappings to user/device contexts, emphasizing the importance of situational awareness in gesture-based interactions. Overall, the systematic analysis and description of gestures contribute to a semantic model that supports the development of gesture-controlled interfaces.",
    "Device B is a specific type of Device, which serves as a hardware device capable of detecting user gestures. This paper addresses the problem of the absence of a systematic analysis and description of gestures, highlighting the need for a structured framework in Human Device Interactions. Within the HDGI ontology, Gesture is a broader term that encompasses a gesture, and it can contain one or more gestures, which allows for the representation of complex movements. The semantic model of gestures is combined with its associated knowledge to enhance understanding of these interactions. The provided gestures, which may not align with user expectations, fall under the broader category of Gesture. This ontology is designed around seven main classes, including hdgi:Affordance, which is essential for modeling Human Device Gesture Interaction. Gesture affordance mapping is another important concept that links user gestures to their potential actions. New gestures are also categorized under a gesture, contributing to the evolving landscape of gesture recognition. Our approach focuses on designing an ontology that systematically describes these relationships. The HDGI-gesture repository serves as a centralized resource for contemporary gestures and their mappings, while gestures used in Human Device Interactions are recognized as human gestures that convey meaning through physical movements. The shape and dynamics of a certain gesture, along with gesture-related vocabularies, are crucial for ensuring effective communication in gesture-controlled interfaces.",
    "The HDGI ontology has a broader term known as the model, which represents human upper limb region gestures. Within the context of Human Device Interactions (HDI), it is recognized that HDI itself has a broader term, simply referred to as HDI. The hdgi:Gesture class encompasses various gesture types, indicating its broader categorization. Additionally, gesture representation is a concept that has a broader term, the gesture, which is crucial for understanding interactions. Gesture-controlled interfaces are categorized under the broader term of Gesture-based systems. The hdgi:UpperArmGesture is a specific type of gesture that falls under the broader category of a gesture. Furthermore, gesture vocabularies are classified under gesture representation, highlighting their role in defining gestures. The hdgi:Affordance supports the hdgi:Gesture, indicating a relationship between potential actions and gestures. The hdgi:HandGesture also falls under the broader term of a gesture, as does the hdgi:Gesture class, which is part of the broader classes in the ontology. Different gestures are categorized under the term Gesture, emphasizing the diversity of gestures used in interactions. The hdgi:Context includes the hdgi:UserContext, which is essential for understanding user interactions. Gesture representation further encompasses hand gestures, while gesture-related semantics also falls under the broader term of Gesture. The hdgi:Pose is another class that is categorized under Gesture, as are manufacturer-defined gestures. The concept of Gesture can be described using the hdgi:includesGesture property, which aggregates multiple gestures. Lastly, tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices are classified under the broader term of Device, illustrating their application in gesture recognition. Overall, Human Device Gesture Interaction has a broader term known as the gesture, encapsulating the study of gestures in device interactions.",
    "Currently available and contemporary gestures are a subset of gestures, which are defined as specific actions performed by users to communicate with devices. This ontology considers relative positions, which describe the spatial relationships of body parts in gestures. Additionally, mapping different gestures with their semantic relationships to affordances is a broader term that encompasses the concept of Gesture. Various gesture types also fall under this broader category. The Human Device Gesture Interaction framework is fundamentally linked to communication, as it facilitates the interaction between users and devices through gestures. Gestures related to device interactions are classified under the term 'the gesture', which includes gestures used in Human Device Interactions (HDI) and the provided gestures. The concept of Gesture allows users to effectively interact with devices. Evaluation plays a crucial role in assessing the expressive power of our ontology, which is designed to be extensible to incorporate emerging gestures. Gesture affordance mapping is another key aspect that links gestures to their corresponding device affordances and integrates with gesture recognition systems. Furthermore, gesture recognition device configurations are essential setups that define how devices interpret gestures, and they are categorized under the broader term Device. Overall, the Human Device Gesture Interaction (HDGI) ontology maps device affordances and assists developers in creating more intuitive gesture recognition systems.",
    "The HDGI-gesture repository serves as a vital resource to find and contains currently available and contemporary gestures, which are essential for designers and developers in the realm of Human Device Interaction. Gesture recognition devices are categorized under the broader term Device, highlighting their role in interpreting user gestures. Commonly used gestures fall under the broader category of Gesture, which encompasses various classes of movements and poses. The design and development of gestural interfaces is a systematic process that relates to gestural inputs, emphasizing the importance of integrating gestures into user interfaces. Gesture recognition systems are instrumental in running mapping processes that link gestures to their corresponding device affordances. This ontology can assist developers by providing a structured framework for understanding gestures and their applications. Gesture affordance mapping is a crucial aspect of mapping, linking user gestures to the functionalities they represent. The shape and dynamics of a certain gesture, along with gesture-related vocabularies, are fundamental components of a gesture, which is further defined within the gesture ontology. Device A, as a specific instance of a Device, is capable of receiving gesture-based interactions. Additionally, gestures related to device interactions are categorized as hand gestures, which are essential for effective communication with devices. The HDGI ontology, which encompasses a model for representing gestures, includes classes such as hdgi:Gesture and hdgi:Context, the latter of which includes hdgi:DeviceContext, providing a comprehensive understanding of the contextual factors influencing gesture recognition.",
    "The personalization of gestures encompasses a broader term known as Gesture, which includes various forms of user gestures. Within this context, hdgi:Pose is recognized as being similar to the hdgi:Gesture class, both of which contribute to the understanding of gesture-based interactions. Device B communicates with Device A, facilitating interaction through gesture recognition. Furthermore, Gesture-controlled interfaces are categorized under body-based contextual gestures, highlighting their role in user-device interactions. Users engage with devices, establishing communication through the Device framework. The concept of user specific gesture semantics also falls under the broader category of Gesture, as do their preferred gestures, which reflect individual interaction styles. The relationships between gestures and device affordances are essential for understanding the gesture itself, while gesture representation is linked to specific movements like an arm gesture. Geometrical gestures, too, are classified under the broader term Gesture. Additionally, a tool designed to integrate the HDGI ontology is categorized as a tool, and hdgi:ForearmGesture is another subclass under Gesture. The gesture ontology provides a framework that includes hand gestures, and the user's choice of gestures is also recognized as part of the broader Gesture category. Tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices are essential for effective mapping within the HDGI framework. The HDGI web app serves as an application that supports these interactions, while upper arm gestures are similarly classified under Gesture. Lastly, a new namespace, https://w3id.org/hdgi, is defined with the prefix hdgi, establishing a structured approach to gesture interactions.",
    "In the HDGI ontology, the concept of 'Device' is categorized under 'classes', indicating its role as a fundamental component in human-device interactions. The 'gesture repository' serves as a broader term for 'Gesture', highlighting its importance in storing and managing gesture vocabularies. Furthermore, 'hdgi:Affordance' is afforded by 'hdgi:Device', establishing a connection between the potential actions users can perform and the devices they interact with. The 'hdgi:Observer' and 'hdgi:Device' also fall under the broader category of 'classes', emphasizing their significance in the ontology. Various types of gestures, such as 'different gestures' and 'hdgi:Pose', are encompassed within the broader term 'a gesture', which represents specific actions users take to communicate with devices. Additionally, 'hdgi:UpperArmGesture' and 'hdgi:LegGesture' are classified under 'classes' and 'Gesture', respectively, indicating their specific roles in gesture recognition. The 'gestural interfaces' are linked to 'gestural inputs', showcasing the relationship between user interactions and device responses. The 'systematic analysis and description of gestures' is a broader term for 'the gesture', which encompasses the formal examination of gestures in human-device interactions. Moreover, the 'relationships between gestures and device affordances' are categorized under 'hand gestures', illustrating the connections between user movements and device functionalities. Lastly, 'gesture recognition systems' are classified under 'gesture recognition device', indicating their role in interpreting user gestures, while 'gesture affordance mapping' is also linked to 'gesture recognition device', highlighting the systematic process of linking gestures to device actions.",
    "The hdgi:HandGesture is categorized under classes, indicating its broader classification within the HDGI ontology. However, the process of mapping different gestures with their semantic relationships to affordances is not considered an ontology itself. The human device gesture interaction knowledge base aims to bring about a better user experience by systematically analyzing gestures. Device B, which is a type of product, is designed to detect user gestures and facilitate interactions. These systems, which include gesture-controlled interfaces, fall under the broader category of systems. Predefined mappings of a gesture are classified under the broader term Gesture, which also encompasses the concept of the gesture itself. Furthermore, commonly used gestures for certain affordances and arm-based gestures are both categorized under Gesture. The HDGI v0.1 serves as a model for mapping gestures to device affordances. The HDGI-gesture repository is a valuable resource that contains relevant mappings to device affordances and is utilized to find these mappings. The systematic analysis and description of gestures is a broader term that includes hand gestures, while gesture types and emerging gestures are also classified under the broader category of Gesture. Lastly, gesture vocabularies represent various defined gestures that are essential for understanding human-device interactions.",
    "Gesture recognition systems encompass a broader category known as systems, which serve as interactive platforms for user engagement. Within this domain, Gesture-controlled interfaces utilize established standards to enhance user interaction. The HDGI ontology plays a crucial role by aligning with external ontologies, facilitating a comprehensive understanding of gestures. A user's gesture is a specific instance of hand gestures, which are further categorized under gesture types, including hdgi:Movement. Gesture-controlled interfaces also fall under the broader classification of interfaces and are supported by hand-gesture systems. The gesture set, which includes various gestures, is a subset of the broader category known as Gesture. New gestures, introduced by designers, expand the concept of the gesture, which is integral to communication in human-device interactions. Additionally, gestures related to device interactions can be classified as an arm gesture, while the mapping of different gestures with their semantic relationships to affordances is essential for effective interaction. The hdgi:Device class models the relationship between devices and their manufacturers, emphasizing the role of tools in this ecosystem. Soli, a gesture recognition technology, is categorized under Device, showcasing its application in gesture-controlled environments. Commonly used gestures serve as a reference point for understanding a gesture's dynamics and characteristics.",
    "In the realm of gesture recognition, the concept of Gesture encompasses a broader category that includes hand gestures, which are essential for communication in both human interactions and human-computer interactions. The presented ontology is described in an extensible way, allowing for the systematic representation of gestures within the HDGI ontology, which itself has a broader term of concepts that include various ideas related to gestures. Within this framework, hdgi:Gesture and hdgi:Movement are classified under movements, highlighting the dynamic nature of gestures. Furthermore, the ontology categorizes classes that encompass various gesture types, including gestural interfaces, which serve as interfaces for user interaction. New gestures are identified as a subset of hand gestures, emphasizing the innovative approaches taken by manufacturers, developers, and designers to create commonly used gestures for certain affordances. The shape and dynamics of a certain gesture are essential for understanding its form, while the gesture ontology provides a formal structure for analyzing an arm gesture. Additionally, body-based contextual gestures are recognized as a broader term for a gesture, and hdgi:FacialGesture is classified under Gesture, illustrating the diversity of gestures in human-device interactions. The hdgi:DeviceContext and hdgi:Context classes further contribute to the understanding of gestures by modeling the contextual factors influencing user interactions. Personalization of gestures allows for customization to enhance user experience, and the sensor-independent ontology contributes to the broader understanding of Gesture, which includes hdgi:UpperArmGesture as a specific subclass of gestures.",
    "User gestures encompass a broader category known as a gesture, which includes various forms of interaction. Within this framework, Gesture A is a specific type of gesture that falls under the general classification of Gesture. Similarly, gesture selection is another aspect that is categorized as a Gesture. A particular gesture also belongs to this broader category, highlighting the diversity of gestures available for user interaction. The seven main classes of the HDGI ontology include hdgi:Human, which represents the human entity involved in these interactions. Users often have their preferred gestures, which are specific movements they favor, and these gestures are also classified under the broader term a gesture. User specific gesture semantics, which refers to the interpretation of gestures based on individual context, is another important aspect that is categorized under a gesture. The presented ontology effectively describes human gestures and is capable of providing detailed semantic information about them. The semantics of these gestures also fall under the broader category of Gesture, emphasizing their significance in communication. Furthermore, user specific gesture semantics is linked to gesture recognition systems, which are essential for interpreting user inputs. Geometrical gestures, which represent distinct shapes, are classified as a gesture as well. Within the ontology, hdgi:HandGesture and hdgi:ForearmGesture are subclasses that detail specific types of gestures, both of which are categorized under the general term a gesture. The user's choice of gestures is another important concept that reflects individual preferences in gesture-based interactions. The universal gesture standard aims to establish a consistent framework for gesture recognition, also classified under Gesture. Gesture-controlled interfaces contribute to greater variation in ways of utilizing gestures, leading to diverse implementations across systems. Detection is a critical process run by gesture recognition systems to identify user gestures. Lastly, the relationships between gestures and device affordances are systematically mapped, with an arm gesture serving as a broader term for these connections, while upper arm gestures are also categorized under a gesture.",
    "Osumar et al. modelled a gesture ontology, which serves as a formal framework for understanding human gestures. Within this context, the gesture repository has a broader term of a gesture, indicating its role in storing and defining gestures. The system, which refers to gesture-controlled interfaces, is a broader term for these systems, highlighting the various applications of gesture recognition. Additionally, hdgi:UpperArmGesture is categorized under hand gestures, while Soli is recognized as a type of gesture recognition system. Existing gesture vocabularies fall under the broader category of Gesture, and hdgi:Pose is classified as a type of gesture. The hdgi:LegGesture also relates to the broader concept of a gesture. Gesture recognition systems are encompassed within the broader term of system, and sample usage is associated with applications that utilize these technologies. Users must adjust to the system, which can often be non-intuitive. Gesture-referent mapping is another concept that falls under Gesture, emphasizing the relationship between gestures and their meanings. Furthermore, the concept of movements is a broader term for Gesture, while the systematic analysis and description of gestures pertains to an arm gesture. The hdgi:HandGesture is a subclass of hand gestures, and gesture representation is a broader term for human gesture. The HDGI ontology associates with affordance, linking gestures to their intended uses. User's gestures are categorized under an arm gesture, and currently available and contemporary gestures are defined as a broader term for the gesture.",
    "The Human Device Gesture Interaction encompasses a broader term known as the model, which is designed to represent various data representations of poses and gestures in interactions. Within this framework, a user is a specific instance of the broader category of users who engage with gesture-controlled systems. The relationships between gestures and device affordances are linked to certain affordances, highlighting the systematic connections that enhance user-device interactions. Furthermore, the gestural interaction taxonomy serves as a broader term for gestural interfaces, providing a structured approach to understanding these interactions. Predefined mappings of a gesture fall under the general category of a gesture, while the semantic model of gestures is specifically related to an arm gesture, illustrating the depth of gesture classification. The gesture ontology plays a crucial role in describing related referents, which are essential for interpreting gestures within the context of device interactions. Additionally, the hdgi:Pose is categorized under classes, emphasizing its role in the HDGI ontology. The gesture, as a concept, is a broader term for a gesture, which includes commonly used gestures for certain affordances. The seven main classes of the HDGI ontology consist of hdgi:Gesture, which models various human upper limb movements. Our model is capable of modeling data received from different manufacturers' devices, showcasing its flexibility. Gesture recognition systems facilitate hand gesture recognition, allowing for intuitive user interactions. These systems run on hand gesture recognition technology, which is a broader term encompassing various gesture recognition methods. Predefined one to one mappings are a subset of mapping, which links gestures to their meanings. Gesture subclasses, which include specific categories of gestures, are also classified under the broader term Gesture. This ontology effectively maps affordances, enhancing the understanding of user interactions. Lastly, hand, forearm, and upper arm gestures are modeled in detail in the HDGI, illustrating the comprehensive nature of this ontology.",
    "The HDGI ontology models hand, forearm, and upper arm gestures, providing a structured framework for understanding various types of gestures. Within this framework, arm-based gestures are categorized under the broader term 'a gesture', while the concept of 'Gesture' encompasses 'an arm gesture'. The provided gestures are also classified under the broader term 'the gesture', indicating their role in human-device interactions. Furthermore, mappings to concepts and properties are linked to the broader category of mapping, which facilitates the understanding of gesture semantics. The relationships introduced in HDGI are similarly categorized under the term relationship, highlighting the connections between different gestures. Gesture affordance mapping, which is essential for hand gesture recognition, is classified under both 'hand gesture recognition' and 'the gesture', emphasizing its importance in gesture-controlled interfaces. Emerging gestures and gesture vocabularies are both recognized as broader terms under 'a gesture', indicating their relevance in the evolving landscape of gesture interactions. The gesture can contain one or more gestures, which are also categorized under 'the gesture'. Gesture interactions allow users to engage with devices, and the concept of Gesture is further linked to Human Device Interactions. Additionally, gesture vocabularies are classified under gesture recognition systems, illustrating their role in interpreting user gestures. The gesture set is defined as a collection of specific gestures, while 'best gestures' are identified as the most effective movements within the broader category of Gesture. Listing 1.4 provides insights into the hdgi:Device, which is crucial for understanding user gesture semantics. Predefined mappings of a gesture are categorized under mapping, and tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices fall under the broader term tool, facilitating the integration of gesture recognition technologies.",
    "Gesture-based systems are increasingly recognized as methods for controlling interactive systems, highlighting their significance in modern technology. These systems encompass a variety of approaches, including the relationships between gestures and device affordances, which are systematically linked to affordance X. Within this context, currently available and contemporary gestures are categorized under hand gestures, providing a framework for interaction. The Microsoft Kinect-based skeleton serves as a foundational model based on the gesture ontology, facilitating the development of gesture recognition systems. These systems are essential for enabling communication through gestures, which are further classified into sequences of gestures that fall under the broader category of Gesture. New gestures, including specific arm gestures, expand the possibilities for user interaction. Additionally, gesture affordance mapping plays a crucial role in linking gestures to their corresponding affordances, enhancing the user experience. The HDGI-gesture repository acts as a centralized resource for these gestures, while hand gesture recognition indicates the importance of gesture-related vocabularies in ensuring effective communication between users and devices. Overall, Human Device Gesture Interaction encompasses the study of these interactions, rooted in the understanding of human gestures and their applications in various contexts.",
    "The gesture ontology describes and aims to describe mid-air gestures of the human body, providing a formal framework for understanding these movements. This ontology facilitates automated systems, enhancing the interaction between users and devices. Designers can refer to the HDGI-gesture repository to access a centralized database of gestures. Device A, which is capable of receiving gesture-based interactions, falls under the broader category of products. The shape and dynamics of a certain gesture, along with gesture-related vocabularies, are encompassed within the broader concept of the gesture. User specific gesture semantics relate to gesture recognition devices, which interpret user gestures. The provided gestures are categorized as hand gestures, which also include gesture affordance mapping. A user, in the context of these systems, interacts with gesture selection, which defines the limited gestures available for input. The HDGI-service allows for the use of HDGI-service endpoints, facilitating access to gesture vocabularies. One or more gestures can be classified as hand gestures, while redundant gesture vocabularies fall under the broader category of Gesture. Additionally, gesture vocabularies are part of the broader concept of Gesture-based systems, which utilize human gestures for interaction.",
    "The system encompasses a broader term known as system, which refers to the collection of Internet of Things technologies. Within this context, gestures related to device interactions are categorized under the broader term human gesture, highlighting the significance of physical movements in communication. Soli, a gesture recognition device, plays a crucial role in this domain. The HDGI-gesture repository, which is associated with hand gestures, provides a centralized database for contemporary gestures. Additionally, the sensor-independent ontology is a framework that describes gestures without relying on specific sensors, further emphasizing the concept of a gesture. The hdgi:Movement class consists of a predefined set of movements essential for gesture-controlled interactions. Data received from different manufacturers' devices is categorized under the broader term Device, indicating its relevance in the interaction landscape. Gesture A, gesture selection, and a particular gesture all fall under the broader category of a gesture, showcasing the structured approach to gesture classification. Gesture subclasses have been added to include hdgi:UpperArmGesture, which itself is a subclass of dynamic gestures. Gesture recognition software tools are classified under applications, demonstrating their utility in various contexts. The personalization of gestures enhances user experience, with users being a broader category that includes people interacting with these systems. Furthermore, hdgi:UpperArmGesture is recognized as an arm gesture, while the shape and dynamics of a certain gesture and gesture-related vocabularies are both categorized under hand gestures. Finally, Human Device Gesture Interaction is a broader term that encompasses the field of Human-Computer Interaction (HCI), illustrating the interdisciplinary nature of this area.",
    "The semantics of these gestures encompasses the broader concept of a gesture, which is essential for understanding user interactions. Within this framework, the mapping of other gestures is categorized under the broader term Gesture, highlighting the need for a comprehensive understanding of various user gestures. Users, who are individuals interacting with gesture-controlled systems, often exhibit their preferred gestures, which are specific movements they find intuitive. This relationship emphasizes the importance of recognizing user gestures as a subset of the broader category of a user. Furthermore, their products, which include various devices, fall under the general term products, indicating a wide range of gesture-enabled technologies. Gesture subclasses, such as hdgi:HandGesture, are integral to this ontology, with hdgi:HandGesture being a specific type of gesture that includes arm gestures. Geometrical gestures, which represent distinct shapes, are also categorized under the broader term shape. Affordance mapping plays a crucial role in linking user gestures to device actions, and it is categorized under the broader mapping term. A user must adjust to the system, which may involve different gestures that are classified under the broader term the gesture. The ontology describes gestures used in Human Device Interactions (HDI), providing a structured approach to understanding these interactions. Hand gesture recognition, a key technology in this domain, is classified under the broader term gesture recognition device. Lastly, the shape and dynamics of a certain gesture are essential for understanding how gestures function, with dynamics being a broader term that encompasses these characteristics.",
    "Gesture-related semantics encompasses the broader concept of the gesture, which includes various forms of human interaction. Within this framework, hdgi:Pose is also categorized under the gesture, highlighting its significance in understanding specific configurations of human limbs during interactions. Gestures are primarily performed using the hand, which itself is a broader term for Gesture, indicating the integral role of hand movements in communication. Furthermore, gesture data is classified under Gesture, emphasizing the importance of collecting information about these physical movements. The gesture recognition technology, Soli, is categorized under systems, showcasing its application in interactive platforms. Static gestures, which are a subset of Gesture, represent specific poses at a moment in time. The gesture ontology serves as a broader term for human gestures, providing a structured approach to understanding these interactions. A user, who interacts with these systems, is also recognized as a broader term within this context. The term system encompasses various interactive technologies, while hdgi:ObservableAffordance is classified under classes, indicating its role in understanding affordances in gesture interactions. Additionally, hdgi:ThumbCurled is a specific pose categorized under Gesture, illustrating the diversity of gestures. An ontology is essential to understand and interpret user gestures, facilitating effective communication between users and devices. The hdgi:Gesture class further categorizes human gestures, reinforcing the connection between gestures and their meanings. Gesture vocabularies represent different gestures, providing a lexicon for interaction. Manufacturer-defined gestures, existing gesture vocabularies, and gesture-referent mapping all contribute to the broader understanding of gestures, with each term playing a role in the complex landscape of human-device interactions. Lastly, hdgi:ForearmGesture and Listing 1.4 provide additional insights into the classification and contextual understanding of gestures within the HDGI ontology.",
    "The concept of relevant mappings encompasses the broader term mapping, which refers to the process of linking gesture vocabularies to their corresponding referents within the Human Device Gesture Interaction ontology. Microsoft Kinect, a prominent motion sensing device, falls under the broader category of Device, which includes various components capable of hosting multiple affordances. Gesture-related semantics, essential for understanding user intentions, is a subset of communication, highlighting the importance of interpreting gestures in human-device interactions. Existing gesture recognition systems are classified under gesture recognition devices, showcasing the technologies that detect and interpret human gestures. Leading hand-gesture supported systems, such as Microsoft HoloLens 2, are categorized as systems that utilize gestures for user interaction. The context in which gestures are interpreted is part of the broader classes within the HDGI ontology. The class hdgi:Movement, which models human upper limb movements, is a specific instance of movements. A new namespace, https://w3id.org/hdgi, is utilized for defining all the classes used in the ontology, ensuring a structured approach to gesture representation. The gesture itself is supported by a device, emphasizing the interaction between users and technology. The semantic model of gestures is a broader term for the model that represents gestures within the ontology. Mid-air gestures of the human body are categorized under Gesture, illustrating the various forms of non-verbal communication. Our model, which is the HDGI ontology, also falls under the broader category of model. Gesture-controlled interfaces contribute to a surge in their numbers, reflecting the growing interest in gesture-based interactions. Different gestures are a broader term for hand gestures, which are crucial for effective communication. The relationships between gestures and device affordances highlight the connections between human gestures and the functional capabilities of devices. People, as users of these systems, have varying expectations and preferences, and the mapping of different gestures with their semantic relationships to affordances is essential for understanding user interactions. Currently available and contemporary gestures include an arm gesture, showcasing the predefined set of gestures accessible through the HDGI-gesture repository. Gesture-related semantics also encompasses hand gestures, while hdgi:Pose represents specific configurations of hand gestures.",
    "The web application is a specific instance within the broader category of applications, which encompasses various software systems utilizing gesture recognition capabilities. Hand-gesture supported systems fall under the larger umbrella of systems, which also includes interfaces that facilitate user interaction. Gesture types are classified under the general concept of the gesture, while knowledge of the arm gestures is a subset of the broader category known as Gesture. User gestures represent the actions performed by a user, which are also categorized under the term a user. Device interactions are a component of Human-Computer Interaction (HCI), highlighting the relationship between users and technology. The size or speed of hand gestures is another aspect that is classified under Gesture. The seven main classes of the HDGI ontology establish relationships with the hdgi:Observer, which plays a crucial role in understanding gesture interactions. The Human Device Gesture Interaction (HDGI) ontology serves as a foundational framework for these concepts. Additionally, hdgi:LegGesture is a subclass within the classes of gestures, while hdgi:supportsGesture represents the relationship between affordances and the gestures they support. Gesture-referent mapping is a critical aspect of mapping, linking gestures to their intended meanings. Gesture subclasses provide a detailed categorization of different types of gestures, all of which fall under the general term a gesture. Gesture vocabularies are essential for defining the relationships to systems, as they help in understanding how gestures are interpreted within various contexts. The problem of hand gesture recognition is a significant challenge faced by gesture recognition systems, which aim to accurately interpret user inputs. A user, who interacts with these systems, is a broader term that encompasses all individuals engaging with gesture-controlled interfaces. An arm gesture is a specific type of gesture that falls under the general category of a gesture, while manufacturer-defined gestures are specific hand gestures created by manufacturers, which may not align with user expectations.",
    "The seven main classes of the HDGI ontology consist of hdgi:Device, which plays a crucial role in human-device interactions. Soli, a gesture recognition technology, has a broader term encompassing various products that utilize gesture interfaces. The property hdgi:includesGesture is linked to properties, indicating its role in defining gestures within the ontology. Mid-air gestures and human upper limb region gestures are both categorized under the broader term Gesture, highlighting their significance in gesture-based interactions. The device, which is a type of Device, is essential for facilitating these interactions. The HDGI-Mapping Service is classified as an API-driven RESTful web service, providing access to gesture mappings. Gestures that do not carry a referent to a particular affordance of a device also fall under the Gesture category. Existing gesture vocabularies are part of the broader gesture-related vocabularies, which help standardize interactions. The hdgi:Device is associated with the concept of relationship, emphasizing the connections between gestures. Device B interacts with affordance X, showcasing the practical application of these concepts. The term 'best gestures' refers to the most effective gestures, which are also categorized as a gesture. Existing gesture recognition systems are classified under systems, indicating their role in various applications. The provided gestures, which may not align with user expectations, are categorized as an arm gesture. Gesture affordance mapping is another concept that falls under an arm gesture, illustrating the systematic linking of gestures to device functionalities. Designers, who create these interfaces, are considered users in this context. The relationship between each gesture is analyzed within the broader category of Gesture, while the concept of one or more gestures is also classified as an arm gesture.",
    "Commonly used gestures encompass a broader term known as the gesture, which refers to the physical movements made by users to interact with devices. Systems are designed to have the capability to detect gestural inputs, allowing for intuitive user interactions. The systematic analysis and description of gestures is a broader term that includes human gestures, which are essential for understanding user interactions. A sequence of gestures can be categorized under a gesture, while a user's gesture also falls under the broader category of human gestures. Designers actively search and identify commonly used gestures to enhance user experience. This ontology, which has its own namespace, serves as a structured framework for understanding gestures. Within this framework, the finger pose is recognized as a broader term under Gesture, and mapping different gestures with their semantic relationships to affordances is classified under hand gestures. The user's choice of gestures is influenced by context, which is crucial for accurate interpretation. Systems are capable of understanding user specific gesture semantics, ensuring that interactions are personalized. Gesture types, which include various classifications, also fall under hand gestures. The semantic model of gestures provides a broader understanding of human gestures. A user benefits from this system as they do not have to memorize a particular gesture, simplifying the interaction process. This ontology is part of a larger framework of ontologies, and gestures related to device interactions are categorized under a gesture. The HDGI-gesture repository serves as a comprehensive resource for arm gestures, while user/device contexts are recognized as a broader term under context. GES, or Gesture Elicitation Studies, is a broader term that encompasses systems designed for gesture-based interactions. Lastly, the shape of gestures is understood within the context of their properties, highlighting the intricate relationship between gesture dynamics and device interactions.",
    "An ontology systematically maps and describes gesture vocabularies, which are essential for understanding human-device interactions. Within this framework, gesture-related vocabularies encompass broader terms such as an arm gesture, while the concept of Gesture includes human gestures that involve multiple movements and poses of body parts. Body-based contextual gestures are categorized under the general term 'the gesture', and the shape and dynamics of a certain gesture also relate back to an arm gesture. The HDGI ontology models specific configurations through hdgi:Pose, which includes each individual hdgi:Finger poses. This service facilitates the use of HDGI-service endpoints, allowing users to query gesture vocabularies. However, the existence of redundant gesture vocabularies can lead to confusion, as the provided gestures may not align with user\u2019s expectations. Existing approaches aim to address the challenges posed by the ubiquitousness in human-device gesture interactions. Furthermore, the personalization of gestures enhances user experience by adapting interactions to individual preferences. Gesture representation serves as a systematic model that connects concepts related to gestures, ensuring a comprehensive understanding of user gestures, which are ultimately performed by a user interacting with gesture-controlled systems.",
    "Commonly used gestures encompass a broader category known as hand gestures, which are essential in various forms of communication. A large number of studies can be found dealing with gestural interfaces, highlighting the importance of user gestures, which are a subset of the broader term 'the gesture'. Upper limb related gestures also fall under the category of a gesture, emphasizing the significance of physical movements in human-device interactions. The relationships between gestures and device affordances are based on the inherent properties of devices that enable specific interactions. Soli, a gesture recognition technology, operates within the broader system of gesture-controlled interfaces. Furthermore, user specific gesture semantics, which enhance hand gesture recognition, are crucial for understanding individual user interactions. The concept of 5 geometrical shapes represents a specific category of shapes that are recognized in gesture interactions. The hdgi:Device class is part of a broader model that facilitates gesture recognition. New gestures, which can be seen as innovative human gestures, contribute to the evolving landscape of interaction methods. Additionally, user specific gesture semantics and their preferred gestures are integral to the understanding of the gesture, while rich gestural inputs provide a diverse range of gestural inputs for effective communication. The Gesture category is predefined with their meaning and actions, ensuring clarity in human-device interactions. Lastly, the gesture ontology serves as a formal framework that categorizes gestures, enhancing the understanding of their meanings and applications.",
    "The HDGI ontology encompasses various gesture categories, with hdgi:ForearmGesture being a subclass of the broader term 'the gesture'. Designers, who play a crucial role in creating gesture interfaces, are also considered under the broader term 'a user'. The user's choice of gestures is another aspect that falls under 'the gesture', highlighting the importance of individual preferences in gesture recognition. Gesture recognition software tools, which include technologies like Soli, are categorized under the broader term 'tool'. Additionally, facial gestures are recognized as a type of Gesture, while upper arm gestures also belong to the broader category of 'the gesture'. The hdgi:Pose class is integral to the ontology, as it involves and is used by hdgi:BodyPart, emphasizing the anatomical aspects of gesture interactions. Users are expected to understand endpoint structures, which facilitate their interaction with gesture recognition systems. Leading hand-gesture supported systems, which include Soli, are classified under the broader term 'system', indicating their role in advanced gesture-controlled environments. The ontology also addresses the mapping of other gestures, which is a subset of 'a gesture', and identifies seven main classes, including hdgi:Movement, that are essential for modeling human-device interactions. Furthermore, body-based contextual gestures are categorized under hand gestures, and the gesture repository serves as a comprehensive resource for defining and accessing gesture vocabularies, which are informed by elements observed from existing gesture vocabularies.",
    "Designers, producers, and vendors are actively integrating gesture-controlled interfaces into their products, enhancing user interaction. Within the HDGI ontology, hdgi:FacialGesture is categorized under classes, which represent various types of gestures. The mapping of other gestures is a crucial aspect that has a broader term in gesture recognition systems, highlighting the need for semantic relationships among gestures. User specific gesture semantics also falls under existing gesture recognition systems, emphasizing the importance of personalized interactions. System-wide consistent languages introduce the concept of Gesture, which is essential for clear communication across devices. Certain affordances understand the shape and dynamics of a certain gesture, facilitating effective user-device interactions. Soli, a technology for hand gesture recognition, plays a significant role in this domain. The personalization of gestures is linked to hand gestures, allowing for tailored user experiences. Interoperability among interfaces is vital for ensuring that different systems can communicate effectively, all of which are encompassed under the broader term of interface. User gestures, which are a subset of hand gestures, are crucial for interaction. The presented ontology includes mappings to affordance, providing a structured approach to understanding gestures. The classes and properties of the HDGI ontology represent human upper limb region gestures, while hand-gesture supported systems are categorized under system. The term hand has a broader term in a gesture, indicating its fundamental role in gesture-based interactions. Gesture data is also a broader term for a gesture, capturing the essence of user movements. Static gestures are defined as a type of gesture, characterized by specific poses. Gestural information, which encompasses non-verbal cues, is categorized under Gesture. Our study looked at gesture vocabularies, contributing to the understanding of user specific gesture semantics, which is also related to hand gestures.",
    "In the realm of gesture recognition, various entities and their relationships are defined within the HDGI ontology. For instance, hdgi:LegGesture is categorized under the broader term 'the gesture', while hdgi:ThumbCurled falls under 'a gesture'. Additionally, 'different gestures' are classified as a type of 'an arm gesture', and the 'relationships between gestures and device affordances' also relate back to 'a gesture'. Users often have 'their preferred gestures', which are encompassed by 'hand gestures'. The concept of 'gesture-related semantics' is linked to 'an arm gesture', and hdgi:Pose is similarly categorized. The study of Human Device Gesture Interaction is a subset of the broader field of Human-Computer Interaction. Furthermore, the 'mapping of other gestures' is a part of the overarching concept of 'mapping'. Gestures have become prevalent in modern automobiles, enhancing user interaction. However, the gesture ontology currently misses details like the finger pose or movements, which are crucial for accurate gesture recognition. The ontology establishes connections among seven main classes, including hdgi:Context, which is essential for understanding user interactions. Moreover, 'mapping' is related to 'relationship', and 'all classes and relationships' fall under the broader category of 'classes'. Geometrical gestures, hdgi:ForearmGesture, and the user's choice of gestures are all classified as 'hand gestures'. Lastly, Listing 1.4 provides insights into the relationship between hdgi:Affordance and other elements within the ontology.",
    "Soli has a broader term that encompasses existing gesture recognition systems, which are integral to the development of interactive technologies. Leap Motion, a notable example, falls under the broader category of devices that facilitate gesture recognition. Within this domain, upper arm gestures are classified as a subset of hand gestures, highlighting the diversity of movements that can be recognized. The knowledge associated with these gestures is crucial for users, as it informs their understanding of how to interact with systems effectively. The hdgi:UpperArmGesture is a specific type of human gesture that is part of a larger framework of gestures recognized by existing systems. Furthermore, manufacturer-defined gestures can be seen as a specific type of arm gesture, which may not always align with user expectations. The effect of a gesture is a broader concept that encompasses various types of gestures, including those defined within the HDGI-service endpoints, which serve as access points for querying gesture vocabularies. The gesture repository plays a vital role in storing hand gestures and their predefined mappings, ensuring that users can intuitively interact with devices. Pose modeling contributes to the systematic analysis and description of gestures, including mid-air gestures of the human body, which are essential for non-contact interactions. Commonly used gestures for certain affordances are identified by designers, who aim to create intuitive interfaces for users. All these systems, including head gestures, represent a broader category of gestures that enhance user interaction across various applications.",
    "Arm-based gestures encompass a broader category known as the gesture, which includes various forms of physical movements. One specific example is the gesture 'Right Hand Swipe Left', which falls under the general classification of Gesture. The systematic analysis and description of gestures is a formal approach that categorizes these movements as a type of gesture. Within the HDGI ontology, hdgi:HandGesture is recognized as a subclass of human gesture, emphasizing the importance of hand movements in interactions. Gestural interfaces leverage Human-Computer Interaction (HCI) principles to facilitate user engagement. The HDGI ontology itself utilizes OWL2 to enhance its expressiveness. Additionally, a user's gesture is considered a broader term for a gesture, while the gestural interaction taxonomy provides a framework for understanding gestural inputs. Factors such as the size or speed of hand gestures are also categorized under the broader term of a gesture. Furthermore, hdgi:LegGesture is classified as a type of hand gestures, illustrating the diversity of gestures. Knowledge of the arm gestures is essential for understanding the broader category of gestures. Emerging gestures and gesture vocabularies are also included under the umbrella of the gesture, with the latter being relevant to hand-gesture supported systems. The process of mapping different gestures with their semantic relationships to affordances is categorized as an arm gesture. Existing gesture recognition systems are a subset of hand gesture recognition technologies, while GES represents a broader system for gesture elicitation studies. The leap-motion SDK is a specific tool within the category of gesture recognition systems. Lastly, the gesture set is defined as a collection of gestures that fall under the broader category of the gesture, and gesture subclasses are categorized under gesture types, illustrating the structured nature of gesture classification.",
    "In the realm of gesture-based interaction, various concepts are interconnected through broader terms. For instance, gesture vocabularies encompass arm-based gestures, while gesture types include classifications such as an arm gesture. The semantic model of gestures serves as a foundational element for understanding a gesture, which itself is a fundamental concept in this domain. Additionally, mappings to concepts and properties are categorized under properties, highlighting their significance in defining relationships within the ontology. Human upper limb region gestures are also classified as a gesture, emphasizing the importance of upper limb movements in communication. Gestural interfaces are linked to gestural information, showcasing how gestures facilitate interaction. Mid-air gestures, movements, and gestures that do not carry a referent to a particular affordance of a device are all categorized under the broader term of a gesture. The hdgi:BodyPart class is part of the broader classes that define the anatomy involved in gestures. Furthermore, gestures to answer a call in a car fall under the category of Gesture, illustrating their practical applications. The HDGI ontology provides a structured framework for understanding these gestures, with 43 gestures representing a specific set of commands. The relationship between each gesture is crucial for interpreting user intentions, and the Gesture class encompasses various types of gestures. Predefined mappings of a gesture are essential for recognizing hand gestures, which are integral to effective communication. The prefix hdgi is utilized for all the classes defined in the ontology, ensuring a cohesive structure. Lastly, commonly used gestures for certain affordances are recognized as hand gestures, further emphasizing the role of gestures in facilitating user-device interactions.",
    "In the realm of gesture-based interactions, the hdgi:Pose models the configurations of the hdgi:Finger, highlighting the intricate relationship between these two entities. Systems are designed to perform affordance mapping, which is essential for interpreting user gestures within the broader context of Human-Computer Interaction. The hdgi:DeviceContext and hdgi:Context both relate to the model, emphasizing the importance of contextual factors in understanding user interactions. Furthermore, the semantics of these gestures play a crucial role in enhancing interoperability among interfaces, ensuring that different systems can effectively communicate and understand user intent. Commonly used gestures, which include an arm gesture, are foundational to this interaction, while finger poses represent specific movements that contribute to the overall gesture vocabulary. The presented ontology is based on Semantic Web standards, facilitating the systematic mapping of gestures to their meanings. As gesture-controlled interfaces gain popularity within Internet of Things (IoT) systems, the relationships between various gesture types, such as arm-based gestures and hand gestures, continue to evolve, enriching the landscape of human-device interactions.",
    "This variety of gesture standards can confuse a user, highlighting the challenges faced in human-device interactions. The system is designed to identify the semantics of these gestures, which are crucial for effective communication. Device affordances have a broader term known as Device, while hdgi:ForearmPose is categorized under the broader term Gesture. Similarly, hdgi:Gesture encompasses various concepts, including actions that users can perform. New gestures are also classified under the broader term a gesture, indicating their role in enhancing user interaction. Users express their interaction intentions through gestures, which are essential for conveying commands to devices. The seven main classes of the HDGI ontology consist of hdgi:Pose, among others, which help in structuring these interactions. The gesture set, which includes hand gestures, is vital for effective communication. Standards in gesture recognition are evolving, giving rise to a greater variation of standards that complicate user experience. The process of mapping different gestures with their semantic relationships to affordances is essential for understanding certain affordances in gesture-controlled systems. Furthermore, the mapping of other gestures is linked to gesture recognition devices, which interpret the pose and movement of human upper limbs, a broader category that includes various movements. Gesture can have multiple poses, and human gestures are classified under the broader term a gesture. Additionally, hdgi:FacialGesture falls under the category of the gesture, emphasizing the diversity of gestures used in communication. Overall, hand gestures play a significant role in communication, and the relationship between gestures and their affordances is modeled through the concept of hdgi:affordedBy.",
    "The entity '6 commands (43 gestures)' has a broader term known as 'Gesture', which encompasses various forms of human interaction. Within this context, 'the provided gestures' are categorized under the broader term 'human gesture', indicating their role in conveying meaning through physical movements. The 'sensor-independent ontology' serves as a framework that includes 'the gesture', emphasizing the intrinsic properties of gestures without relying on specific sensors. Furthermore, 'body-based contextual gestures' are a subset of 'an arm gesture', highlighting the specific movements involved in human-device interactions. In terms of communication, 'interactive information' is sent out to a 'Device', facilitating user interaction. The 'hdgi:Pose' class represents 'static gestures', which are defined poses involving specific body parts. Additionally, 'Gesture A' is a broader category that includes 'the gesture', while 'gesture affordance mapping' and 'gesture selection' also fall under the umbrella of 'human gesture'. The relationship between gestures is captured by 'hdgi:includesGesture', which is a broader term for 'relationship'. A 'particular gesture' is another term that refers to 'the gesture', emphasizing its significance in interaction. 'Microsoft Kinect' is categorized under 'systems', showcasing its role in gesture recognition. The 'hdgi:ThumbCurled' pose is a specific type of 'hdgi:Pose', and 'all the classes used in the ontology' are part of the broader category of 'classes'. 'Atomic gestures' are fundamental components of 'Gesture', while 'one or more gestures' are also classified as 'human gesture'. The 'personalization of gestures' is linked to 'an arm gesture', allowing for tailored interactions. The 'hdgi:Pose' class further relates to 'position', which defines the spatial arrangement of gestures. The 'gesture set' was limited to '5 geometrical shapes', illustrating the specific gestures recognized in gesture-controlled interfaces. Lastly, 'Device A' is associated with 'affordance X', indicating its potential interaction capabilities.",
    "The touchscreen interface is a broader term that encompasses various products designed for interactive use. Within this context, endpoints serve as specific access points in the interface, allowing users to interact with gesture vocabularies. Gestures can involve movements of multiple body parts, while the hdgi:Movement class is distinct from hdgi:Pose, indicating a separation in gesture categorization. The semantics of these gestures relate to the broader concept of a gesture, which includes user gestures that can be classified under arm gestures. Commonly used gestures are associated with certain affordances, and user-specific gesture semantics also fall under the category of arm gestures. The universal gesture standard is a broader term that includes various standards and gestures, aiming to create consistency in gesture recognition. Facial gestures, as a subset, are categorized under gestures. The design and development of gestural interfaces is guided by established design principles. However, an ontology may prevent or limit the use and extensibility of gesture definitions. Gesture subclasses have been added to include hdgi:ForearmGesture, which is part of a broader classification. Our model supports both Euler angles and quaternions for pose representation, enhancing flexibility in gesture recognition. Finally, the HDGI-gesture repository serves as a comprehensive resource for human gestures and their mappings to device affordances.",
    "In the context of gesture interactions, 'dynamics' has a broader term of 'properties', indicating that the characteristics of gestures are essential for understanding their interactions with devices. Geometrical gestures are categorized under the broader term of 'an arm gesture', while 'hdgi:ForearmGesture' and 'hdgi:FacialGesture' are both classified under 'gesture subclasses' and 'hand gestures', respectively. The user's choice of gestures also falls under the category of 'an arm gesture', highlighting the importance of user preferences in gesture selection. Additionally, 'end hdgi:Pose' is a specific instance of 'hdgi:Pose', which models the configurations of human limbs during interactions. Device B is equipped with 'affordance Y', enabling specific user interactions through gestures. The classification of gestures is further enriched by the existence of 'upper arm gestures', which are also categorized as 'an arm gesture'. Existing gesture vocabularies enhance the reuse of gesture-related vocabularies, which are essential for defining and categorizing human gestures. The shape and dynamics of a certain gesture are encompassed within the broader category of 'human gesture', while 'hand, forearm, and upper arm gestures' collectively fall under the general term 'Gesture'. The 'gesture repository' serves as a centralized system for managing these gestures, and the 'sensor-independent ontology' provides a framework for understanding hand gestures without relying on specific sensors. However, some manufacturer-defined gestures are considered undesirable or counter-intuitive, which can lead to confusion in user interactions. Gesture A is a defined type of gesture that is categorized under 'hand gestures', and both 'gesture selection' and 'hdgi:Finger' are classified under broader terms related to hand gestures and classes, respectively.",
    "A particular gesture is categorized under hand gestures, which encompass various forms of physical movements used for communication. All these systems, which include technologies like gesture recognition devices, fall under the broader category of systems. Gesture data, essential for understanding user interactions, is a subset of gesture interactions. Additionally, gestural information is a broader term that includes the concept of a gesture, which refers to specific actions performed by users. The affordances of a device, which describe the potential interactions it supports, are part of the broader category of devices. The hdgi:Pose class, representing specific configurations of human limbs, is encompassed within the model that facilitates gesture recognition. Existing gesture vocabularies, which document predefined gestures, are related to the broader concept of the gesture itself. Villarreal-Narvaez et al. have shown the significance of gestures in their research on gesture interactions. The leap-motion SDK, a tool for developing gesture recognition applications, is classified under gesture recognition devices. The seven main classes of the HDGI ontology include hdgi:Affordance, which represents user-device interaction potentials. Movement recognition, crucial for interpreting gestures, is part of the broader applications that utilize this technology. The Arm, as an anatomical structure, is categorized within the classes of the HDGI ontology. Human Device Interactions (HDI) is a broader term that encompasses the field of Human-Computer Interaction (HCI). The semantics of these gestures, which provide meaning to hand gestures, are essential for effective communication. Gesture subclasses, which include hdgi:LegGesture, are specific categories defined within the ontology. The hdgi:LegGesture itself is a subclass of gestures, highlighting its role in gesture interactions. Gesture-referent mapping, which connects gestures to their intended meanings, is a broader concept that includes the gesture. The hdgi:hasRotation property is part of the mapping process that defines relationships in gesture representation. Finally, HDGI v0.1 is a version of the ontology that systematically describes upper limb gestures and their corresponding device affordances.",
    "The HDGI ontology encompasses various concepts related to gestures, where hdgi:HandGesture is recognized as having a broader term of a gesture. This ontology is built upon Semantic Web standards (RDF, RDFS, and OWL2), which facilitate the integration of gesture data across devices. Microsoft Kinect, a notable product, also falls under the broader category of products that utilize gesture recognition technology. Within the ontology, hdgi:Affordance is linked to the broader term relationship, highlighting the connections between different gestures and their meanings. The hdgi:IndexFinger is categorized under classes, which include the seven main classes essential for modeling human-device interactions. Furthermore, the concept of variety can confuse a user, as differing gesture standards may lead to inconsistent user experiences. The ontology also addresses existing gesture vocabularies, which are broader than gesture vocabularies, and emphasizes the need for a universal gesture standard that encompasses hand gestures. New poses are classified under hdgi:Pose, while hdgi:LegGesture is recognized as a broader term for an arm gesture. The tools for mapping HDGI v0.1 to Leap Motion and the Oculus Quest devices illustrate the application of this ontology in real-world scenarios. Additionally, the effect of a gesture is a broader concept that encompasses a gesture itself, emphasizing the intended outcomes of gestures in interactions. Gesture Elicitation Studies (GES) aim to analyze user preferences, which are essential for refining gesture vocabularies and enhancing user interaction with devices.",
    "In the realm of gesture recognition, the concept of Gesture encompasses a broader term known as action, which refers to the desired outcomes users aim to achieve through gestures. The hdgi:LocalCoordinateSystem is integral to systems that utilize gesture-based interfaces, ensuring accurate positioning and rotation in 3D space. Within this context, a gestural sign is a specific type of gesture that conveys meaning through body movements, while the problem of hand gesture recognition highlights the challenges faced in accurately interpreting these gestures. Various attempts in this field have a different scope compared to an ontology that systematically categorizes gestures. Gesture subclasses, which include specific types of gestures, fall under the broader category of the gesture itself. Predefined mappings of a gesture are essential for understanding an arm gesture, which is a specific movement used in communication. The presented ontology serves as a formal representation of gestures, contributing to the broader field of ontology. Head gestures also represent a form of a gesture, emphasizing the diversity of non-verbal communication. Gesture has become prevalent in Augmented Reality, where devices might allow for potential human behaviors through intuitive interactions. The gesture 'Right Hand Swipe Left' is categorized under a gesture, illustrating the specificity of gestures in user interactions. Endpoint structures are crucial for defining endpoints in gesture recognition systems, while commonly used gestures for certain affordances are recognized as arm gestures. Existing gesture vocabularies provide a foundation for hand gestures, which are essential for effective communication in gesture-controlled interfaces. Developers can refer to the HDGI-gesture repository for resources on contemporary gestures. The hdgi:Movement class is part of the broader classes that define dynamic gestures. All these systems have the capability to detect rich gestural inputs, enhancing user interaction. Lastly, individual users have their own preferences, which are encompassed within the broader term of preferences, reflecting the diversity of user interactions in gesture-controlled environments.",
    "The concept of gesture-referent mapping encompasses hand gestures, which serve as a broader category for various forms of non-verbal communication. Within this framework, arm-based gestures are identified as a specific type of gesture, further categorized under an arm gesture. The notion of 'best gestures' is also included, representing the most effective gestures that can be performed, which fall under the general term of the gesture. In the context of affordances, hdgi:ObservableAffordance is a broader term for affordance X, indicating the potential uses that can be communicated through gestures. The action of call is categorized under the broader concept of communication, illustrating how gestures facilitate user interactions. Different gestures and gesture-related semantics are both broader terms for human gesture, emphasizing the importance of understanding various movements in human-device interactions. Similarly, hdgi:Pose is a broader term for human gesture, modeling specific configurations of the upper limbs. Currently available and contemporary gestures are encompassed within the broader category of a gesture, while supported gestures relate to the concepts that define them. Emerging gestures, which are new movements, also fall under the category of an arm gesture. The relationship between gestures and device affordances is a broader concept of affordance, highlighting the systematic connections that enhance user interaction. The start hdgi:Pose is a specific instance of hdgi:Pose, marking the initial state of a gesture. Gesture Elicitation Studies (GES) play a crucial role in enabling the collection of end users\u2019 preferences for symbolic input, which is essential for effective gesture-based systems. However, the problematic nature of system designers defining gestures based on their own preferences can lead to confusion among people, who have diverse expectations for interaction. To address this, inter-mapping is vital as it helps to bring about interoperability among interfaces, ensuring that different gesture-controlled systems can understand and utilize a common set of gestures.",
    "The gesture set encompasses a broader term known as an arm gesture, which is a specific type of gesture. In the context of automotive interactions, gestures to answer a call in a car are categorized under the broader term of a gesture. Elements observed from existing gesture vocabularies are extracted from gesture vocabularies, indicating a relationship where the former has a broader term of gesture vocabularies. Additionally, multiple movements and poses of body parts fall under the broader category of hdgi:Pose. A sequence of gestures is also classified under the broader term of the gesture, while the specific set of 43 gestures is recognized as a type of gesture. The hdgi:LocalCoordinateSystem is categorized within the broader term of classes, and GES is related to gesture-referent mapping as a broader term. The HDGI RESTful service is part of the broader category of Services, and manufacturer-defined gestures are included under the broader term of human gesture. Furthermore, a gesture itself is defined as a broader term of a gesture. The ubiquitousness in human-device gesture interactions highlights a broader problem in the field. Gesture Elicitation Studies (GES) are recognized as a significant aspect of this research area. The hdgi:Affordance is categorized under the broader term of model, while Microsoft Kinect is classified as a system. Our ontology is designed to be extended to include other body parts beyond the upper limbs. The hdgi:Gesture class distinguishes between dynamic gestures, which are characterized by specific movements. Lastly, computational systems are recognized as a broader term of systems, and new GES are categorized under the broader term of GES.",
    "The Human Device Interaction (HDI) encompasses various concepts, including the systematic analysis of gestures used in interactions between humans and devices. Within this framework, products such as TV and blinds are recognized as part of a broader category of devices. The HDGI Mapping service, which integrates with existing gesture recognition systems, falls under the umbrella of Services that facilitate gesture-based interactions. In the ontology, hdgi:Position is linked to mapping, which helps in understanding the spatial placement of gestures. Gesture vocabularies are categorized under sequences of gestures, while gesture subclasses are defined as specific types of hand gestures. The study of Human Device Interactions is fundamentally about communication, where users convey their intentions through gestures. The provided gestures, including specific actions like pinching their thumb and index finger together, are classified as gestures within the ontology. Additionally, hdgi:Device is recognized as a subclass of sosa:Platform, indicating its role in gesture-controlled interfaces. An arm gesture is also categorized under hand gestures, and gesture affordance mapping connects user gestures to device functionalities. The concept of one or more gestures allows for the representation of complex interactions, while commonly used gestures for certain affordances enhance user-device communication. Blinds are included in the broader category of Devices, and hdgi:ForearmPose represents a specific gesture within the ontology. The user, who interacts with these systems, is central to the study of gesture recognition. Leading hand-gesture supported systems, such as Microsoft Kinect, exemplify the technology that enables intuitive user interactions. Finally, the hdgi:Observer class represents entities that observe and map gestures, contributing to the understanding of Human Device Interactions.",
    "The concept of 'best gestures' encompasses a broader category known as hand gestures, which are essential for effective communication. Within this framework, a predefined set of movements is recognized as a more general term for movements, highlighting the importance of specific actions in gesture-controlled interfaces. The ontology property hdgi:supportsGesture is related to actions, indicating how gestures can facilitate desired outcomes. Additionally, redundant gesture vocabularies, which can lead to confusion, are classified under the broader term of the gesture. The integration of Swagger UI with the HDGI web app enhances the user experience by providing a visual interface for API interactions. Furthermore, the process of mapping different gestures with their semantic relationships to affordances falls under the category of human gestures, emphasizing the connections between gestures and their meanings. The relationships between various gesture vocabularies are also categorized under the broader term of relationship, illustrating the complexity of gesture interactions. The hdgi:Device class is part of the broader concepts that define devices in human-device interactions. Upper limb related gestures are another subset of the broader category of the gesture, while gesture types are classified under human gestures, showcasing the diversity of gestures used. Redundant gesture vocabularies are identified as a specific type of gesture vocabulary, which can be mitigated through the development of standardized gesture vocabularies. Gesture subclasses, including hdgi:FacialGesture, contribute to the classification of gestures, with the latter being a specific category within this framework. The HDGI-gesture repository serves as a comprehensive resource, representing a broader category of gestures and their mappings to device affordances.",
    "The HDGI ontology encompasses various concepts related to gesture interactions, where 'hdgi:UserContext' has a broader term of 'concepts', indicating its role in understanding user interactions. Within this framework, '6 commands (43 gestures)' is categorized under the broader term 'a gesture', highlighting its significance in gesture recognition. Additionally, 'hdgi:FacialGesture' is recognized as a broader term for 'an arm gesture', illustrating the relationship between different types of gestures. A resource described as 'a dictionary for manufacturers, designers, and developers' acts as a 'dictionary', serving as a comprehensive reference for understanding gestures. The ontology also addresses the shape and dynamics of gestures, which fall under the broader category of 'a gesture'. Furthermore, 'atomic gestures', 'gesture-related vocabularies', and 'gestures that do not carry a referent' are all classified under 'a gesture', emphasizing the diverse aspects of gesture classification. The HDGI ontology is instrumental in assisting and carrying out automated reasoning tasks, showcasing its utility in computational processes. Gesture interactions are fundamentally composed of 'physical movements', which are essential for conveying user intent. In practical applications, systems like 'iDrive' expect user interactions, reinforcing the importance of understanding gestures. The ontology also incorporates 'sensor-independent ontology', which is categorized under 'an arm gesture', and 'Gesture A', which is another type of arm gesture. The process of 'gesture selection' is also recognized as a broader term for 'an arm gesture', indicating its relevance in user interactions. Moreover, 'commonly used gestures for certain affordances' are linked to 'affordance X', demonstrating the connection between gestures and device functionalities. Lastly, the 'mapping of other gestures' is categorized under 'hand gesture recognition', highlighting the ongoing efforts to establish semantic relationships among gestures.",
    "Commonly used gestures encompass a broader category known as human gestures, which are essential for communication. Within this framework, the mapping of other gestures is recognized as a specific type of gesture that conveys interaction intentions, further categorized under the general term 'the gesture.' Each different system, which refers to various interactive platforms, falls under the broader classification of systems that utilize gesture-based controls. Users of these systems are typically accustomed to a set of conventional controls, which can lead to confusion when faced with redundant gesture vocabularies, a subset of hand gestures that complicate user interactions. The semantics of these gestures, including arm gestures, are crucial for understanding their meanings. Gesture types, which include facial gestures, are integral to the study of gesture interactions, a key aspect of Human-Computer Interaction (HCI). The hdgi:Palm, a fundamental component of gesture modeling, is classified under classes that define various concepts in this domain. Arm-based gestures are linked to the relationships between different gestures, while the universal gesture standard aims to unify these interactions under a common framework. The hdgi:LocalCoordinateSystem is another critical element that supports the functioning of systems. Overall, gesture recognition systems operate through independent layers, processing gesture data that encompasses a wide range of movements, including hand, forearm, and upper arm gestures, all of which are categorized under the broader term 'a gesture.' Finally, the Human Device Gesture Interaction framework is grounded in ontology, providing a structured understanding of these complex interactions.",
    "In the realm of Human-Computer Interaction (HCI), various concepts are interconnected through broader terms and associations. For instance, a static gesture and hand both fall under the broader category of the gesture, which encompasses physical movements made by users. Additionally, relevant mappings to device affordances are linked to the broader concept of device affordances, highlighting the connections between gestures and the functional capabilities of devices. The hdgi:Pose class is associated with hdgi:Rotation, indicating the importance of rotational aspects in gesture recognition. Tools like the leap-motion SDK serve as essential resources for developers, while gesture types are classified under the umbrella of HCI. Furthermore, the mapping of other gestures is related to existing gesture recognition systems, emphasizing the need for comprehensive frameworks. The BodyPart class is part of the broader classes category, and specific gestures like hdgi:ThumbCurled are also categorized under the gesture. Human upper limb region gestures are associated with context, illustrating how situational factors influence gesture interpretation. The hdgi:Human class is a part of the ontology, which encompasses various concepts related to gestures. Relationships between gestures and device affordances are linked to the term affordance, further clarifying the functional aspects of gestures. Products like iDrive and Leap Motion are categorized under the broader term products, showcasing their role in gesture-based interactions. Body-based contextual gestures and the personalization of gestures are both classified under human gesture, indicating their significance in enhancing user experience. Lastly, knowledge of GES is linked to gesture vocabularies, while Gesture Elicitation Studies map to gesture interactions in Mixed Reality, demonstrating the evolving landscape of gesture-based technology.",
    "Designers, producers, and vendors have increased in numbers as they focus on integrating their products, which are devices that utilize gesture-controlled interfaces. Affordance mapping, a crucial process in this context, has a broader term known as certain affordances, which refers to the specific capabilities that gestures can provide. Existing gesture vocabularies encompass a range of movements, including an arm gesture, which is also a broader term for a 'swipe gesture' and dynamic gestures, both categorized under the general concept of Gesture. The notation method serves as a systematic approach within systems that utilize these gestures. Furthermore, the mapping of other gestures relates to hand gestures, while conventional controls are a broader term for interfaces that users interact with. The end hdgi:Pose signifies the conclusion of gesture movements, which are instances defined within our ontology, a formal framework that describes human gestures. User gestures, which include human gestures, are essential for interaction, and tools like the leap-motion SDK enhance gesture recognition capabilities. Users' expectations, which can lead to confusion, are part of the broader concept of expectations regarding gesture-controlled interfaces. Their preferred gestures, which are also human gestures, reflect individual user preferences. Gesture-referent mapping connects gestures to their intended actions, while mid-air gestures of the human body are a subset of the broader category of gestures. Different gestures, which can be classified as a gesture, highlight the diversity in human-device interactions. Lastly, user specific gesture semantics and developers play a significant role in shaping the interaction landscape, with developers being the professionals who create these systems for users.",
    "These systems have binary or a few choices, limiting the gesture selection available to users. Gesture carries a referent to a particular affordance of a device, illustrating how specific gestures can activate device functions. Within this context, particular sets of users has a broader term, user, indicating that users can be categorized into distinct groups. Gesture-related semantics has a broader term, a gesture, which encompasses the meanings associated with gestures. Additionally, hdgi:Pose also has a broader term, a gesture, highlighting its role in defining specific configurations of gestures. Gesture maps to the affordance of answering a call in a car, demonstrating a practical application of gesture recognition in automotive interfaces. The interconnected knowledge base is instead of predefined one to one mappings, suggesting a shift towards more flexible gesture recognition systems. Geometrical gestures have a broader term, human gesture, indicating their role in human-device interactions. Furthermore, hdgi:Movement has a broader term, concepts, which includes various ideas related to gestures. A gesture has a broader term, action, emphasizing its purpose in conveying user intent. A predefined set of movements describes hdgi:Finger, which is essential for gesture recognition. Affordance can be potential uses of Device, and it can also be hosted by Device, showcasing the relationship between gestures and device functionalities. The term hand has a broader term, hand gestures, which are crucial for communication. Gesture data has a broader term, hand gestures, indicating its relevance in analyzing interactions. The leap-motion SDK has a broader term, hand gesture recognition, which focuses on interpreting hand movements. Static gesture has a broader term, hand gestures, further categorizing gestures based on their characteristics. Lastly, hdgi:ForearmGesture has a broader term, human gesture, and user's choice of gestures also has a broader term, human gesture, emphasizing the importance of user preferences in gesture interactions.",
    "Developers actively search and identify commonly used gestures to enhance user interaction with devices. In the realm of computational systems, there exists a broader term known as 'system', which encompasses various technologies that facilitate gesture recognition. Within this context, upper arm gestures are categorized under the broader term 'human gesture', while knowledge of the arm gestures and the size or speed of hand gestures also fall under the umbrella of 'the gesture'. Specific gestures, such as 'hdgi:ThumbCurled', are classified as hand gestures, which are essential for effective communication in gesture-based interfaces. Gesture vocabularies are supported by different vendors, each contributing to the diversity of interaction designs. Furthermore, gesture affordance mapping is a concept that links user gestures to their corresponding actions, enhancing the understanding of user/device contexts. Human Device Interactions (HDI) is a broader field that encompasses Human-Computer Interaction, focusing on how users interact with devices through gestures. Users are often accustomed to a set of conventional controls, which can lead to confusion when interacting with gesture-controlled systems. The classification of gesture types includes head gestures, which are integral to non-verbal communication. Additionally, the gesture repository serves as a centralized system for managing gesture vocabularies, while specific gestures like the 'bloom' gesture exemplify practical applications in augmented reality. Overall, affordance mapping plays a crucial role in defining how gestures relate to potential device interactions, with mid-air gestures also being recognized as a significant category within the broader concept of 'the gesture'.",
    "Human upper limb region gestures encompass a broader category known as the gesture, which includes various forms of physical movements made by users. Within this framework, hdgi:ForearmPose is a specific type of gesture that falls under the broader category of hdgi:Pose. The knowledge of GES, which is derived from Gesture Elicitation Studies (GES), contributes to our understanding of user preferences in gesture-based interactions. The HDGI ontology plays a crucial role in describing the state of the art in gesture recognition, illustrating concepts such as proof-of-concept implementation. Body-based contextual gestures are integral to Human-Computer Interaction (HCI), while gestures that do not carry a referent to a particular affordance of a device also belong to the broader category of the gesture. Gesture subclasses, including an arm gesture, help classify different types of gestures, with mid-air gestures being particularly relevant in various applications. Gesture recognition systems enhance User Experience (UX) by interpreting gestural inputs, which are essential in HCI. The HDGI ontology further illustrates multiple poses and relevant mappings to device affordances, providing a comprehensive understanding of how gestures relate to device capabilities. Additionally, hdgi:Finger models each individual hdgi:Finger poses, contributing to the detailed representation of gestures. The leap-motion SDK is a significant component of existing gesture recognition systems, enabling users to send out interactive information through their gestures.",
    "The term affordance should be mapped to Gesture, indicating a relationship between the perceived properties of objects and the actions that can be performed with them. In the realm of gestures, 'best gestures' has a broader term of an arm gesture, highlighting the effectiveness of specific arm movements in interaction contexts. Gesture Elicitation Studies (GES) are categorized under the broader term study, which encompasses research methodologies aimed at understanding user preferences for gestures. Furthermore, mid-air gestures of the human body fall under the broader category of hand gestures, emphasizing the significance of hand movements in gesture recognition. Modern automobiles are classified as products, showcasing their role in contemporary technology. User gestures are a subset of Human-Computer Interaction (HCI), illustrating the importance of physical movements in user-device interactions. The hdgi:LegGesture is a specific type of human gesture, while the gesture ontology is a broader framework that includes various ontological concepts. The hdgi:LocalCoordinateSystem is categorized under properties, which define the characteristics of gestures. Gesture can be extended to include other body parts, indicating the potential for a wider range of interactions. The hdgi:LittleFinger and hdgi:MiddleFinger are both classified under classes, representing specific anatomical structures relevant to gesture recognition. The mapping of different gestures with their semantic relationships to affordances is a fundamental aspect of a gesture, which is essential for understanding user interactions. All classes and relationships within the ontology contribute to the understanding of relationships in gesture recognition. However, their ontological framework does not consider the mapping of other gestures, which limits its comprehensiveness. Devices facilitate automated systems, enhancing the efficiency of interactions. User specific gesture semantics also fall under the broader term of Human-Computer Interaction (HCI), emphasizing the personalized nature of gesture interpretation. The hdgi:Gesture is a class within the ontology that models gestures, while finger poses are significant components of the gesture category. Finally, Internet of Things (IoT) systems are classified as systems, representing the interconnected nature of devices in modern technology.",
    "These GES, which refer to Gesture Elicitation Studies, have a broader term known as GES. Within this context, gesture types encompass a broader category that includes a gesture, while Each hdgi:Movement is a specific instance of movements. The advancements in AR and VR technologies have significantly contributed to the development of gestural interfaces. Furthermore, the personalization of gestures is tailored for particular sets of users, enhancing their interaction experience. The size or speed of hand gestures falls under the broader category of hand gestures, just as knowledge of the arm gestures does. Additionally, a sequence of gestures is classified under an arm gesture. Designers and researchers, who are collectively referred to as Designers, play a crucial role in this field. A specific gesture, such as pinching their thumb and index finger together, is categorized as a gesture. The BMW iDrive touchscreen is an example of products that utilize these concepts. For a user, achieving success in their interactions is essential. Many researchers have attempted to define various gesture vocabularies to facilitate this. Predefined mappings of a gesture are part of the broader category of human gestures. Developers, who are integral to this process, work with a user to create effective interactions. Human upper limb region gestures, mid-air gestures, and gestures that do not carry a referent to a particular affordance of a device are all classified as hand gestures. Lastly, hdgi:Position is a broader term that encompasses classes, which represent various concepts related to human gestures.",
    "In the context of human-device interactions, a particular affordance of a device is categorized under the broader term Device, which encompasses various functionalities that can be activated through gestures. These gestures are often tailored to particular sets of users, who represent distinct groups of individuals interacting with the system, which itself is a broader term for each different system that utilizes gesture-based controls. Commonly used gestures for certain affordances fall under the category of human gestures, which are essential for effective communication with devices. The relationship between each gesture is analyzed within the broader context of hand gestures, while the term affordance can have supported gestures that enhance user experience. The concept of eight atomic gestures is classified under Gesture, indicating the fundamental movements that contribute to more complex interactions. Additionally, the start hdgi:Pose is recognized as an instance that marks the beginning of a gesture, while several possible poses are classified under hdgi:Pose, allowing for a variety of static gestures. Emerging gestures and redundant gesture vocabularies highlight the evolving nature of gesture interactions, where multiple gestures may represent the same action, leading to potential confusion. Overall, the seven main classes of the HDGI ontology, including hdgi:BodyPart, provide a structured framework for understanding the dynamics of gestures in human-device interactions.",
    "The gesture set has a broader term known as human gesture, which encompasses various forms of physical movements made by individuals. Within this context, the finger pose is categorized under hand gestures, highlighting the significance of specific finger configurations in communication. The hdgi:Quaternion is a mathematical representation that falls under the broader category of classes, which are fundamental components in the HDGI ontology. This ontology can assist manufacturers in developing gesture-controlled interfaces. An initial step towards building a comprehensive human device gesture interaction knowledge base is crucial for enhancing user experience. The Microsoft HoloLens, a mixed reality headset, is classified as a Device, which can host multiple affordances. Upper limb related gestures are a subset of arm gestures, while arm movements are recognized as a type of Gesture. The hdgi:UpperArm class, along with the hdgi:ForearmPose, also belongs to the classes category. Better user experience is a broader term for User Experience (UX), emphasizing the importance of user satisfaction. Atomic gestures represent the fundamental components of gesture types, and gestures that do not carry a referent are classified under a gesture. Gesture vocabularies are defined for similar referents, facilitating the understanding of various gestures. Facial gestures are another category that falls under the broader term of the gesture. The concept of one or more poses is encapsulated within the hdgi:Pose class, while body-based contextual gestures are also categorized as a gesture. The notation method serves as a systematic approach within the system that enhances gesture recognition. Finally, affordance mapping is achieved through an interconnected knowledge base, which enables the interpretation of user gestures and their corresponding actions.",
    "In the realm of human gestures, hand gestures serve as a broader category that encompasses various forms of physical movements made by individuals. Within this context, an arm gesture is a specific type of hand gesture that relates to the anatomical structure of the Arm. The concept of hand gesture recognition is linked to the broader problem of effectively mapping diverse gesture vocabularies to their corresponding actions in human-device interactions. The hdgi:Gesture class plays a crucial role in this mapping process, as it sequences gestures according to the Allen time framework. Additionally, devices are typically associated with one manufacturer, highlighting the unique relationship between hardware and gesture control. The personalization of gestures is another important aspect, as it allows for the adaptation of gestures to individual user preferences, which is essential for enhancing user experience. Furthermore, gestures have become prevalent in Virtual Reality, where users can interact with digital environments. Users can tap on the icon that appears near the wrist to access functionalities, showcasing the integration of gesture-based interactions in modern technology. The mapping of other gestures is also significant, as it extends the understanding of arm gestures within the broader context of human-computer interaction. Concepts such as gesture-related semantics and hdgi:Pose further enrich the ontology, providing a structured approach to analyzing user gestures and their meanings. Overall, the design of these systems is guided by a rationale that aims to improve the interaction between users and devices, ensuring that gestures are effectively recognized and interpreted.",
    "Gestural information encompasses the broader concept of the gesture, which refers to physical movements made by users to convey interaction intentions. Users often try to utilize it, an API that provides a comprehensive view of endpoint structures for real-time interaction with the HDGI service. Developers, who create software applications, are a broader category that includes users. Within the HDGI ontology, hdgi:Pose models the configurations of the human forearm, which is a component of the broader category of poses. Each pose is associated with a specific hdgi:timestamp, indicating when the position was recorded. Particular sets of users represent distinct groups with unique preferences, also falling under the broader term of a user. Geometrical gestures, which are specific hand movements representing geometric shapes, are classified under the broader term of a gesture. The HDGI ontology itself is a semantic model that describes human gestures in interactions. The Sample mapping service continues to integrate with gesture recognition software tools, enabling users to customize their gesture vocabularies. Additionally, hdgi:ForearmGesture and hdgi:FacialGesture are subclasses of gestures that involve specific movements of the forearm and facial expressions, respectively, both categorized under human gestures. The hand, a crucial part of gesture-based interactions, is a broader term for an arm gesture, which includes various forms of gesture data. The user's choice of gestures is also a broader category that encompasses various gestures, including static gestures, which are defined as specific poses at a point in time. Finally, interactive information is sent out to systems that utilize these gestures for effective communication.",
    "Upper arm gestures are a specific type of gesture that falls under the broader category of gestures. The gestural interaction taxonomy, proposed by Scoditti et al., serves as a guiding framework for designers and researchers in the field of gesture-based interactions. Within this taxonomy, facial gestures are recognized as a broader term encompassing hand gestures. Gesture Elicitation Studies, which are methodologies for collecting and analyzing user gestures, are considered an introduction to Gesture Elicitation Studies (GES), highlighting their significance in understanding user preferences for symbolic input gestures. A 'swipe gesture' is another example of a gesture that is categorized under the broader term of gestures. The sensor-independent ontology provides a framework for understanding human gestures without relying on specific sensors. Additionally, the gesture repository serves as a centralized system for defining and accessing various gestures, which are essential for effective human-device interactions. The effect of a gesture is understood as the intended outcome of a specific gesture, while dynamic gestures represent a category of gestures characterized by their movement. Users often interact with devices through gestures, such as pinching to access the 'start' menu. Gesture A and gesture selection are also classified under human gestures, emphasizing the importance of predefined movements in gesture interactions. Relevant classes are modeled for Human Device Interaction (HDI), which encompasses the study of gesture interactions within the broader field of Human-Computer Interaction.",
    "Hand, forearm, and upper arm gestures are categorized under commonly used gestures, which serve as a reference for understanding human-device interactions. These gestures are part of the broader field of Human Device Interaction (HDI), which includes relevant classes that focus on the upper limb region. Within this framework, the hdgi:Finger class is a specific category that falls under the general classes of gestures and movements. Touchscreens, as a type of interface, facilitate the use of hand gestures, which are integral to Human-Computer Interaction (HCI). Gestural signs, which represent specific movements, are encompassed by the broader concept of the gesture. The semantics of these gestures relate to human gestures, providing meaning to the physical movements made during interactions. Certain affordances are linked to affordance X, highlighting the capabilities gestures can provide. Gesture types are also classified under Human-Computer Interaction, emphasizing the various ways users can interact with devices. Gesture vocabularies, which are essential for understanding data related to gestures, are crucial for effective communication in Internet of Things (IoT) systems, which are part of a larger system of interconnected devices. Head gestures and hdgi:LegGesture are specific forms of gestures that fall under the general category of gestures. The universal gesture standard aims to unify human gestures, while the gesture 'Right Hand Swipe Left' exemplifies a specific interaction. Mid-air gestures of the human body are a subset of arm gestures, and new poses represent additional movements that can be modeled. Gestural information is a key aspect of hand gestures, and potential poses are foundational elements in the HDGI ontology. Finally, semantical relationships define the connections between gestures and their meanings, contributing to the understanding of relationships in gesture recognition.",
    "The hdgi:Pose class describes the hdgi:Pose, which models specific configurations of human upper limbs for gesture-based interactions. Within this context, the 'bloom' gesture is categorized under a broader term known as a gesture. Similarly, the sosa:Platform is classified under the broader category of classes, while the size or speed of hand gestures and knowledge of the arm gestures both fall under the umbrella of an arm gesture. Different SDKs-systems are also encompassed within the broader term systems. Existing approaches are categorized under the term approach, and a large number of studies deal with the problem of hand gesture recognition. The hdgi:Affordance class is a broader concept that includes various concepts related to user-device interactions. The effect of a gesture is recognized as a type of hand gesture, and gestures to answer a call in a car are classified under the general term the gesture. Our study examined the current literature on these topics, while Swagger UI serves as a tool that falls under a broader category of tools. The 43 gestures used in an experimental study are also classified as gestures. Mid-air gestures, predefined mappings of a gesture, human upper limb region gestures, and gestures that do not carry a referent to a particular affordance of a device are all categorized as arm gestures. Lastly, the icon is recognized as a broader term for symbols, and existing gesture vocabularies are classified under human gesture.",
    "The gesture encompasses a broader term known as a gesture, which includes commonly used gestures for certain affordances. Within the framework of concepts, classes represent fundamental components related to human gestures and their interactions with devices. Appropriate techniques are essential as they address the problem at hand, guiding designers and researchers in developing effective gestural interactions. The term affordance should be mapped to a gesture, highlighting its significance in user-device communication. Gestural signs, which fall under the broader category of hand gestures, convey meaning through specific movements. Interactive information is a subset of communication, emphasizing the data users express through gestures. The relationship between each gesture is linked to an arm gesture, illustrating the interconnectedness of gestures. Arm-based gestures also belong to the broader category of gestures. The ontology property hdgi:supportsGesture is a crucial aspect of the ontology, as it defines the relationship between affordances and the gestures that support them. Gesture-referent mapping is another important concept, representing the connection between human gestures and their intended actions. Head gestures, like hand gestures, are vital forms of non-verbal communication. Device B plays a significant role in detecting, understanding, and communicating user intent, thereby enhancing interaction. The gesture 'Right Hand Swipe Left' is categorized under hand gestures, demonstrating its specific application. The expressive power of our ontology reflects its capability to describe and map gesture vocabularies effectively. The property hdgi:hasPosition is integral to mapping, linking poses to their spatial coordinates. Finally, emerging gestures represent new movements that can be integrated into gesture-controlled interfaces, further expanding the definition of a gesture.",
    "Gesture vocabularies encompass a broader term known as a gesture, which includes various defined movements and poses. Within the HDGI ontology, the class hdgi:Observer is categorized under the broader term ontology, indicating its role in observing human gestures. Different gestures are semantically related to the concept of affordance, highlighting how specific movements can suggest possible interactions. The ontology also includes hdgi:Pose, which has subclasses such as hdgi:FootPose, representing specific configurations of body parts. Products like blinds fall under a broader category of products, while hdgi:Forearm is classified within the broader term classes. The property hasLocalCoordinateSystem is related to mapping, establishing a connection between positions in 3D space and their local coordinate systems. Additionally, sensors are categorized under the broader term Device, which is itself a part of the ontology. The hdgi:ForearmPose is recognized as a broader term for the gesture, while finger poses are classified under an arm gesture. The gesture set is another broader term for a gesture, and body-based contextual gestures are linked to Human-Computer Interaction, indicating their relevance in this field. The hdgi:UpperArmGesture is also categorized under ontology, while body-based contextual gestures relate to concepts within the domain. Furthermore, multiple movements and poses of body parts are classified under movements, and the start pose is recognized as a broader term for hdgi:Pose. User Experience (MBUX) is categorized under systems, emphasizing its role in enhancing user interaction. Finally, Gesture is a broader term for human behaviors, illustrating the connection between gestures and the actions exhibited by individuals.",
    "The HDGI ontology encompasses various concepts, with 'hdgi:UserContext' being a broader term under the general category of 'ontology'. Within the realm of Human-Computer Interaction, 'gestural inputs' and 'user gestures' are classified as broader terms, highlighting their significance in this multidisciplinary field. The ontology also categorizes 'classes and properties' under the broader concept of 'concepts', which includes various elements essential for understanding gesture interactions. Specific types of gestures, such as 'gestures to answer a call in a car', fall under the broader category of 'hand gestures'. Furthermore, the '6 commands (43 gestures)' are recognized as a subset of 'the gesture', illustrating the structured nature of gesture classification. The ontology defines 'gesture subclasses' as a broader term for 'human gesture', which includes various forms of non-verbal communication like 'an arm gesture'. Devices such as 'a TV' are categorized under the broader term 'Device', emphasizing their role in gesture-based interactions. The relationship between 'movements' and 'physical movements' is also established, indicating the importance of specific actions in gesture recognition. Additionally, 'eight atomic gestures' are defined as fundamental components of 'a gesture', while 'necessary affordances' are recognized as broader terms for 'certain affordances', essential for effective interaction. The concept of 'people' is linked to 'different expectations', reflecting the variability in user interactions. Lastly, 'user specific gesture semantics' is categorized under 'Human-Computer Interaction', and 'ForearmPose' is recognized as a broader term for 'Gesture', further illustrating the complexity and interrelatedness of these concepts.",
    "Interactive systems encompass a broader category known as systems, which utilize gesture-based interfaces for user interaction. Within this domain, the knowledge of GES is a subset of the broader concept of knowledge, reflecting insights gained from Gesture Elicitation Studies. User specific gesture semantics, which are integral to understanding individual user interactions, fall under the umbrella of concepts. The hdgi:HandGesture is classified within the ontology, which serves as a formal representation of gesture-related concepts. Gesture vocabularies are crucial resources that will be accessible to and are available to the research community, providing a foundation for understanding gesture interactions. Atomic gestures, fundamental components of gestures, are categorized under the broader term of the gesture. The classes and properties of the HDGI ontology also belong to the ontology category, facilitating a structured understanding of gestures. Gestures that do not carry a referent to a particular affordance of a device are included in the broader category of certain affordances. Additionally, geometrical gestures are classified under concepts, while 'best gestures' are recognized as a type of human gesture. The human, as an individual user, is able to perform actions using the device, which responds to corresponding affordances. The icon serves as a visual representation within gesture-based interfaces and is categorized under the broader term icon. The hdgi:Finger is a specific class within the model that represents the anatomical structure of a finger. Attributes, which describe specific properties of gestures, are a subset of properties. A hdgi:Pose represents a static gesture and is categorized under hdgi:Pose. Arm movements, which are essential for communication, are classified as a type of gesture. Finally, the process of modelling the infinite set of concepts, features, attributes, and relationships is a broader aspect of modelling in gesture interaction.",
    "The Microsoft Kinect is a motion sensing device that allows users to interact with computers through mid-air gestures and facilitates movement recognition. Within the context of gesture interactions, certain affordances are represented by the hdgi:ActuatableAffordance, which has a broader term encompassing necessary affordances. The mapping of different gestures with their semantic relationships to affordances is crucial, although it is not considered an affordance itself. This mapping is related to the concept of affordance, which describes the properties of objects that suggest their possible uses. Additionally, the hdgi:LocalCoordinateSystem and relationships between gestures are fundamental concepts that help define the context in which gestures are interpreted. The hdgi:ForearmPose and hdgi:FacialGesture are specific types of hand gestures, each having broader terms within the classification of gestures. A sequence of gestures can be understood as a structured arrangement of human gestures, while knowledge of the arm gestures relates to the anatomical structure of the Arm. The review conducted by Villarreal Narvaez et al. in 2020 highlights the importance of hand gesture recognition and the need for interoperability among interfaces, which is essential for enhancing user experience. Users often look at the start icon on devices like the Microsoft HoloLens 2, which serves as an interactive element in gesture-controlled interfaces.",
    "Facial gestures have a broader term known as an arm gesture, indicating the relationship between different types of non-verbal communication. The HDGI-service, which is a fully API-driven web service, falls under the broader category of Services that facilitate gesture recognition. Upper limbs are essential for interacting with a Device, highlighting the physical aspect of gesture-based communication. Additionally, hand, forearm, and upper arm gestures are encompassed within the broader term of the gesture, illustrating the various forms of gestures used in communication. The sensor-independent ontology serves as a broader term for a gesture, emphasizing its role in defining gestures without relying on specific sensors. The set of 6 commands (43 gestures) is categorized under hand gestures, showcasing a specific application of gestures in gesture recognition. The problem of hand gesture recognition is a broader issue that encompasses the general problem of mapping gestures to actions. This core ontology design pattern will be registered in the ontology design pattern initiative, contributing to the development of reusable ontology structures. An ontology is specifically designed to capture and describe individual body parts, which are crucial for understanding gestures. The hdgi:Pose class can contain one or more poses, allowing for a detailed representation of gestures. A particular gesture is a specific instance of a broader category known as a gesture. Furthermore, one or more poses are classified under hdgi:Pose, indicating the hierarchical structure of gesture representation. Gestures that do not carry a referent to a particular affordance of a device are categorized under affordance X, highlighting their lack of specific functional meaning. Gesture A is another broader term for a gesture, while gesture selection refers to the process of choosing gestures within this framework. User Experience (MBUX) is a broader term for products that incorporate advanced gesture controls. The study aimed to define and formalize the relationship between each gesture, contributing to the understanding of gesture interactions. An ontology serves as a broader term for the systematic representation of concepts, while equivalent classes and properties fall under the broader category of classes, facilitating interoperability in gesture representation.",
    "The gesture ontology facilitates automated reasoning, enhancing the ability to derive conclusions from gesture-related data. This ontology also supports interoperability among interfaces, which in turn increases User Experience (UX) by allowing seamless interactions across different systems. Within this framework, relevant mappings to device affordances have a broader term, which is a particular affordance of a device, indicating the specific capabilities that can be activated through gestures. Knowledge of the arm gestures is categorized under the broader term knowledge, emphasizing the importance of understanding gestures for effective interaction. Furthermore, corresponding affordances relate to the broader concept of affordance X, while atomic gestures are recognized as a subset of hand gestures. The studies in this field have attempted to define and formalize the relationship between each gesture, contributing to a deeper understanding of gesture semantics. Devices might allow for various human behaviors, showcasing their interactive potential. The pursuit of a better user experience is encapsulated within the broader term user experience, highlighting the significance of usability and satisfaction in interactions. However, redundant gesture vocabularies complicate the understanding of human gestures, as they introduce multiple representations for the same action. The semantics of these gestures fall under the broader category of a gesture, which is essential for effective communication in gesture-controlled environments. Additionally, hdgi:ActuatableAffordance is a broader term for affordance X, indicating its role in user interactions. The relationship between each gesture is a broader concept that encompasses the various relationships that exist among gestures. Mappings to concepts and properties also have a broader term, concepts, which are crucial for understanding gesture meanings. Gestures have become prevalent in smart homes, where they enhance user interaction with technology. However, hand gestures have not been considered in their approach, which limits the effectiveness of gesture recognition systems. Automated systems, which include gesture-controlled devices, have a broader term, systems, indicating their diverse applications. The universal gesture standard aims to unify gesture recognition across devices, although it remains a concept rather than a reality. Lastly, gestural information, which includes the nuances of an arm gesture, is vital for interpreting non-verbal cues in communication.",
    "Gesture Elicitation Studies encompass a broader term known as study, which facilitates the collection of end users' preferences for symbolic input. This research is integral to understanding Human Device Interactions, a field that falls under the broader category of Human-Computer Interaction (HCI). Within this context, affordance is a concept that relates to classes, indicating the properties of objects that suggest their possible uses. Users interact with interfaces to perform a certain task, aiming for a better user experience, which is a subset of UX. The modelling of the infinite set of concepts, features, attributes, and relationships is crucial for understanding the relationship between different gestures, including upper limb related gestures, which are a type of human gesture. System designers play a key role in defining Gesture, while the gesture itself understands its referents, which are the specific actions it conveys. The hdgi:Movement class is part of the broader ontology that categorizes various aspects of gesture interactions. Different SDKs-systems are encompassed within the system category, which includes the Internet of Things (IoT) and its associated systems. Device manufacturers are responsible for introducing new gestures, which are essential for enhancing user interaction. The hdgi:LocalCoordinateSystem defines the spatial parameters for gesture recognition, while predefined mappings and standards are concepts that guide the implementation of gesture interfaces. Different vendors contribute to the diversity of these systems, and dynamic gestures represent a classification within gesture types. Designers and researchers create appropriate techniques to enhance the effectiveness of gestural interactions, while hand, forearm, and upper arm gestures are all categorized under hand gestures.",
    "BMW\u2019s iDrive infotainment system is categorized under systems, which encompass various interactive platforms utilizing gesture-based interfaces. Within this context, the effect of a gesture is a specific instance of an arm gesture, which is a broader category that includes arm-based gestures, all of which are integral to the field of Human-Computer Interaction. Furthermore, an increased number of commands expands upon the concept of commands, indicating the need for more diverse interactions. The hdgi:LocalCoordinateSystem is a component of a model that aids in understanding gesture recognition in 3D space. Concepts and properties in these ontologies relate to the broader category of concepts, while the mapping of other gestures pertains to human gestures, highlighting the complexity of gesture vocabularies that also fall under the umbrella of concepts. The HDGI-Mapping Service serves as a resource within Services, facilitating the integration of gesture vocabularies. Existing gesture vocabularies represent a subset of gestures, which are defined actions in gesture-controlled systems. The gestural sign, which refers to the effect of a gesture, is crucial for understanding user interactions. A referent is linked to their preferred gestures, emphasizing the personalized nature of gesture interactions. Section 3 of the study describes the principles of Design, which guide the development of user interfaces. Data received from different manufacturers-devices is a broader term for data that informs gesture recognition systems. The gestural sign is a type of arm gesture, while hdgi:DeviceContext and hdgi:Context are both classified under ontology, providing a framework for understanding gesture interactions. Microsoft Kinect enables posture or movement recognition, enhancing the interaction experience. Lastly, gesture-referent mapping is a fundamental aspect of a gesture, illustrating the relationship between user actions and their intended effects.",
    "Head gestures encompass a broader category that includes an arm gesture, highlighting the diverse forms of non-verbal communication. Within the realm of gesture recognition, HDGI-service endpoints serve as a broader term for Services, which facilitate the integration of gesture vocabularies. The problem of the absence of a systematic analysis and description of gestures is a significant issue that falls under the broader category of problems faced in this field. The ontology defined at https://w3id.org/hdgi represents a structured framework that is part of the broader ontology landscape. The gesture ontology specifically focuses on capturing holistic posture, which is essential for understanding human gestures in context. Gesture recognition systems are designed to cater to user needs, ensuring that interactions are intuitive and effective. Small-scale user studies, which are a broader term for studies conducted to refine gesture interactions, play a crucial role in this process. The hdgi:ObservableAffordance class is a broader term for affordance, emphasizing the importance of observable properties in gesture interactions. The gestural sign is a broader term that encompasses various forms of gestural signs, including specific gestures like 'Right Hand Swipe Left', which is categorized as an arm gesture. Our focus in this research is not on modeling the infinite set of concepts, features, attributes, and relationships associated with gestures, but rather on the specific interactions that users engage in. A user communicates affordance through gestures, utilizing their touchscreen, which is a broader term for touchscreen technology. User Experience (MBUX) represents a broader system that enhances interaction through advanced gesture controls, while Microsoft HoloLens is a broader term for systems that utilize gesture-based interfaces. The mapping of other gestures carries similar referents, indicating the need for a structured approach to understanding these relationships. Static gestures, hand movements, and gesture data all fall under the broader category of human gestures, emphasizing the diverse ways in which gestures can be interpreted. Finally, appropriate techniques are essential and represent a broader term for the various methods used in designing effective gestural interactions.",
    "Each hdgi:Movement must have a hdgi:Duration, indicating that every specific gesture is associated with a defined time measurement. The term affordance is crucial in understanding the relationship between a device and its potential uses, emphasizing the perceived properties that guide user interactions. Hand gestures, which are integral to Human-Computer Interaction, consist of various physical movements of the face, limbs, or body, and are categorized under broader terms such as human gestures. For instance, hdgi:ThumbCurled is a specific type of human gesture. The problem at hand, which involves mapping different gestures with their semantic relationships to affordances, is a significant challenge in the field, as it relates to the broader problem of establishing effective gesture vocabularies. User context plays a vital role in this interaction, as it encompasses the specific circumstances that influence how gestures are interpreted within a given context. Additionally, hdgi:Pose, which includes subclasses like hdgi:LegPose, represents the static configurations of body parts used in gesture-based interactions. The Sample mapping service is a web application that facilitates the integration of gesture recognition tools, allowing users to customize their gesture vocabularies. Overall, the classification of gestures into subclasses, such as hand, forearm, and upper arm gestures, enhances our understanding of a particular gesture's role in interactive systems, which are essential for modern technologies like the Microsoft HoloLens.",
    "The concept of '43 gestures' encompasses a broader category known as 'an arm gesture', which in turn falls under the more general term 'a gesture'. Within the realm of gesture recognition, 'relative positions' is a term that relates to 'position', indicating the spatial arrangement of body parts during gestures. A specific gesture, such as 'pinch their thumb and index finger together', is categorized under the broader term 'the gesture'. Additionally, 'specific property' is a subset of 'properties', highlighting the characteristics associated with gestures. The '3D hand gesture taxonomy' includes a 'notation method', which aids in the classification of gestures in three-dimensional space. Furthermore, 'device affordances' are a broader category that includes 'certain affordances', emphasizing the capabilities gestures provide in human-device interactions. The property 'hdgi:hasRotation' is related to the broader concept of 'relationship', which encompasses the connections between different gestures. Similarly, 'a movement' is a more specific term under the broader category of 'movements', while 'finger pose' is a subset of 'fingers', indicating the importance of finger configurations in gesture recognition. The term 'alignments' also falls under the broader category of 'relationship', facilitating the integration of different ontologies. In the context of products, 'BMW\u2019s iDrive infotainment system' is classified under 'products', just as 'TV' is. The notion of 'best gestures' is a specific instance of 'a gesture', while 'mid-air gestures of the human body' are categorized under 'human gesture'. The '5 geometrical shapes' represent a specific subset of 'concepts', and 'observed properties' are included within the broader category of 'properties'. Lastly, 'details like the finger pose or movements' are encompassed by 'movements', and 'a set of conventional controls' is a broader term for 'conventional controls', which are familiar user interface elements. The term 'their meaning and actions' is also a broader category that includes 'meaning', reflecting the semantic interpretations associated with gestures.",
    "The hdgi:ForearmPose is categorized under an arm gesture, indicating its role as a specific type of gesture within the broader context of arm movements. The HDGI-Mapping Service, which is an API-driven service, provides essential functionalities that align with the broader category of APIs. New movements are recognized as part of the concepts that encompass various gesture definitions, while atomic gestures serve as fundamental components of movements, highlighting their importance in gesture recognition. The zPosition is a specific aspect of position, further illustrating the spatial attributes involved in gesture modeling. Additionally, the sensor-independent ontology is classified under concepts, emphasizing its relevance in understanding gestures without relying on specific sensors. A sequence of gestures is defined as a type of gesture, showcasing the structured nature of gesture interactions. The size or speed of hand gestures and knowledge of the arm gestures both fall under the broader category of human gestures, reflecting their significance in communication. The Arm class is a classification within the broader classes that represent various anatomical structures. Relevant mappings facilitate connections with external ontologies, enhancing interoperability. Movements of multiple body parts are categorized under movements, indicating their complexity in gesture interactions. The hdgi:Palm is recognized as a fundamental aspect of the model used for gesture representation. Gesture A is another concept that categorizes gestures into atomic types, while intrinsic and extrinsic properties are essential characteristics that define properties of gestures. Human gestures are described in an extensible way, allowing for future adaptations. Designers and developers must find individual studies to inform their work, particularly in the context of products like Microsoft HoloLens, which exemplify the integration of gesture interfaces. Finally, gesture-related vocabularies are crucial for increasing the need for interoperability in gesture-controlled systems, while human upper limb region gestures are a significant subset of human gestures.",
    "Mid-air gestures are a broader category that encompasses human gestures, highlighting the significance of physical movements in communication. Within this context, ForearmPose is identified as a specific type of gesture, which is part of the broader classification of gestures. Gesture recognition software and services fall under the umbrella of Services, indicating their role in facilitating interaction through gestures. Designers and developers are required to read or learn necessary data to effectively implement these systems, while designers and researchers need a systematic structure that aids in their work. This systematic structure, in turn, helps them navigate the complexities of gestural interactions. Affordance Y is a specific interaction capability that is part of certain affordances, while gesture data is a broader term that includes gestural information, essential for understanding user interactions. Device affordances are categorized under affordance X, illustrating the relationship between gestures and device functionalities. Gestures that do not carry a referent to a particular affordance of a device are classified as human gestures, emphasizing their general nature. The entity '6 commands (43 gestures)' represents a specific set of movements, which can be seen as a subset of arm gestures. Designers follow a reference guide to standardize their approach, and a predefined set of movements describes the anatomical structure of the forearm as defined in the HDGI ontology. Additionally, gestures that do not carry a referent are included under the broader category of gestures. The action of pinching the thumb and index finger together is categorized as a hand gesture, showcasing its role in interaction. Several sensors enable movement recognition, which is crucial for capturing gestures. The integration of these systems makes it easier to utilize third-party Software Development Kits and Services. Finally, the relevant code, data, and ontology are essential resources that fall under the broader category of code, facilitating community contributions and advancements in gesture recognition.",
    "In the realm of gesture-controlled interfaces, commands are understood as a broader term encompassing actions that users aim to perform. These actions are influenced by different expectations, which are themselves a subset of general expectations regarding user interaction. The relationship between each gesture is analyzed within the context of human gestures, highlighting how gestures are interconnected. Commonly used gestures for certain affordances fall under the category of affordances, which suggest possible uses to users. Multiple studies have shown the effectiveness of 'best gestures' that enhance user interaction. Poses and-or movements are broader terms that include movements, essential for gesture recognition. The hdgi:Pose class is part of the ontology that models these gestures. Atomic gestures represent fundamental components of gestures, specifically related to an arm gesture. Automated systems are categorized under systems that facilitate gesture interactions, while the Oculus Quest is a specific device that exemplifies this technology. Redundant gesture vocabularies complicate user interactions as they represent multiple gestures for the same action, which can lead to confusion. The relevant code, data, and ontology are made available for the community, contributing to the development of the ontology itself. The Internet of Things encompasses systems that utilize gesture controls, and expectations play a crucial role in shaping User Experience (UX). Gestures extend beyond just the upper limbs of the human body, incorporating various movements. The finger pose is a specific type of human gesture, while hdgi:Finger relates to pose modeling within the ontology. The affordance of answering a call in a car is contextualized within the broader concept of context. Finally, this core ontology design pattern serves as a foundational structure for the core ontology design pattern, ensuring consistency in gesture representation.",
    "This paper has a broader term of research, indicating its foundational role in the systematic investigation of gesture interactions. Within this context, data is recognized as having a broader term of knowledge, emphasizing the importance of understanding user interactions. Users often hold out their hand to interact with devices, which are designed to receive user intent. The ontology hdgi:includesGesture captures the movements of individual body parts, allowing for a comprehensive representation of gestures. Furthermore, gesture data is categorized under the broader term of data, while upper limb related gestures are classified under the broader term of a gesture. The start pose is defined as a broader term of hdgi:Pose, which models specific configurations of the upper limbs. Techniques for designing gestural interactions fall under the broader term of approach, highlighting the methodologies employed in this field. The sensor-independent ontology encompasses intrinsic and extrinsic properties, providing a framework for understanding gestures without reliance on specific sensors. Additionally, hdgi:includesGesture is recognized as a broader term of ontology, which formalizes the relationships between gestures. This service, which facilitates access to gesture vocabularies, is categorized under the broader term of Services. BMW\u2019s iDrive infotainment system is classified under the broader term of system, showcasing its role in gesture-controlled interfaces. The class hdgi:ObservableAffordance is a broader term of the term affordance, which refers to the potential uses of devices. Affordance Y is identified as a broader term of affordance X, indicating a hierarchy of interaction capabilities. Equivalent classes and properties are recognized as broader terms of properties, which define the characteristics of gestures. APIs play a crucial role as they map and upload to the gesture repository, enabling the integration of gesture data. Lastly, atomic hdgi:Movement is a broader term of movements, while gestural signs are classified under the broader term of symbols, illustrating the complexity of non-verbal communication in gesture interactions.",
    "This service can be deployed in their own private cloud, providing a flexible environment for users. Within the realm of gesture recognition, hand, forearm, and upper arm gestures are encompassed under the broader category of an arm gesture. Similarly, gestures that do not carry a referent fall under the umbrella of hand gestures. Mid-air gestures are classified within the broader context of Human-Computer Interaction (HCI). The concept of gesture-referent mapping is part of a larger framework of concepts that help define the relationships between gestures and their meanings. Finger poses are considered a subset of physical movements, highlighting the intricate details of gesture recognition. The hdgi:Palm is a specific model within the HDGI ontology, while hdgi:BodyPart and movements are both broader terms that relate to the overarching concepts in gesture interaction. The mapping of other gestures is categorized under the general term of a gesture, and a 'swipe gesture' is recognized as a specific instance of the broader category of the gesture. Relevant classes are classified under classes, which represent various components of gesture interaction. Dynamic gestures are also a type of the gesture, illustrating the diversity of movements involved in user actions. Users interact with systems, such as Microsoft HoloLens, to open the start menu, showcasing the practical applications of these gestures. Mid-air gestures of the human body are integral to understanding the human body\u2019s role in gesture recognition. The hdgi:Position class describes the spatial aspects of gestures, while the API-driven RESTful web service serves as a foundational element of APIs that facilitate these interactions.",
    "Developers define and capture mid-air gestures, which are essential for enhancing user interaction in various applications. The concept of a call has a broader term known as action, indicating that it is part of a larger set of user interactions. Similarly, the pose and movement of human upper limbs can be categorized under a broader term called a pose, while human gestures fall under the category of physical movements. The hand, as a crucial component of gesture-based interactions, is also classified under the broader term of a gesture. Gesture data, static gestures, and the icon that appears near the wrist are all encompassed within the broader category of gestures and icons, respectively. The upper arm is recognized as a fundamental part of the Gesture category, and specific poses like hdgi:ThumbCurled are also classified as gestures. Several sensors facilitate posture or movement recognition, which is vital for capturing user gestures. The hdgi:Pose class models the hdgi:UpperArm, further illustrating the relationship between body parts and gestures. Leading hand-gesture supported systems, which include devices like Microsoft HoloLens, are designed to utilize these gestures effectively. Individual studies contribute to the broader field of study, exploring various gesture vocabularies. Facial gestures, another form of human gesture, enhance communication by conveying emotions and intentions. Designers find it convenient to integrate the affordance of answering a call in a car into their products, improving user experience. Computational systems play a crucial role in recognizing gestural information, allowing for seamless interaction. All the classes used in the ontology operate independently of external ontologies, ensuring flexibility in application. Device manufacturers can refer to the HDGI-gesture repository for guidance on contemporary gestures and their mappings to device affordances.",
    "The hdgi:LittleFinger is categorized under the broader term fingers, which also includes the hdgi:MiddleFinger. In the context of gesture interactions, hdgi:Position is associated with a hdgi:LocalCoordinateSystem, which is essential for modeling the spatial placement of gestures. This relationship indicates that hdgi:Position has exactly one hdgi:LocalCoordinateSystem, and the latter is used for defining the position in a 3D space. Furthermore, Human Device Interactions encompasses the broader field of Human-Computer Interaction, highlighting the importance of understanding user gestures, such as the 'bloom' gesture, which is a specific instance of the broader category of gestures. The concept of affordance, which includes affordance mapping, is crucial for interpreting user intent in gesture-controlled interfaces, and is supported by standards like Semantic Web standards that facilitate interoperability. Additionally, the hdgi:Device class represents devices manufactured by DeviceManufacturers, linking the technical aspects of gesture recognition with the practical applications in user interfaces.",
    "Qualisys motion capture has a broader term known as systems, which encompasses various interactive platforms utilizing gesture-based interfaces. Within this context, a 'swipe gesture' and dynamic gestures are both categorized under hand gestures, indicating their role in facilitating user interactions. Furthermore, an ontology serves as a broader term for the concept of ontology itself, highlighting its importance in knowledge representation. The hdgi:Position class is a specific instance of the broader model category, which includes various representations of gestures. APIs play a crucial role by allowing the integration of gesture vocabularies, enhancing the functionality of gesture recognition systems. Mid-air gestures of the human body are classified under the broader term of a gesture, emphasizing their significance in human-device interactions. User needs are fundamental to understanding the broader category of users, while gestural information is a subset of human gestures that conveys meaningful data. The movement of the user\u2019s right arm falls under the broader category of movements, illustrating the complexity of gesture recognition. Additionally, sosa:Sensor is categorized within classes, which represent various components in gesture recognition. Commonly used gestures for certain affordances relate to the broader concept of affordance, indicating their practical applications. Human Device Interactions are integral to User Experience (UX), showcasing the relationship between user satisfaction and device interaction. A 'how-to' documentation is a type of documentation that provides guidance on using gesture recognition systems. ForearmPose is a specific instance of hdgi:Pose, which models various poses in gesture interactions. Pose modeling is a broader term for the process of modeling gestures, while the HDGI-Mapping Service is classified as a type of Web service interface, facilitating access to gesture data. Knowledge of the arm gestures and the size or speed of hand gestures are both broader terms for the concept of a gesture, emphasizing their relevance in gesture recognition. Lastly, the hdgi:Finger class is divided to represent the hdgi:IndexFinger, illustrating the detailed categorization of anatomical structures in gesture recognition.",
    "The concept of 'variety' encompasses a broader term known as 'concepts', which refers to the various ideas associated with gesture-controlled interfaces. A 'review conducted by Villarreal Narvaez et al. in 2020' serves as a broader term for 'study', highlighting the ongoing development in gesture recognition. Within the realm of 'Mixed Reality', 'gesture interactions in Mixed Reality' represent a broader term that integrates both physical and digital elements. The 'effect of a gesture' is a broader term for 'human gesture', emphasizing the significance of gestures in communication. Furthermore, the 'notation method' is categorized under the broader term 'techniques', which outlines methods for gesture representation. The specific 'bloom' gesture is a type of 'hand gestures', while 'human upper limb region gestures' and 'mid-air gestures' are both broader terms for 'a gesture'. The property 'hdgi:hasPosition' falls under the broader category of 'properties', and 'gestures that do not carry a referent to a particular affordance of a device' are also classified as 'a gesture'. 'Gestural interfaces' are built based on a 'manufacturer\u2019s design decision', indicating the influence of design choices on user interaction. The 'movement of the user\u2019s right arm' is performed during 'an arm gesture', and the action of 'pinching their thumb and index finger together' is a specific type of 'an arm gesture'. Additionally, 'gestural sign' is a broader term for 'human gesture', while the 'size or speed of hand gestures' relates to the broader attributes of gestures. The class 'hdgi:ObservableAffordance' is part of the broader 'ontology', and the gesture 'Right Hand Swipe Left' is illustrated in 'Figure 2', showcasing its dynamic nature. The 'relationship between each gesture' is a broader term for 'a gesture', indicating the interconnectedness of gestures. Lastly, 'RESTful APIs' are categorized under the broader term 'interface', and 'eight atomic gestures' are fundamental components of 'the gesture'.",
    "Head gestures are a subset of human gestures, which also includes specific movements like the gesture 'Right Hand Swipe Left'. The HDGI RESTful service is designed to be released and deployed in the Cloud, facilitating the integration of gesture recognition systems. Within the HDGI ontology, hdgi:ForearmGesture is categorized under broader ontological concepts, while hdgi:Finger is classified under concepts that encompass various gestures. Gestures related to device interactions are performed using the human's upper limb region, highlighting the importance of physical movements in controlling devices. Additionally, hdgi:Thumb is classified under classes that represent different anatomical structures involved in gestures. The finger pose is recognized as a broader category of a gesture, emphasizing the significance of specific finger movements. The concept of 'guessability of a system' is also categorized under concepts, indicating its relevance in user interaction design. A set of predefined SPARQL endpoints serves as a broader category for endpoints that facilitate data retrieval from the HDGI ontology. Actuators, which convert control signals into physical actions, fall under the broader category of devices. The gestural sign is directly related to the desired effect of an action, demonstrating how gestures convey user intentions. API clients are classified under applications, showcasing their role in interacting with various software systems. The hdgi:UpperArm is another class within the broader category of classes in the ontology. The alignment of the ontology is a broader term for alignments that ensure compatibility with external ontologies. The human body is a broader term for body, emphasizing the anatomical aspects involved in gesture interactions. Arm movements are recognized as a broader category of gestures, while the modeling of the infinite set of concepts, features, attributes, and relationships is categorized under attributes, reflecting the complexity of gesture representation. Affordance X is another concept that indicates potential interactions a device can provide, and hdgi:Palm is classified under pose modeling, representing the anatomical structure essential for gesture recognition.",
    "Gestures to answer a call in a car are a specific type of human gesture, which is a broader category that encompasses various forms of physical movements made by individuals. Within the realm of gesture recognition, the relationship between different gestures is explored through concepts that define their meanings and actions. User needs play a crucial role in shaping the interactions of a user with gesture-controlled systems, highlighting the importance of understanding these requirements. Techniques such as pose modeling are essential for analyzing gestures, and they are linked to broader methodologies in the field. A set of 43 gestures is recognized as part of the human gesture category, demonstrating the diversity of gestures available for interaction. The ontology serves as a foundational framework that allows systems to effectively utilize gesture recognition, with entities like hdgi:DeviceManufacturer being classified under it. Additionally, hdgi:ActuatableAffordance is a subclass of sosa:ActuatableProperty, indicating its role in human-device interactions. The hdgi:ThumbCurled pose is defined by its position, specifically the xPosition, which is a spatial attribute that contributes to the understanding of gestures. The challenge of enhancing the knowledge level of computational systems is a significant concern, as it relates to the ability to accurately interpret gestures. Endpoints in APIs facilitate access to gesture vocabularies, while gestures that do not carry a referent fall under the broader category of arm gestures. The knowledge gained from Gesture Elicitation Studies has resulted in an impressive amount of knowledge, enriching the understanding of user interactions. Lastly, hdgi:Pose involves the configuration of a single body part, emphasizing the intricate details of gesture-based communication.",
    "The Semantic Web standards (RDF, RDFS, and OWL2) have a broader term known as standards, which encompass various guidelines for implementing gesture interfaces. Within this context, eight atomic gestures are categorized under the broader term hand gestures. Affordance mapping, which links user gestures to potential actions, is related to the term affordance. The studies in this field explore the capability of identifying the relationship between gestures, which is crucial for understanding their meanings. Ontology plays a significant role by providing mappings to concepts and properties, facilitating a structured approach to gesture recognition. For instance, Listing 1.3 explains the hdgi:Position, which includes the spatial attribute zPosition, indicating its maximum of one instance. Static gestures are considered a type of action, while position is a broader term that encompasses various concepts. The icon that appears near the wrist is a specific instance of the broader category of icons used in gesture-based interfaces. The capability of identifying the relationship is essential for defining the connections between different gestures, which are represented in the ontology. Additionally, hdgi:Position is associated with zPosition, and hdgi:RingFinger is categorized under fingers. Details like the finger pose or movements are significant for accurately recognizing gestures, with the hdgi:ForearmPose representing a broader category of human gestures. Dynamic gestures involve hdgi:BodyPart and are classified under movements, highlighting the intricate relationships within gesture recognition.",
    "In the realm of gesture-controlled interfaces, 'meaning' has a broader term known as 'concepts', which encompasses various ideas related to arm-based gestures. The 'pose and movement of human upper limbs' is categorized under the broader term 'upper limbs', highlighting the anatomical structures involved in these gestures. The 'HDGI web app', a RESTful web application, falls under the broader category of 'web application', serving as a comprehensive tool for managing gesture vocabularies. Various 'attempts' in gesture recognition research are encapsulated under the term 'the attempts above', indicating a range of methodologies. Additionally, 'arm movements' are recognized as a subset of 'hand gestures', while 'facial gestures' are considered a type of 'gesture'. The research efforts known as 'these GES' have resulted in 'an impressive amount of knowledge' regarding user-preferred gestures. Furthermore, 'existing research using ontologies' utilizes 'ontology' as a framework, which itself is a broader term that encompasses various studies. Notably, 'Their ontology' misses critical 'details like the finger pose or movements', which are essential for accurate gesture recognition. The set of '6 commands (43 gestures)' is classified under 'human gesture', illustrating the specific gestures used in interaction. 'Smart homes' represent a broader category of 'systems' that utilize gesture recognition. The concept of 'atomic gestures' is illustrated in 'Figure 2', showcasing the fundamental components of gestures. 'Qualisys motion capture' is a system that tracks movements, contributing to the understanding of gestures. The term 'end hdgi:Pose' is related to 'concepts', indicating the final state of a gesture. 'Expectations' regarding user interaction are linked to 'user experience', emphasizing the importance of intuitive design. The 'movement of the user\u2019s right arm' is categorized under 'Arm', reflecting the anatomical focus of gesture studies. Moreover, 'interoperability among interfaces' is a broader term for 'interoperability', which is crucial for seamless interaction across devices. Lastly, 'device manufacturers' are included under the broader category of 'user', highlighting their role in the development of gesture-controlled systems.",
    "Gestures have become prevalent in computer games, enhancing the interactive experience for players. The ontology provides mappings to various concepts and properties in these ontologies, facilitating a better understanding of gesture interactions. The BMW iDrive touchscreen, which is a specific type of touchscreen, exemplifies how touch interfaces are evolving. Palm and finger positions are essential components of gesture recognition, categorized under the broader term position. Atomic gestures, fundamental movements associated with human gestures, are crucial for effective communication in human-device interactions. Additionally, the infinite set of concepts, features, attributes, and relationships related to gestures highlights the complexity of relationships within gesture studies. Conventional controls are encompassed within the broader User Experience (UX) framework, which also includes user expectations regarding gesture interfaces. A 'swipe gesture' is a specific type of dynamic gesture that falls under the category of arm gestures. The Web service interface serves as a broader term for interfaces that facilitate communication between software applications. Dynamic gestures, characterized by specific movements, are also classified as arm gestures. Certain affordances provided by gestures are integral to the concept of affordance in design. The zPosition attribute is a spatial characteristic that contributes to the understanding of gestures. The start icon on devices like the Microsoft HoloLens 2 is an example of an icon that users can interact with through gestures. The namespace https://w3id.org/hdgi is defined for the HDGI ontology, ensuring clarity in gesture-related data. The hdgi:timestamp is derived from and is positioned between end hdgi:Pose, indicating the timing of gestures. The user's choice of gestures reflects broader human behaviors, while gestural information is a key aspect of understanding a gesture. Lastly, a lack of linked data poses challenges in the field, emphasizing the need for comprehensive data collection in gesture studies.",
    "User needs encompass the broader term of a user, highlighting the specific requirements and expectations that inform product design. Within this context, the detailed representation of the hand serves as a broader term for hand, emphasizing the importance of accurately modeling hand gestures. The upper arm is recognized as a broader term for a gesture, indicating its role in gesture-based interactions. Interactive information, which conveys user intent through gestures, is a broader term for data, reflecting the essential information needed for effective communication with devices. Mid-air gestures are categorized under the broader term of Human-Computer Interaction, illustrating their significance in this multidisciplinary field. The sosa:ObservableProperty is a broader term for classes, representing observable properties within the ontology. The end pose, marking the conclusion of a gesture, is a broader term for hdgi:Pose, which models specific configurations of body parts. User's initial attempts at interaction must be met with success, ensuring that users achieve their intended outcomes. The hdgi:Forearm is a broader term for model, indicating its foundational role in gesture recognition. The hdgi:Pose class models the position of gestures in three-dimensional space. Existing knowledge serves as a broader term for knowledge, representing the accumulated understanding necessary for effective gesture interactions. The effect of a gesture is a broader term for a gesture itself, emphasizing its intended outcomes. Each individual hdgi:Finger poses is a broader term for a pose, highlighting the unique configurations of fingers in gesture interactions. Hand, forearm, and upper arm gestures collectively fall under the broader term of human gesture, showcasing their communicative functions. The hdgi:Duration is a broader term for attributes, which describe the properties of gestures. The gesture 'Right Hand Swipe Left' is illustrated in Listing 1.1, providing a detailed enumeration of its components. The 'bloom' gesture is a broader term for an arm gesture, exemplifying specific hand movements in augmented reality. Each hdgi:Movement is related to rotation change, indicating the distinct alterations in body part orientation. Finally, hdgi:ActuatableAffordance and modelling the infinite set of concepts, features, attributes, and relationships are both broader terms for concepts, representing the diverse characteristics associated with gesture interactions.",
    "Pose modeling is closely related to the gesture known as 'Right Hand Swipe Left', which is a specific instance of a gesture. In the realm of gesture studies, GES has a broader term that encompasses ontology, which serves as a formal representation of concepts and relationships within the domain. Affordance, a key concept in design, also has a broader term known as affordance X, indicating a specific potential use of a device. Similarly, a gestural sign is a broader term for a gesture, highlighting the non-verbal communication aspect of gestures. The structured elements within the HDGI ontology, referred to as classes and properties, are expressed using OWL2, a semantic web ontology language. API clients and smart homes are both categorized under the broader term products, indicating their role in technology and user interaction. Research conducted by Khairunizam et al. aimed to describe knowledge of arm gestures, focusing on the recognition of geometrical gestures. Additionally, head gestures are classified under the broader term a gesture, emphasizing their role in communication. The results of this study can serve as a guideline for organizing hand gestures to enhance usability. The gesture 'Right Hand Swipe Left' is also categorized as a gesture, reinforcing its significance in gesture recognition. The user's lack of knowledge of the relevant symbols is related to the concept of success in gesture-based systems, where achieving expected outcomes is crucial. In terms of modeling, hdgi:Rotation could be represented using hdgi:Quaternion, providing flexibility in gesture data integration. The hdgi:FacialGesture is another subclass within the ontology, while hdgi:ObservableAffordance is a subclass of sosa:ObservableProperty, focusing on observable properties in gesture interactions. Device manufacturers are linked to the broader category of users, highlighting their role in creating technology for interaction. The focus of Khairunizam et al. was primarily on geometrical gestures, which are essential for gesture recognition. A pose, as a fundamental component of gestures, must involve hdgi:BodyPart, ensuring that body movements are accurately represented. The sensor-independent ontology provides a framework for understanding gestures without relying on specific sensors, while a particular affordance of a device falls under the broader category of certain affordances, indicating the various functionalities that gestures can activate.",
    "The HDGI ontology encompasses all classes and relationships within its own namespace, specifically designed to model human gestures and their associated affordances. Within this framework, the anatomical structure of the palm, referred to as hdgi:Palm, has a broader term known as concepts, which also includes the notion of human gestures that relate to actions. One significant affordance is the capability of answering a call in a car, which falls under certain affordances. The start hdgi:Pose is another concept that serves as a foundational element in gesture modeling. This notation employs numeric terminology to represent gestures systematically. Additionally, example models illustrate the broader category of models that encompass various representations of gestures. Gestures to answer a call in a car and a specific set of 43 gestures are categorized under the broader term of a gesture. The necessary data required for effective gesture recognition is part of the larger data category. The formalization of HDGI v0.1 is a structured representation that falls under the broader concept of formalization. Human-Computer Interaction (HCI) is a multidisciplinary field that is encapsulated within the broader term HCI. Gestures have become prevalent in automobiles, enhancing user interaction. The BodyPart class represents anatomical structures involved in gestures and is categorized under concepts. Arm movements, which are a type of movement, are also included in this classification. Human Device Interactions focus on user experience, emphasizing the importance of relative positions in gesture recognition. The hdgi:timestamp is derived from the start hdgi:Pose and indicates the timing of specific gestures. Lastly, ForearmPose is recognized as a broader term under the concept of the gesture.",
    "In the realm of gesture-based interactions, 'similar referents' has a broader term known as 'concepts', which encompasses various ideas related to gestures. The 'eight atomic gestures' are fundamental components that fall under the broader category of 'an arm gesture'. Additionally, the newly established namespace 'https://w3id.org/hdgi' serves as a broader term for 'namespace', facilitating the organization of gesture-related data. The term 'user' is a broader classification for 'actor', highlighting the role of individuals in these interactions. Furthermore, 'physical movements' are categorized under the broader term 'action', which is also applicable to 'hdgi:involves', indicating the necessity of specific body parts in gestures. 'Human Device Interactions' is a broader concept within 'UX', emphasizing the user experience in these contexts. The 'web application' is a broader term for 'applications', which utilize gesture recognition technologies. The concept of 'that class as the subject' is a broader term for 'classes', which represent various gesture-related concepts. Personalization plays a crucial role in enhancing the 'personalization of gestures', indicating a broader relationship. The gesture 'Right Hand Swipe Left' is composed of 'atomic gestures', which are the building blocks of this specific movement. 'API clients' are categorized under the broader term 'tool', which aids in integrating gesture recognition systems. The term 'their meaning and actions' is also a broader classification under 'action', linking gestures to their intended outcomes. The 'hdgi:ForearmPose' is a specific type of 'a gesture', while 'a particular affordance of a device' is a broader term for 'affordance X', indicating the capabilities offered by devices. The 'Oculus Quest' is classified under 'products', showcasing its role in the market. Moreover, 'certain affordances' fall under the broader term 'the term affordance', which defines the potential uses of gestures. Lastly, the 'affordance of answering a call in a car' is categorized under 'affordance X', illustrating its specific functionality.",
    "The concept of zPosition has a broader term known as concepts, which encompasses various ideas related to arm-based gestures. In the realm of actions, facial gestures are categorized under a broader term, action. Additionally, third party Software Development Kits and Services fall under the broader category of Services, which are essential for integrating gesture recognition applications. Designers would find a central location convenient for organizing gesture vocabularies. The alignment of the ontology plays a crucial role in facilitating connections with external ontologies. Smart homes are classified under the broader term system, while end users are a subset of users who interact with these systems. However, Their ontology misses a detailed representation of the hand, which is vital for accurate gesture modeling. Atomic gestures are specifically listed in Listing 1.1, providing a breakdown of fundamental gesture components. Human-Computer Interaction (HCI) is a broader term that encompasses the field of Human-Computer Interaction. Arm movements are categorized under the broader term an arm gesture. The hdgi:Pose is specifically placed in 3D space, while the hdgi:Finger class is divided to represent both the hdgi:LittleFinger and hdgi:MiddleFinger. Necessary affordances are a broader term for affordance, and properties within the ontology have global domain and range restrictions. The entity 6 commands (43 gestures) is classified under the broader term a gesture. Affordance Y is a broader term for features associated with device interactions. Notably, gestures that do not carry a referent do not consider gestures that lack a specific meaning related to a device's affordance. Lastly, human upper limb region gestures are associated with affordance, highlighting the importance of gesture recognition in interaction design.",
    "The hdgi:Movement class relates to dynamic gestures, which consist of specific movements characterized by a start and end pose. One notable gesture is 'pinch their thumb and index finger together', which has a broader term of human gesture, indicating its role in communication. The concepts of use and extensibility are essential in the ontology, highlighting its adaptability for new gestures. OWL2, a semantic web ontology language, is categorized under standards, which also includes Swagger UI, a tool for API documentation. Additionally, gestures that do not carry a referent to a particular affordance of a device fall under the broader category of affordance. The start icon, a graphical element on devices, is a specific instance of the broader category of the icon. API clients are designed to interact with leading hand-gesture supported systems, enhancing user experience. ForearmPose, a specific static gesture, is classified under hand gestures, while hdgi:Duration is another concept within the ontology. The relationship between hdgi:BodyPart and ontology emphasizes the anatomical aspects of gesture interactions. Furthermore, hdgi:hasPosition defines the spatial relationships of poses, and a predefined set of movements describes the hdgi:UpperArm. Corresponding affordances are linked to the broader concept of affordance, and a reference guide is provided for this notation, which organizes gestures for usability. The effect of a gesture is an important aspect, with its broader term being effect, while dynamic gestures culminate in an end hdgi:Pose, marking the conclusion of the gesture sequence.",
    "In the context of the HDGI ontology, the 'range of the property' has a broader term known as 'attributes', which encompasses the specific properties associated with arm-based gestures. Furthermore, 'integration and documentation' is a broader term for 'documentation', highlighting the essential processes in ontology engineering. The concept of 'hdgi:ActuatableAffordance' is a subclass of 'affordance', indicating that it represents affordances activated through user gestures. Similarly, 'affordance X' is categorized under 'the term affordance', emphasizing its role in defining potential interactions. The 'context dependent action or manipulation possibilities' are broader concepts that include 'action', showcasing the variability of device interactions based on user context. Additionally, 'hdgi:Duration' can be derived from 'hdgi:timestamp', linking time measurements to gesture movements. The 'upper arm positions' are a subset of 'position', which refers to the spatial orientations of the upper arm. 'Device affordances' also fall under the broader category of 'concepts', while 'manufacturer-defined gestures' are a result of 'design decisions' made by manufacturers. 'Atomic gestures' include 'a movement', which is a fundamental component of gesture sequences. The category of 'hand, forearm, and upper arm gestures' is a broader term for 'a gesture', illustrating the complexity of human gestures. 'User intent' is a broader term for 'intent', reflecting the goals users aim to achieve through gestures. Users can obtain a comprehensive view of the API, which aids in understanding the system's functionalities. Research conducted by Khairunizam et al. focused on recognizing 'gestural information', which encompasses the non-verbal cues conveyed through gestures. The 'hdgi:Position' class includes 'xPosition', representing the horizontal coordinate in 3D space, and it has a maximum of one such coordinate. Lastly, 'hdgi:Rotation' is categorized under broader 'concepts', while 'gestures that do not carry a referent' are classified as 'human gestures', indicating their lack of specific meaning.",
    "In the context of gesture recognition, the hdgi:Finger is classified under a broader term known as a pose, which represents specific configurations of body parts. The mapping service, which facilitates the integration of gesture recognition tools, falls under the broader category of Services. Human gestures are closely related to the concept of affordance, as they describe the properties of objects that suggest their possible uses. This relationship is reciprocal, with affordance also mapping to human gestures. The Foundational Model of Anatomy (FMA) reuses and extends various classes that are essential for understanding anatomical structures. The ontology is actively developing elements observed from existing gesture vocabularies, which inform the modeling of gestures. The rotation of a pose is a specific aspect of the hdgi:Pose class, which encompasses the orientation of body parts in three-dimensional space. Additionally, head gestures are categorized under the broader term action, while interaction intentions are linked to user intent, highlighting the goals users aim to achieve through gestures. The concept of 'best gestures' refers to movements that correspond to the same referent, indicating that different gestures can signify the same action. Received data, which pertains to the information captured during gesture recognition, is a broader category that includes data necessary for understanding gesture interactions. End users, who are the final consumers of gesture-controlled interfaces, are encompassed within the broader term a user. Wobbrock et al. emphasize and introduce the concept of 'guessability of a system', which is crucial for enhancing user experience. This research is specifically restricted to analyzing 6 commands (43 gestures) related to gesture recognition for controlling devices. The hdgi:Position class describes the spatial placement of poses and is part of the broader concepts in the ontology. The sosa:ObservableProperty class represents measurable properties and is categorized under properties. The newly defined namespace for the HDGI ontology ensures a structured representation of gesture-related classes. Furthermore, gestures that do not carry a referent to a particular affordance of a device are excluded from the ontology, while further body parts are described in relation to new poses that can be represented in future applications.",
    "The ontology encompasses all the classes used in the ontology, which has a broader term of 'ontology'. Within this framework, gestures to answer a call in a car are categorized under the broader term 'action'. Arm movements are classified under the broader term 'Arm', while the pose and movement of human upper limbs relate to the broader category of the upper limbs of the human body. The attribute 'xPosition' falls under the broader term 'attributes', and conventional controls are linked to the broader concept of user experience. In terms of pose modeling, 'hdgi:Forearm' is a specific class that has a broader term of 'pose modeling'. A 'swipe gesture' is illustrated in Figure 2 and is also classified under the broader term 'human gesture'. The directional attribute 'hdgi:yAxisDirection' is categorized under 'attributes', and 'hdgi:ThumbCurled' represents a specific pose that is a broader term of 'a pose'. The 'hdgi:Finger' is mapped to the anatomical classification of the 'Region of hand'. RESTful APIs facilitate faster integration with third party Software Development Kits and Services. Necessary affordances are classified under the broader term 'the term affordance', while dynamic gestures, which consist of a start hdgi:Pose, are also categorized under 'human gesture'. Finally, coordinate systems are classified under the broader term 'systems', and 'hdgi:ForearmPose' is a part of the broader category of 'concepts'.",
    "The concept of affordance has a broader term known as Human-Computer Interaction (HCI), which encompasses various aspects of how users interact with technology. Within this framework, the affordances of a device are categorized under broader concepts, highlighting the potential interactions users can have with devices. User intent plays a crucial role in this interaction, as it is directed towards specific affordance X, which represents a particular capability of a device. Additionally, the sosa:ActuatableProperty is another concept that falls under the broader category of concepts, indicating properties that can be manipulated by users. However, not all gestures are meaningful; gestures that do not carry a referent to a particular affordance of a device are classified under the term affordance. The hdgi:Finger class is specifically divided to represent individual fingers, such as the hdgi:RingFinger, while the hdgi:IndexFinger is mapped to the anatomical classification known as the 'Region of hand'. The ontology also includes the hdgi:Finger class, which is a broader term that encompasses the anatomical structures of fingers. Microsoft Kinect, a type of sensor, enables gesture-based interactions and is categorized under the broader term of sensors. Each hdgi:Movement is related to position change, which is a fundamental aspect of gesture interactions. Gesture Elicitation Studies utilize devices like Microsoft HoloLens to gather data on user gestures. Furthermore, a certain task is a broader term for the specific actions users aim to accomplish, while hdgi:timestamp and end hdgi:Pose are broader terms under attributes and poses, respectively. Corresponding affordances and hdgi:ActuatableAffordance also fall under the term affordance, indicating the functionalities that devices can perform in response to gestures. Lastly, the Arm is classified under ontology, and hdgi:zRotation represents a broader term related to attributes, while the range of the property is categorized under concepts.",
    "In the context of gesture recognition, 'rotation' has a broader term known as 'attributes', which encompasses various properties associated with gestures. API client stubs facilitate the integration of gesture recognition software/services, serving as essential components for developers. Observed properties also fall under the broader category of attributes, highlighting their significance in understanding user interactions. End users, who represent a broader term for a user, are crucial in the evaluation of gesture-based systems. Further experiments are necessary to validate the proposed taxonomy and notation method, which aims to enhance the understanding of gestures. Affordance, defined as context dependent action or manipulation possibilities, plays a vital role in how users interact with devices. The 'bloom' gesture is a specific instance of a human gesture, while the forearm is categorized under the broader term of Gesture. Specific properties assert the range of the property, ensuring accurate representation within the ontology. Device affordances, which are broader than affordance, define the inherent capabilities of devices. The gesture of pinching their thumb and index finger together is classified as a gesture, illustrating the complexity of human interactions. Independent layers, a broader term for Design, highlight the modular nature of gesture recognition systems. ForearmPose is recognized as a type of arm gesture, while typing commands are categorized under actions, emphasizing their role in user input. Online search engines do not provide gesture-related semantics, indicating a gap in understanding user intentions through gestures. New poses extend the concept of a pose, allowing for more diverse representations of gestures. Properties within the ontology are governed by guarded local restrictions, ensuring precise definitions. Users can utilize their other hand to tap the icon that appears near the wrist, demonstrating the interactive capabilities of gesture-controlled interfaces. Finally, Brown et al. conclude that affordance is dependent on the context and perspective of the user, underscoring the importance of modeling both affordances and contexts in gesture interactions.",
    "The infinite set of concepts, features, attributes, and relationships encompasses various attributes, while the Sample mapping service is a specific instance of Services that aids in gesture recognition integration. In the realm of properties, the perceived and actual properties of the thing are categorized under broader properties, and poses and-or movements are classified as actions. RESTful endpoints serve as a subset of endpoints, and hdgi:FootPose is a specific type of pose modeling. The start pose is a particular instance of position, and the TV and blinds were used in the experiment to evaluate gesture recognition. Listing 1.3 provides an explanation of the hasLocalCoordinateSystem, which is crucial for understanding spatial relationships. Affordance Y is a broader term that falls under the concept of affordance. Additionally, Java version 1.9 or higher is a prerequisite to run the web application, which in turn requires this version of Java. Upper limb related gestures are intrinsically linked to upper limbs, and their research aimed to achieve higher accuracy in gesture recognition. Observation is a broader term that includes data, while sosa:Platform is categorized under concepts. Gestures that do not carry a referent are a type of gesture, and position change is a specific instance of position. Developers find the affordance of answering a call in a car to be particularly convenient, and Design is a broader term that encompasses various concepts.",
    "In the context of the HDGI ontology, various entities are interconnected through broader terms and classifications. For instance, 'xPosition' is categorized under 'concepts', while 'hdgi:LocalCoordinateSystem' falls under the broader category of 'ontology'. The anatomical structure of 'hdgi:Finger' is classified as part of 'upper limbs', and the 'infinite set of concepts, features, attributes, and relationships' is encompassed within 'features'. Additionally, 'eight atomic gestures' are recognized as a subset of 'human gesture'. The 'hdgi:UpperArm' is equivalent to the 'Arm' class, illustrating the relationship between these anatomical components. The process of 'annotating' is integral to 'ontology engineering', highlighting its role in the development of ontologies. Furthermore, 'geometrical shapes' are classified under 'shape', and 'hdgi:yAxisDirection' is included within 'concepts'. The 'hdgi:Palm' and 'start hdgi:Pose' are both broader terms for 'a pose', emphasizing their significance in gesture modeling. The ontology itself is a broader term for 'model', while 'hdgi:yPosition' relates to 'position'. 'hdgi:xRotation' is categorized under 'attributes', and 'multiple movements and poses of body parts' are classified as 'a pose'. 'Arm movements' are also recognized as a type of 'human gesture'. The dynamic 'swipe gesture' is illustrated in 'Listing 1.1', which details its components. The 'upper arm' is classified under 'the gesture', and the 'human device gesture interaction knowledge base' serves as a comprehensive repository within the broader 'knowledge base'. Lastly, 'hdgi:timestamp' is categorized under 'concepts', indicating its relevance in the context of gesture interactions.",
    "This study is a part of the broader field of research, focusing on defining a semantic model of gestures in Human Device Interactions (HDI). Within this context, an example of a pose modeling is categorized under the broader term of pose modeling, which systematically represents body positions and rotations. The hdgi:Forearm is recognized as a component of the concepts related to gestures, alongside Augmented Reality and User Experience (UX), which also fall under the same umbrella of concepts. Unity3D11 serves as a tool that aids in the development of these interactions. The coordinate systems utilized in these studies are integral to the system of gesture recognition. Customized SPARQL endpoints are specific types of endpoints that facilitate interaction with gesture vocabularies. The hand, which is part of the upper limbs, plays a crucial role in performing gestures, such as the gesture 'Right Hand Swipe Left', which is specifically executed with the right hand. Their research aims to describe and enhance knowledge of the arm gestures, contributing to a knowledge base that is built towards achieving a better user experience. Additionally, hdgi:zRotation is another concept that is part of the broader category of concepts. Small-scale user studies are a subset of user studies that help refine gesture interactions. The problem of having different origin points is a significant challenge within the broader problem of mapping diverse gesture vocabularies to their corresponding actions.",
    "The hdgi:Forearm is a defined class that models the anatomical structure of the human forearm. In the realm of software development, Swagger codegen is being integrated with the HDGI web app, enhancing its capabilities. The Internet of Things (IoT) systems encompass a broader term known as the Internet of Things, which refers to interconnected devices that communicate over the internet. Similarly, 'their own private cloud' is a specific instance of the broader concept of Cloud computing. In the context of physical interactions, arm movements are categorized under the broader term of physical movements. The concept of affordance is related to features, indicating how design elements suggest their uses. RDF, a standard model for data interchange, falls under the broader category of standards. Device affordances, which describe the properties of devices that enable interactions, are a subset of the term affordance. The hdgi:zAxisDirection and hdgi:xAxisDirection are both classified under attributes, which define specific directional properties within the HDGI ontology. This view of affordance has become standard in both Design and Human-Computer Interaction, emphasizing the perceived properties of objects. The desired effect of an action is a broader term for effect, which refers to the outcomes intended by gestures. The upper arm is recognized as a broader category that includes hand gestures. The infinite set of concepts, features, attributes, and relationships associated with gestures is encompassed within the broader term concepts. Furthermore, equivalent classes and properties are derived from external ontologies, facilitating interoperability. The HDGI ontology includes several possible poses, with hdgi:FootPose being one of its subclasses. Lastly, hdgi:Palm is another entity that falls under the broader category of ontology.",
    "This view has a broader term known as concepts, which encompasses various ideas related to arm-based gestures. In the realm of digital entertainment, computer games are categorized under the broader term products, highlighting their role as interactive software. The hdgi:Thumb, a specific subclass of fingers, illustrates the anatomical structures involved in gesture-based interactions. The 'bloom' gesture is recognized as a type of gesture, showcasing how specific hand movements can facilitate user interactions. Furthermore, Swagger codegen capabilities are integrated into the HDGI web app, enhancing its functionality. Augmented Reality (AR) serves as a broader term for AR technologies, which enrich user experiences by overlaying digital information onto the real world. Several sensors, which include devices like Microsoft Kinect, fall under the broader category of sensors, enabling the recognition of gestures. The hdgi:UpperArm is a component of the model that represents the upper limb in gesture recognition. Gestural information is closely related to arm movements, providing essential data for understanding user interactions. API clients are designed to work with leading hand-gesture supported systems, such as Microsoft Kinect. The desired effect of an action is a broader term that encompasses the various actions users aim to achieve through gestures. Affordance Y, which represents specific interaction capabilities, is a subset of the broader concept of affordance. Wobbrock et al. emphasize the importance of user\u2019s initial attempts in gesture recognition. The hdgi:LocalCoordinateSystem class models the hdgi:Position class, ensuring accurate representation of gestures in 3D space. Despite the existence of few studies, there have been attempts to define and formalize the relationship between each gesture. Different SDKs-systems utilize various coordinate systems, which can lead to inconsistencies in gesture recognition. Dynamic gestures, characterized by specific movements, are a broader category that includes atomic gestures. Manufacturers actively search and identify commonly used gestures to enhance user experience. Affordance X represents a specific capability within the broader context of gesture recognition. Lastly, the hdgi:Pose class models the concept of 'rotation', allowing for precise representation of gestures in three-dimensional space.",
    "Gesture-related semantics is a specialized area that falls under the broader term of semantics, focusing on the meanings associated with gestures in human-device interactions. One specific gesture, known as 'Right Hand Swipe Left', is composed of eight atomic gestures, which are the fundamental movements that define this action. In the context of gesture modeling, fingers are categorized into left and right entities, allowing for a detailed understanding of their roles in gestures. Additionally, the local coordinate system, which is essential for defining positions in interactive systems, is part of the broader category of systems that utilize gesture-based interfaces. The hdgi:BodyPart class within the HDGI ontology enables the representation of further body parts, while the hdgi:UpperArm class is recognized as equivalent to the Arm class in anatomical terms. Designers and manufacturers play a crucial role in creating these interactive systems, and their work is encompassed within the broader category of Designers. Furthermore, the concept of affordance, which describes how users perceive the possible uses of objects, is integral to Human-Computer Interaction. The hdgi:hasRotation property is part of the ontology that facilitates the modeling of gestures, and the hdgi:Device class is characterized by a many to many relationship, indicating its complex interactions with various components. Alignments between the HDGI ontology and external ontologies enhance interoperability, while concepts like hdgi:xRotation and affordance are essential for understanding gesture dynamics. User context significantly influences User Experience (UX), and the classification of Humans falls under the broader category of classes, which includes various aspects of gesture representation, such as the eight atomic gestures that collectively define a gesture.",
    "The HDGI ontology encompasses various elements related to pose modeling, with hdgi:LegPose being a specific instance that falls under this broader category. The alignment of the ontology plays a crucial role in enhancing the overall ontology framework, ensuring effective integration with other ontologies. Within this framework, features of interest are categorized under attributes, highlighting the specific properties associated with gestures. Each body part is linked to a potential pose, which is essential for accurately representing gestures. Augmented Reality, a technology that enhances user interaction, is classified under immersive technologies. The anatomical structures such as hdgi:Palm and hdgi:LittleFinger are recognized as part of the upper limbs and are mapped to the 'Region of hand', which is vital for gesture modeling. Dynamic gestures are defined by their composition of atomic hdgi:Movement, while the concept of rotation change is categorized under actions, indicating the desired outcomes of gestures. Human gestures are understood as a subset of human behaviors, reflecting the physical movements that convey meaning. Observations collected within the Semantic Sensor Network (SSN) contribute to the broader concepts in the ontology. Additionally, arm movements are classified under a gesture, and hdgi:ForearmPose is recognized as a specific type of pose. Customized SPARQL endpoints can be developed to facilitate interaction with the web application, enhancing the usability of the HDGI ontology.",
    "The Internet of Things (IoT) encompasses various concepts, including the predefined directional attribute hdgi:xAxisDirection. Within the realm of gesture recognition, eight atomic gestures are fundamental, each including a movement that contributes to dynamic gestures, which are broader terms encompassing actions. The pose and movement of human upper limbs relate to the human's upper limb region, highlighting the anatomical aspects involved in gestures. A specific example of a gesture is the 'bloom' gesture, which opens the 'start' menu in augmented reality (AR) systems. The desired effect of an action, also referred to as a referent, is crucial in understanding user interactions. In gesture modeling, multiple poses represent a broader category of a pose, while features of interest are categorized under features. The ForearmPose is a specific type of human gesture, and the Z-axis points outwards from the user, indicating spatial orientation. AR is classified under immersive technologies, which enhance user experiences. Existing research using ontologies moves beyond traditional taxonomies, providing a more nuanced understanding of gestures. Context-dependent action or manipulation possibilities are viewed from the perspective of a particular actor, emphasizing the variability in user interactions. The hdgi:DeviceManufacturer is a broader term for manufacturers, and hdgi:Position falls under the ontology category. Eight atomic gestures are also classified under atomic gestures, while the affordance of answering a call in a car is another action category. Lastly, example models serve to illustrate the start pose in gesture interactions.",
    "Taxonomies define gesture vocabularies, which are essential for understanding the various sets of defined gestures used in gesture-based interaction systems. Facial gestures, a subset of human behaviors, illustrate how non-verbal movements can convey emotions and intentions. The upper arm is a key component of an arm gesture, which is a specific form of non-verbal communication. Usability is a critical aspect that falls under the broader term of User Experience (UX), emphasizing the importance of effective interaction design. Gesture-referent mapping relates gestures to their intended actions, highlighting the connection to the concept of referent. The end pose signifies the final position of a gesture, which is categorized under the broader term of position. Virtual Reality (VR) is a form of immersive technology that enhances user interaction through intuitive gestures. The Foundational Model of Anatomy serves as a model that provides a structured representation of human anatomy. Several possible poses are subclasses of a pose, allowing for diverse representations of static gestures. The term affordance encompasses concepts that describe the potential uses of objects in interaction design. Integration is a crucial part of ontology engineering, ensuring that various ontologies can work together effectively. Microsoft HoloLens, a mixed reality headset, is categorized under Augmented Reality, showcasing its role in enhancing user experiences. The 5 geometrical shapes represent a broader category of geometrical shapes used in gesture recognition. The hdgi:Finger class is part of the upper limbs of the human body, which are essential for performing gestures. The hdgi:ForearmPose is a specific representation within the ontology, while upper limbs are integral components of the human body. Additionally, several possible poses have been added as subclasses to hdgi:LegPose, further enriching the ontology. The hdgi:Position class includes the attribute hdgi:yPosition, which specifies the vertical placement of body parts in 3D space, and it is limited to a maximum of one yPosition. Overall, upper limbs are a broader term that encompasses the various anatomical structures of the human body.",
    "The local coordinate system has a broader term known as the system, which encompasses various technologies and frameworks. Body parts correspond to hdgi:Pose, which models the configurations of human upper limbs for gesture interactions. This notation, which is a method for representing hand gestures, has a broader term called notation. User specific gesture semantics, which focuses on individual user interpretations of gestures, falls under the broader category of semantics. The sosa:ActuatableProperty is a class within the ontology that includes properties that can be manipulated by devices. User experience, a crucial aspect of interaction, is also categorized under concepts. Designers and developers are involved in creating new movements, which are custom-defined gestures. The research community, which studies gesture-controlled interfaces, is part of a larger community. The sosa:Sensor is a subclass of sosa:ActuatableProperty, indicating its role in detecting physical properties. Answering a call is a specific action that falls under the broader category of action. UX, or user experience, is another concept that emphasizes user satisfaction. One or more poses, which refer to distinct body configurations, have a broader term known as a pose. Alignments are represented as separate ontology files, facilitating interoperability. Developers would find a central location convenient for accessing gesture vocabularies. A similar study addresses the challenge of how to increase the knowledge level of computational systems, particularly in recognizing gestures. Maier et al. define the concept of affordance, which describes the potential uses of devices. Microsoft HoloLens, a mixed reality headset, is categorized under AR, or Augmented Reality. The Semantic Web serves as a broader framework for concepts, enhancing data sharing and understanding. Semantics plays a vital role in bringing about interoperability among interfaces, allowing different systems to communicate effectively. Lastly, the SSN ontology describes observed properties, detailing the characteristics measured by sensors.",
    "Head gestures are a subset of human behaviors, illustrating the broader category of non-verbal communication. In the realm of User Experience (UX), which encompasses user experience as a whole, system designers evaluate small-scale user studies to refine their designs. Within the HDGI ontology, the hdgi:UpperArm is classified under pose modeling, while the hdgi:RingFinger is mapped to the 'Region of hand', highlighting the anatomical relationships relevant to gesture interactions. The attribute hdgi:yPosition is a broader term that encompasses various attributes related to gesture modeling. Atomic gestures include the start pose, which is crucial for defining gestures. The sosa:Actuator is categorized under classes, indicating its role in gesture interactions. Furthermore, User Experience (UX) is recognized as a broader term for user experience. An example of pose modeling specifically illustrates the start pose, while arm movements are categorized under concepts, emphasizing their significance in gesture interactions. The hdgi:Finger is a broader term for the upper arm, and the hdgi:Forearm is classified under a pose, demonstrating the anatomical structure's relevance. The gestural interaction taxonomy serves as a broader framework for taxonomies, aiding in the organization of gesture-related concepts. Future work is anticipated to expand on the current research, while gestures that do not carry a referent are noted for not considering a particular affordance of a device. Arm movements are also classified under action, and a particular affordance of a device is recognized as a broader term for affordance. In gesture modeling, the end pose is associated with the right palm, which is modeled as the end pose, marking the conclusion of a gesture.",
    "The affordance of answering a call in a car is a specific instance of the broader concept of affordance, which encompasses the perceived and actual properties of objects that suggest their possible uses. In the realm of software development, Software Development Kits serve as tools that facilitate application creation, while immersive technologies include both AR and VR, which are broader categories that enhance user experiences through digital interactions. Modelling processes utilize Turtle syntax to create formal representations, and external ontologies provide a broader context for understanding ontological frameworks. Concepts related to Virtual Reality and Mixed Reality further illustrate the diverse applications of these technologies. The system designed for gesture-controlled interfaces improves system reliability, and the sosa:Platform is a foundational ontology class that supports this interaction. Within gesture recognition, the ForearmPose is categorized as a type of gesture, and the Internet of Things (IoT) is a significant domain that encompasses various interconnected devices. Best practices in this field provide mappings to concepts and properties, while documentation is an essential part of ontology engineering. Additionally, potential poses are specific configurations that fall under the broader category of poses, and Augmented Reality is recognized as a subset of Virtual Reality. The hdgi:Pose class is associated with time stamps, and the Foundational Model of Anatomy (FMA) ontology represents a structured approach to anatomical concepts, while the SSN ontology describes the procedures involved in sensor interactions. Overall, these relationships highlight the interconnectedness of concepts within the fields of technology and design.",
    "The ontology encompasses a range of concepts, with gesture vocabularies playing a crucial role in helping to reduce ubiquitousness in gesture-based interactions. Within this framework, user context is recognized as a broader term that includes user experience, highlighting the importance of individual circumstances in shaping interactions. Each body part can be involved in a pose, which is essential for understanding gestures. Related referents, which have a broader term of referent, are vital for linking gestures to their meanings. Section 3 of the paper describes the semantics of the Human Device Gesture Interaction (HDGI) ontology, while Figure 4 illustrates a proof-of-concept implementation that integrates predefined SPARQL endpoints with RESTful APIs. The sosa:Sensor class is categorized under concepts, indicating its broader relevance. Additionally, Swagger codegen serves as a tool that aids in the integration of the HDGI ontology. A 'swipe gesture' is specifically performed with the right hand, which also performs this gesture. The hdgi:Finger class is divided to represent the hdgi:Thumb, showcasing the detailed categorization within the ontology. User context is further classified under UX, emphasizing its significance in user experience. Affordance Y is a broader term for capability, while position change is a broader term for action, illustrating the layered nature of these concepts. The Semantic Sensor Network is classified as an ontology, and SPARQL endpoints are categorized under endpoints, facilitating data access. The hdgi:LocalCoordinateSystem defines the axis direction, ensuring consistency in gesture recognition. Finally, potential human behaviors encompass a broader range of human behaviors, and hdgi:FootPose is recognized as a broader term for a pose.",
    "The integration and documentation is part of ontology engineering, which encompasses various processes essential for the effective use of ontologies. Within this framework, the referent has a broader term known as meaning, while extrinsic properties are categorized under properties. The hdgi:yPosition, a specific attribute within the HDGI ontology, is also classified under the broader concept of concepts. Additionally, Swagger codegen capabilities fall under the tool category, facilitating the generation of client libraries and API documentation. The hdgi:Palm, representing the anatomical structure of the palm, is included in the upper limbs of the human body. In gesture modeling, the start pose is a specific instance of a pose. The SSN ontology describes Observation, a key component in understanding sensor data. Their ontology focuses on holistic posture, which represents the overall alignment of the human body. The mapping service, a web application, provides resources for integrating gesture recognition software. The start icon appears near a user\u2019s wrist, enhancing interaction. User needs are encompassed within the broader concept of User Experience (UX), which emphasizes the importance of usability and satisfaction. Interoperability is another broad concept that ensures different systems can work together effectively. Virtual Reality (VR) is a broader term that includes immersive technologies. A particular affordance of a device is categorized under the term affordance, which defines the possible uses of an object. Khairunizam et al. conducted a similar study aimed at improving gesture recognition. The affordance of answering a call in a car is also classified under the term affordance. The alignment of the ontology helps in applying guarded local restrictions, which enhance compatibility with external ontologies. Lastly, hdgi:LocalCoordinateSystem is a specific class that falls under the broader category of coordinate systems, defining the local positioning used in gesture recognition.",
    "The hdgi:UpperArm has a broader term known as concepts, which encompasses various ideas related to arm-based gestures. Similarly, Allen time is also categorized under concepts, indicating its relevance in the temporal framework for gesture analysis. However, a majority of them are limited in their scope, highlighting the constraints of existing gesture vocabularies. In terms of anatomy, the right palm, which is a specific area of the palm, has a broader term that includes palm itself. The semantics of these gestures, which refers to their meanings, falls under the broader category of semantics. A hdgi:Pose, representing a static gesture, is a specific instance of a pose, which is a broader term for various body configurations. The leap-motion SDK utilizes a right-hand rule, a convention in 3D coordinate systems, to facilitate gesture recognition. Furthermore, API documentation serves as a broader term for documentation that guides users in utilizing the HDGI ontology. Usability, which measures the effectiveness of a system, is a broader term for user experience, emphasizing the importance of user satisfaction. The SOSA ontology also fits within the broader category of concepts, providing a foundational framework for sensor-related interactions. Section 3 of the relevant paper describes the formalization of the HDGI ontology, detailing its design and structure. Additionally, data received from different manufacturers-devices is categorized under the broader term manufacturer, indicating the diverse sources of gesture recognition data. A set of predefined SPARQL endpoints is wrapped with RESTful APIs, facilitating data retrieval from the HDGI ontology. The term action, which refers to the outcomes of gestures, is a broader term for human behaviors, encompassing a wide range of physical movements. The hdgi:Forearm is classified under upper limbs, representing a key anatomical structure in gesture interactions. Virtual Reality, a technology that simulates environments, is a broader term for immersive technologies, enhancing user engagement. The 3D hand gesture taxonomy is categorized under taxonomies, providing a systematic classification of gestures. Usability is also recognized as a broader term for UX, emphasizing the overall user experience. Lastly, global domain and range restrictions are classified under concepts, ensuring clear relationships within the ontology, while the upper arm is identified as a broader term for human gesture, highlighting its significance in communication.",
    "Qualisys motion capture is utilized to capture the movement of the user\u2019s right arm, providing insights into the dynamics of arm gestures. Within the context of gesture modeling, hdgi:Palm is recognized as having a broader term, which is the upper arm, indicating its anatomical significance. The concept of one or more poses encompasses a broader term known as a pose, highlighting the importance of specific configurations in gesture recognition. Atomic gestures are fundamental components that include a single body part, emphasizing the role of individual anatomical segments in gesture performance. The SOSA ontology serves as a lightweight but self-contained core ontology, which is the foundation of the Semantic Sensor Network, facilitating the integration of sensor data. RESTful APIs, which are a broader category of APIs, enable seamless interaction with web services. The forearm is categorized under the broader term of the gesture, illustrating its involvement in various movements. Swagger UI, a tool for visualizing API documentation, falls under the broader category of API documentation itself. Biological concepts are classified under the broader term concepts, reflecting their relevance in gesture modeling. Future work is anticipated to expand upon the research in this field, indicating ongoing developments. The hdgi:Rotation is a broader term for rotation, which is crucial for understanding the orientation of poses in 3D space. The upper limbs of the human body are encompassed within the broader term of the human body, emphasizing their functional importance. Semantic relationships are a broader term for relationships, which are essential in understanding the connections between different gestures. The hdgi:yRotation is categorized under attributes, which are specific properties associated with gestures. The SSN ontology describes features of interest, providing a framework for sensor data analysis. A Web service interface is a broader term for Services, which facilitate the integration of various software applications. Section 3 of the paper describes the syntax, outlining the rules governing the structure of the Human Device Gesture Interaction ontology. Lastly, holistic posture is recognized as a broader term for the human body, emphasizing the overall alignment and positioning of the body in gesture interactions.",
    "The ontological framework has a broader term known as the model, which is essential for representing various data representations in Human Device Interactions. Within this context, the Semantic Sensor Network (SSN) serves as a broader term for ontology, facilitating the modeling of sensor data. User experience (UX) is a broader term that encompasses the overall user experience, while higher accuracy is a broader term for accuracy, indicating improved recognition capabilities. The forearm is categorized under hand gestures, illustrating the anatomical basis of gesture recognition. Concepts are represented by sosa:ObservableProperty, which is a broader term for observable properties in sensing. Similar referents fall under the broader term referent, highlighting the connections between gestures. Our study, which is part of broader research, aims to analyze gesture vocabularies. The Sample mapping service is a web application that provides tools for gesture recognition. API and architecture documentation is a broader term for documentation that guides users in integrating the HDGI ontology. Atomic gestures include the end pose, representing the final position in a gesture sequence. Microsoft HoloLens, a mixed reality headset, is categorized under Mixed Reality. A 'how-to' documentation is provided to assist users with 'how-to' documentation for gesture recognition. The hdgi:Finger class is a broader term for the human's upper limb region, while rotation is a broader term for attributes related to pose orientation. The capability of identifying the relationship is a broader term for capability in gesture recognition. The hdgi:hasPosition property is a broader term for ontology, linking poses to their spatial positions. Researchers have attempted to define gesture vocabularies, contributing to the understanding of gesture interactions. The hdgi:LegPose class is a broader term for a pose, representing static leg positions. Finally, the rationale behind such a design is a broader term for rationale, guiding the development of the HDGI ontology.",
    "Swagger UI has a broader term of API and architecture documentation, which encompasses various tools and resources for understanding APIs. In the realm of gesture recognition, a TV is a broader term for the device used in experiments, while hdgi:Finger serves as a building block of the upper limb region, indicating its importance in gesture-based interactions. Existing research using ontologies is a broader term for research that explores formal frameworks for analyzing gestures. The proposed taxonomy and notation method is categorized under taxonomies, providing a structured approach to gesture representation. The SSN ontology, which describes sensors, is a broader term for ontology, highlighting its role in the Semantic Web. Furthermore, Semantic Web standards (RDF, RDFS, and OWL2) are foundational to the broader concept of the Semantic Web. Redundant gesture vocabularies help to reduce ubiquitousness by addressing the confusion caused by multiple gestures representing the same action. The SOSA ontology is the core of the Semantic Sensor Network (SSN), which models sensor data. The upper arm is a broader term for a gesture, while the human is a broader term for Human, emphasizing the biological aspect of users interacting with devices. The hdgi:BodyPart class is defined in the Foundational Model of Anatomy (FMA), ensuring a structured representation of body parts. Concepts such as hdgi:yRotation and guarded local restrictions are broader terms that enhance the understanding of gestures. Qualisys motion capture is a broader term for motion capture technology, which is essential for tracking movements. The eight atomic gestures include a start pose, illustrating the components of dynamic gestures. Lastly, user needs are a broader term for user experience, reflecting the importance of understanding user interactions with technology, while hdgi:BodyPart is a broader term for body parts involved in gesture interactions.",
    "The 'start pose' is a specific instance of 'a pose', indicating its broader classification within gesture modeling. In the context of the HDGI ontology, 'hdgi:yAxisDirection' is defined as either 'upward', which specifies the vertical orientation of the axis. Additionally, 'Figure 3 - point A' refers to the defined positions of the upper arm, which are crucial for understanding arm movements. Mixed Reality encompasses Virtual Reality, highlighting the relationship between these immersive technologies. User needs are a broader concept that falls under UX, emphasizing the importance of user experience in design. The class 'hdgi:UpperArm' is equivalent to the Arm class in the Foundational Model of Anatomy (FMA), illustrating the anatomical connections in gesture modeling. The ontology of Sensor, Observation, Sample, and Actuator is part of the SOSA framework, which describes sensor functionalities. Furthermore, Palm and finger positions are always relative to the wrist, which is essential for accurate gesture recognition. The concept of rotation is a broader term that encompasses various concepts related to pose orientation in 3D space. A semantic model serves as a broader framework for understanding the relationships between different models in gesture recognition. Semantics, which studies meaning, is a broader term that encompasses the interpretation of gestures. RESTful APIs are categorized under the broader architectural style of RESTful, facilitating web service interactions. The FMA serves as a foundational ontology for anatomical concepts. Their research recognizes arm movements, which are critical for gesture-controlled interfaces. The forearm is a broader term that relates to an arm gesture, indicating its role in non-verbal communication. DeviceManufacturer is a broader term that includes various concepts related to device production. Architecture documentation provides a comprehensive overview of system structures, falling under the broader category of documentation. The relevant code is a broader term that refers to the software associated with the HDGI ontology. Lastly, Sample is a broader term that represents various concepts within the Semantic Sensor Network ontology, while individual body parts can be modeled separately, allowing for detailed gesture analysis.",
    "The 'example models' are designed to represent the 'end pose' of gestures, illustrating the final position of body parts in dynamic interactions. In the context of human anatomy, the 'hand' is a broader term that encompasses the 'human body', highlighting its integral role in gesture-based communication. The concept of 'hdgi:zRotation' is a specific aspect of 'rotation', which is essential for understanding the orientation of poses in 3D space. The 'ontology' serves to capture and describe 'individual body parts', providing a structured framework for gesture modeling. Furthermore, the 'Foundational Model of Anatomy (FMA)' reuses and extends various 'concepts' related to human anatomy. In terms of directional attributes, 'hdgi:xAxisDirection' can be defined as either 'rightward', indicating movement in a specific spatial orientation. 'Smart homes' represent a broader category of 'automated systems', showcasing the integration of technology in residential environments. The term 'experiment' is a broader classification under 'study', emphasizing the systematic investigation of hypotheses. 'Gesture Elicitation Studies (GES)' fall under the umbrella of 'research', focusing on user preferences for gesture inputs. Within the anatomical context, 'hdgi:Forearm' is categorized under the 'upper limbs of the human body', while 'hdgi:UpperArm' is a broader term for 'a pose', representing specific configurations of body parts. The concept of 'linked data' is a broader term for 'data', facilitating the interconnection of structured information. An 'interconnected knowledge base' is a broader term for 'knowledge base', enabling the integration of various gesture recognition systems. The 'ontology design pattern initiative' is part of a broader 'community' effort aimed at enhancing ontology quality. 'Automated reasoning tasks' are classified under the broader term 'task', reflecting the computational processes involved. Researchers require 'gesture-referent mapping' to establish meaningful connections between gestures and their intended actions. A 'hdgi:Pose' specifically involves a 'single body part', capturing the essence of gesture representation. The 'hdgi:Palm' is categorized under the broader term 'human's upper limb region', emphasizing its anatomical significance. Lastly, 'sosa:Sensor' is a broader term for 'ontology', representing devices that measure physical properties, while researchers are actively conducting 'new GES' to refine gesture-referent mappings.",
    "Researchers conduct new Gesture Elicitation Studies (GES) to explore and establish specific gesture-referent mappings. In the realm of sensor technology, the ontology class sosa:Sensor has a broader term, which is sensors, indicating that sensors are devices capable of detecting and measuring physical properties. Furthermore, Semantic Web standards include OWL2, which is an extension of the original OWL, providing greater expressiveness for complex ontological structures. The concept of mid-air gestures of the human body is categorized under the broader term human body, emphasizing the physical movements performed without surface contact. The leap-motion SDK, a tool for gesture recognition, falls under the broader category of Software Development Kits. Designers and manufacturers utilize standard definitions to create consistent gesture vocabularies. Within gesture modeling, ForearmPose is a specific static gesture that is classified under the broader term a pose. API clients, which interact with APIs, are essential for leading hand-gesture supported systems like Microsoft HoloLens. The directional attribute hdgi:yAxisDirection is a broader term for axis direction, indicating vertical orientation. In anatomical terms, hdgi:Palm is a building block of the upper limb region, while hdgi:Thumb is mapped to the 'Region of hand'. Additionally, hdgi:Finger is a broader term for body parts, and complete API documentation serves as a comprehensive resource under the broader category of documentation. The hdgi:Forearm is classified under the upper arm, which is part of the body parts category. Gesture vocabularies will be accessible to the research community, and further experiments are necessary to assess the capability of identifying relationships between gestures beyond their predefined meanings.",
    "The zPosition is based on a local coordinate system, which provides a framework for defining spatial attributes. Within the realm of body parts, the hdgi:IndexFinger is categorized under this broader term. Similarly, the concept of its referents, which denotes the specific actions conveyed by gestures, also falls under the broader category of referent. In the context of gesture ontology, universal and existential class restrictions are encompassed within the broader concepts, while the hdgi:UpperArm is classified under ontology. API documentation serves as a broader term for APIs, and the hdgi:xRotation is a broader term for rotation. Additionally, 'how-to' documentation is a subset of documentation. An example of a pose modeling specifically models the end pose, illustrating the final position in a gesture. The referent itself is a broader term for concepts, and OWL2 extends the Semantic Web framework. The rotation of a pose is also categorized under concepts, while a specific property is defined by universal and existential class restrictions. Osumar et al. represent a broader category of researchers, and the SOSA ontology serves as a foundational framework for sensor-related concepts. Swagger UI is a broader term for complete API documentation, while authors do not map to existing gesture vocabularies. The Foundational Model of Anatomy (FMA) describes biological concepts and is itself a broader term for these concepts, with the hand being classified under body parts.",
    "One approach enables designers and manufacturers to define taxonomies for gesture vocabularies, facilitating the development of gestural interfaces. In the realm of gesture recognition, the Cloud has a broader term known as Cloud, which serves as a computing environment for deploying services. The concept of face, which refers to the physical movements of the human face, is categorized under the broader term body parts. Similarly, the private service, a customizable web application, falls under the broader category of Services. The specific gesture hdgi:ThumbCurled is also classified as a type of body part. The human body, which encompasses all anatomical structures, is a broader term for Human. In the automotive industry, modern automobiles are classified under the broader term automobiles, highlighting their advanced technologies. Ontology engineering, which focuses on the design of knowledge representations, is a subset of ontology. The term rotation change, indicating a distinct alteration in orientation, is included under the broader concept of rotation. Furthermore, eight atomic gestures include movements involving a single body part. Different SDKs-systems utilize coordinate systems to define spatial positioning. System designers evaluate user studies to enhance user interaction. The hdgi:xAxisDirection is defined as either 'leftward', indicating a specific directional attribute. Separate ontology files, which contain mappings to external ontologies, are categorized under ontology. The hdgi:UpperArm models the upper arm and is classified under upper limbs. Different coordinate systems, which can lead to inconsistencies in gesture recognition, are a broader term for coordinate systems. Fingers, essential for gesture-based interactions, are also categorized under body parts. Existing ontologies provide structured knowledge representations and fall under the broader term ontology. Lastly, OWL2, an advanced ontology language, is classified under ontology.",
    "The ontological framework has a broader term known as concepts, which encompasses various ideas and attributes. Within this framework, the term affordance refers to the perceived and actual properties of the thing, highlighting how users interact with objects. The hdgi:zAxisDirection is defined as either 'outward', indicating a specific orientation in a local coordinate system. In the realm of ontology, sosa:ObservableProperty has a broader term that includes ontology itself, while research is categorized under the broader term study. Furthermore, API and architecture documentation falls under the broader category of APIs, and core ontology design patterns are also classified as ontologies. The studies, which investigate gesture recognition, are a subset of research. A referent, which signifies the desired effect of an action, has a broader term that includes referent. One approach in defining taxonomies allows for standardized definitions in gesture vocabularies. API client stubs are another component that falls under APIs. The eight atomic gestures include the end pose, which is a specific position in a gesture. Osumar et al. are recognized as authors who contributed to the development of gesture ontologies. The right palm, a significant body part in gesture recognition, is categorized under body parts. The end pose itself is a broader term for a pose, while namespace serves as a broader term for concepts. A search query can retrieve gestures to answer a call in a car, showcasing the practical application of these concepts. Additionally, hdgi:Rotation could be modeled using Euler angles, which describe the orientation of a body in 3D space. The human body, which has a broader term of human body, is central to these interactions. Lastly, the community allows anyone interested to join as a contributor, fostering collaboration in the development of these ontological resources.",
    "The concept of extensibility encompasses a broader term known as features, which refers to the specific characteristics associated with arm-based gestures. Within the anatomical context, the eight sections of the upper limb region are classified under the broader category of upper limbs. In the realm of directional attributes, hdgi:zAxisDirection and hdgi:xAxisDirection are both broader terms under the general concept of axis direction. The forearm, a crucial component of human gesture, also falls under this broader classification. Additionally, the hdgi:Palm is recognized as a fundamental part of body parts. In the field of software, a comprehensive view of the API is a broader term that includes APIs, which facilitate communication between different software applications. The Foundational Model of Anatomy serves as a foundational ontology, providing a structured representation of human anatomical structures. The hdgi:yAxisDirection is specifically defined as either 'downward', indicating its role in the local coordinate system. The SSN ontology describes the concept of Sample, which represents specific instances of data collected by sensors. Furthermore, multiple movements and poses of body parts are categorized under body parts, while potential human behaviors are classified under the broader term Human. The expressive power of a system is a broader term that includes attributes, which describe the specific properties of gestures. The first version of the Microsoft HoloLens is associated with the 'bloom' gesture, a specific hand movement used to interact with the device. Ontology building is a process that falls under the broader category of ontology. Instances of that property are defined by having that class as the subject, ensuring a structured approach to knowledge representation. Extrinsic properties, which are determined by external factors, are also categorized under attributes. In research, Riener et al. represent a broader group known as researchers, which also includes designers and researchers who work collaboratively in the field.",
    "Many researchers, a collective term for individuals engaged in systematic investigation, has a broader term of researchers. Among these, Scoditti et al. also falls under the broader category of researchers, contributing to the field with their proposed gestural interaction taxonomy. A large number of studies, which encompass extensive research efforts, has a broader term of research. Within the realm of Semantic Web standards, RDF is included as a key component, while RDF itself has a broader term of Semantic Web standards. The concept of Human, which refers to members of the species Homo sapiens, has a broader term of concepts that include various ideas related to gestures. The xPosition attribute is based on a local coordinate system, which is essential for defining spatial relationships in gesture interactions. External ontologies, which are frameworks used for mapping concepts, have a broader term of knowledge base. Gesture Elicitation Studies, methodologies for collecting gesture data, also fall under the broader term of research. The SSN ontology describes the Actuator, a device that converts control signals into physical actions. Gesture vocabularies, which represent defined sets of gestures, are defined in previous studies, while existing research has a broader term of research, highlighting the ongoing exploration of gesture recognition. The hdgi:Forearm is categorized under the broader term of the human's upper limb region. Swagger codegen, a tool for generating API documentation, has a broader term of documentation. In the context of Human Device Interaction, a particular actor is a broader term for an actor, referring to users interacting with devices. Apache Tomcat is a prerequisite to run the web application, which requires Apache Tomcat for its operation. The hdgi:LittleFinger, hdgi:MiddleFinger, and These fingers are all categorized under the broader term of body parts, representing the anatomical structures involved in gesture interactions.",
    "The upper limb region encompasses various concepts related to human anatomy and gesture interactions. Specifically, the hdgi:ForearmPose is defined in relation to the elbow joint, indicating its position and orientation in 3D space. In this context, the user's hand faces the palm, which is crucial for performing gestures. The Foundational Model of Anatomy (FMA) serves as a foundational ontology that categorizes these anatomical structures, while the sosa:Actuator is classified as a subclass of sosa:ActuatableProperty, highlighting its role in facilitating interactions. Additionally, the concept of expressive power is integral to understanding the range of gestures that can be represented, as is the semantics that underlie these interactions. The forearm, a key component of the upper limb, is also recognized as a building block of the upper limb region, further emphasizing its importance in gesture-based systems. Overall, these relationships illustrate the interconnectedness of anatomical structures, gestures, and the ontological frameworks that support their analysis.",
    "The concept of hdgi:zRotation is classified as a type of Euler angles, which are used to describe the orientation of objects in three-dimensional space. In the realm of ontology, the process of ontology building and annotating is encompassed within the broader term of ontology, which represents a structured framework of concepts and relationships. Similarly, hdgi:UpperArm is categorized under the broader term of upper limbs of the human body, highlighting its significance as a key component of human anatomy. Research conducted by Brown et al. falls under the broader category of researchers, while the results of this study contribute to the field of research. BMW, a prominent automotive manufacturer, is associated with its innovative iDrive infotainment system. However, a lack of linked data has resulted in challenges for researchers, limiting their ability to utilize existing gesture-related knowledge. The hdgi:zAxisDirection is defined as either 'inward' or 'outward', indicating its role in spatial orientation. Movements of multiple body parts are classified under the broader term of body parts, emphasizing the complexity of human gestures. The process of formalization is related to the broader concept of concepts, which includes various ideas and attributes. Listing 1.2 serves as a reference, providing an example of a pose modeling associated with specific gestures. The notion of the same referent is a broader term that encompasses the concept of referent itself. Additionally, the permanent URL service is categorized under Services, facilitating access to resources related to the HDGI ontology. The sosa:Actuator is a conceptual entity that falls under the broader term of concepts, while hdgi:RingFinger is classified as a body part. Choi et al. represent a group of authors contributing to the field of gesture recognition. The eight sections of upper limb region are recognized as part of the upper limbs of the human body, further detailing the anatomical components involved in gesture interactions. A new namespace is established as a broader term for namespace, allowing for unique identifiers in the HDGI ontology. Lastly, the leap-motion SDK is categorized under Software Development Kits, providing tools for gesture recognition applications.",
    "Figure 3 - point B is referenced in Figure 3, which illustrates the spatial relationships of various body parts. The upper arm positions are relative to the shoulder joint, highlighting the anatomical connections in gesture modeling. Researchers utilize standard definitions to ensure consistency in their studies, while system designers teach end users how to interact with these gestures effectively. In the realm of mathematics, hdgi:xRotation is a type of Euler angles, which are essential for understanding rotations in 3D space. Additionally, hdgi:Rotation could be modeled using quaternions, providing flexibility in representing poses. The research community encompasses various individual studies, all of which contribute to the broader field of research. zPosition values are based on a local coordinate system, which is crucial for accurately modeling gestures. Multiple studies have identified effective gestures for specific referents, further enriching the knowledge base. SPARQL endpoints serve as APIs that facilitate data access, while their other hand and hdgi:Forearm are both classified under body parts, emphasizing the importance of anatomical understanding in gesture recognition. Finally, ontology building and annotating is an integral part of ontology engineering, ensuring that the knowledge represented is both comprehensive and useful.",
    "The eight sections of the upper limb region include the upper arm, which serves as a key segment for various movements and gestures. In the context of gesture recognition, Palm and finger positions are categorized under body parts, highlighting their importance in human-device interactions. This research, which focuses on gesture recognition, falls under the broader category of research. The hdgi:Quaternion, a mathematical representation used in the HDGI ontology, is a specific type of quaternions that aids in modeling rotations in 3D space. Researchers, including Villarreal-Narvaez et al. and Khairunizam et al., adopt a standardized approach to defining gesture vocabularies, which is essential for the development of gestural interfaces. The face, along with the movement of the user\u2019s right arm, is also classified as body parts, emphasizing their role in gesture interactions. Furthermore, API and architecture documentation plays a crucial role in supporting RESTful endpoints and customized SPARQL endpoints, facilitating interaction with web services. The concept of cardinality, particularly the cardinality of many to many, allows for flexible associations between multiple affordances and devices, enhancing gesture recognition capabilities.",
    "Villarreal-Narvaez et al. is a group of researchers who can be classified under the broader term 'authors'. Similarly, Khairunizam et al. and Maier et al. also fall under the category of authors and researchers, respectively. The hdgi:Position class describes the local coordinate system, which is essential for defining the spatial placement of body parts in gestures. Researchers utilize existing knowledge to inform their studies, while further experiments are a subset of experiments aimed at validating gestural interaction methodologies. The term 'anyone interested to join as a contributor' encompasses individuals who wish to engage as contributors to the HDGI project. The API and architecture documentation plays a crucial role in supporting the functionality of a web application, which can operate as a private service. In terms of anatomy, the user's right arm is categorized under body parts, which also includes further body parts and the right hand. The hdgi:yPosition attribute is based on the local coordinate system, and upper arm positions are also classified as body parts. The Z-axis is defined as pointing inwards in certain gesture recognition systems. The hdgi:UpperArm class represents a component of the human's upper limb region, which is a broader category that includes upper limbs. Lastly, Figure 3 visually depicts the upper limb region, illustrating the relationships among various body parts.",
    "The concept of 'other body parts' is a broader term that encompasses 'body parts', which refers to the various physical components of the human body. Similarly, 'a search query' serves as a broader term for 'search query', indicating its role in information retrieval. In the realm of technical documentation, 'complete API documentation' can be found within 'API documentation', highlighting the comprehensive nature of the former. The 'hdgi:UpperArm' is identified as a building block of the 'upper limb region', which itself has a broader term of 'upper limbs'. Furthermore, 'private cloud' is a broader term for 'Cloud', indicating its exclusive use by a single organization. The term 'the experiment' is a broader term for 'experiment', emphasizing its systematic nature. The 'eight sections of upper limb region' are included within the 'upper limb region', which has a broader term of 'human's upper limb region'. In manufacturing, 'device manufacturers' is a broader term for 'manufacturer', while 'Design' is associated with 'rationale', reflecting the reasoning behind design choices. Additionally, 'a user\u2019s wrist' and 'their hand' are broader terms for 'body parts', illustrating their anatomical significance. The concept of 'extensibility' is a broader term for 'capability', indicating the potential for systems to adapt. Lastly, 'upper limbs' are a broader term for the 'human body', and 'Maier et al.' represents a broader term for 'authors', acknowledging their contributions to the field.",
    "FMA class definitions have a broader term known as taxonomies, which are structured frameworks for organizing information. Individual body parts, which are distinct anatomical segments, fall under the broader category of body parts. Mercedes-Benz, a renowned luxury automobile manufacturer, has developed the User Experience (MBUX) multimedia infotainment system to enhance user interaction. Wobbrock et al., a group of researchers, is a broader term that encompasses various authors in the field. The current literature, which documents gesture vocabularies relevant to Human Device Interactions, is part of the broader category of literature. The left-hand rule coordinate system is a specific type of coordinate system, and it is utilized by Unity3D11 for modeling spatial positions. Existing ontologies fit within the best practice framework, while w3id.org serves as a broader term for Services that facilitate integration with SPARQL endpoints. The hdgi:Thumb and hdgi:UpperArm are specific classes that represent body parts, and they are included in the broader category of body parts. Separate ontology files are hosted in GitHub, which is a platform that allows anyone interested to join as a contributor. Upper limbs, which include arms and hands, are categorized under body parts, and the human's upper limb region is a broader term for the upper limbs of the human body. The GitHub code repository is a broader term for code, which includes the software associated with the HDGI ontology. A similar study, which focuses on arm gesture recognition, is part of the broader field of research, as is their research that aims to enhance gesture recognition accuracy. Lastly, hdgi:yRotation is a type of Euler angles used to describe the orientation of poses in 3D space.",
    "The manufacturer's design decision is a concept that falls under the broader term of manufacturer, which refers to entities responsible for producing goods. In the anatomical context, the upper limb region is a more specific term that encompasses the upper limbs of the human body, which includes the arms and shoulders. Additionally, Services are deployed in a private cloud, a secure computing environment used exclusively by a single organization. The term few studies is a broader category that includes research efforts aimed at understanding various phenomena. Their own private cloud is another term that refers to a customizable private cloud. In terms of anatomy, the human's upper limb region can be further specified as the upper arm, while a single body part is a general term that includes various body parts such as the right palm, which is also categorized under body parts. The upper limb region can also be classified under the upper arm, and the upper limbs of the human body are part of the larger human body structure. In the realm of motion, yaw is a broader term that relates to rotation, which describes the orientation of a pose in three-dimensional space. The holistic posture of the human body is another term that encompasses the overall alignment of the body. Designers and manufacturers are involved in the broader category of manufacturers, while the local coordinate system is a specific type of coordinate system. The wrist, another body part, is also included in the broader category of body parts. The research community represents a collective of scholars within the broader community. Villarreal Narvaez, a notable researcher, is categorized under the broader term of authors, and the Z-axis is a specific component of coordinate systems.",
    "Villarreal-Narvaez is recognized as an author who has contributed to the field of research, which encompasses a broader category of researchers. In the context of body parts, the upper limbs of the human body, including the upper arm and palm, are modeled within the Foundational Model of Anatomy (FMA). The right forearm is specifically defined as the start pose in gesture sequences, illustrating its role in human motion. Additionally, the concepts of pitch and roll are both broader terms under the category of rotation, which can also be represented using quaternions. The term 'those authors' refers to Scoditti et al., who are included in the broader group of authors, similar to Villarreal-Narvaez and Norman, who also belong to this category. Furthermore, Turtle syntax is a specific type of syntax that falls under the broader umbrella of syntax, while SPARQL endpoints represent a broader term for SPARQL, which is essential for querying RDF data.",
    "The human's upper limb region and the upper limb region are both categorized under body parts, indicating their broader anatomical significance. In the realm of 3D rigid body dynamics, yaw, pitch, and roll describe the rotations around the vertical, x-axis, and z-axis respectively. The concept of OpenSource encompasses various initiatives, with OpenSource projects being a subset that operates under the Apache 2.0 license. Additionally, w3id.org utilizes the permanent URL service, which is essential for creating stable links to digital resources. Quaternions are employed by some systems to represent rotations in 3D rigid bodies. In the academic context, authors are a broader category that includes researchers, while previous studies fall under the umbrella of research. The right forearm, a specific body part, is classified as both a forearm and a body part, highlighting its anatomical relevance. Lastly, the many to many relationship is a specific type of cardinality that describes associations between multiple entities."
  ],
  "times": [
    814.5354368686676
  ]
}