{
  "iri": "Paper-90",
  "title": "INTERSPEECH_2008_28_abs",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-90-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-90-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-1",
              "text": "A critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-2",
              "text": "Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition -LRB- ASR -RRB- to investigate an offset adaptation -LRB- OA -RRB- model proposed by Zhang et al. -LSB- 1 -RSB- ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-3",
              "text": "OA improved phase locking in the auditory nerve -LRB- AN -RRB- and raised ASR accuracy for features derived from AN fibers -LRB- ANFs -RRB- ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-4",
              "text": "We also found that OA is crucial for auditory processing by onset neurons -LRB- ONs -RRB- in the next neuronal stage , the auditory brainstem ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-5",
              "text": "Multi-layer perceptrons -LRB- MLPs -RRB- performed much better than standard Gaussian mixture models -LRB- GMMs -RRB- for both our ANF-based and ON-based auditory features ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-6",
              "text": "Similar results were previously obtained with MSG -LRB- Modulation-filtered Spec-troGram -RRB- auditory features -LSB- 2 -RSB- ."
            },
            {
              "iri": "Paper-90-Section-1-Paragraph-1-Sentence-7",
              "text": "Thus we believe researchers working with novel features should consider trying MLPs ."
            }
          ]
        }
      ]
    }
  ],
  "times": [
    0.000179290771484375,
    39.337501764297485,
    54.03529500961304,
    42.53030300140381,
    0.07615137100219727,
    0.000118255615234375,
    0.00018858909606933594,
    40.22913694381714,
    58.41838812828064,
    1.8890187740325928,
    0.13907146453857422,
    0.01254129409790039,
    0.00023484230041503906,
    24.54840898513794,
    0.00031828880310058594,
    0.032523393630981445,
    0.0011012554168701172,
    2.570455312728882,
    4.276831865310669,
    15.129137754440308,
    121.80509042739868,
    4.512880086898804,
    63.10338473320007,
    1.69211745262146,
    0.0008945465087890625,
    0.01680755615234375
  ],
  "nodes": {
    "Entity-offset_adaptation": {
      "node_id": "offset_adaptation",
      "disambiguation_index": 0,
      "label": "offset adaptation",
      "aliases": [
        "offset adaptation"
      ],
      "types": [
        "theoretical concept",
        "model",
        "theory",
        "phenomenon"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "Offset adaptation (OA) refers to a model proposed by Zhang et al. that enhances phase locking in the auditory nerve and improves automatic speech recognition accuracy by addressing the dead time period after intense stimuli in auditory processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "offset adaptation",
          "local_types": [
            "theoretical concept",
            "model",
            "theory",
            "phenomenon"
          ],
          "iri": "Entity-offset_adaptation-Mention-1"
        }
      ],
      "relevance": 0.69677734375
    },
    "Entity-oa": {
      "node_id": "oa",
      "disambiguation_index": 0,
      "label": "OA",
      "aliases": [
        "OA"
      ],
      "types": [
        "intervention",
        "neuroanatomy",
        "entity",
        "abbreviation",
        "biological entity",
        "model",
        "concept",
        "offset adaptation",
        "treatment"
      ],
      "node_type": "named entity",
      "LLM_familiarity": false,
      "description": "OA refers to an offset adaptation model that enhances phase locking in the auditory nerve and improves automatic speech recognition accuracy for features derived from auditory nerve fibers.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "OA",
          "local_types": [
            "intervention",
            "entity",
            "concept",
            "abbreviation",
            "treatment"
          ],
          "iri": "Entity-oa-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-4",
          "local_name": "OA",
          "local_types": [
            "neuroanatomy",
            "abbreviation",
            "concept",
            "biological entity"
          ],
          "iri": "Entity-oa-Mention-2"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "OA",
          "local_types": [
            "offset adaptation",
            "model",
            "concept"
          ],
          "iri": "Entity-oa-Mention-3"
        }
      ],
      "relevance": 0.66064453125
    },
    "Entity-on-based_auditory_feature": {
      "node_id": "on-based_auditory_feature",
      "disambiguation_index": 0,
      "label": "ON-based auditory features",
      "aliases": [
        "ON-based auditory features"
      ],
      "types": [
        "feature",
        "feature extraction",
        "auditory feature"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "ON-based auditory features refer to auditory processing features derived from onset neurons in the auditory brainstem, which are utilized in multi-layer perceptron models for improved performance in sound encoding and recognition tasks.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-5",
          "local_name": "ON-based auditory features",
          "local_types": [
            "feature",
            "feature extraction",
            "auditory feature"
          ],
          "iri": "Entity-on-based_auditory_feature-Mention-1"
        }
      ],
      "relevance": 0.65966796875
    },
    "Entity-encoding_sound": {
      "node_id": "encoding_sound",
      "disambiguation_index": 0,
      "label": "encoding sound",
      "aliases": [
        "encoding sound for neuronal processing",
        "encoding sound"
      ],
      "types": [
        "process",
        "neuroscience",
        "encoding"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The term 'encoding sound' refers to the process by which analog pressure waves are transformed into discrete nerve-action potentials for neuronal processing in the auditory system.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "encoding sound",
          "local_types": [
            "process",
            "neuroscience"
          ],
          "iri": "Entity-encoding_sound-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "encoding sound for neuronal processing",
          "local_types": [
            "process",
            "encoding"
          ],
          "iri": "Entity-encoding_sound-Mention-2"
        }
      ],
      "relevance": 0.62548828125
    },
    "Entity-onset_neuron": {
      "node_id": "onset_neuron",
      "disambiguation_index": 0,
      "label": "onset neurons",
      "aliases": [
        "onset neurons"
      ],
      "types": [
        "neuroanatomy",
        "neuroscience",
        "neuron type",
        "neural cell type",
        "biological entity",
        "biological structure",
        "neuron"
      ],
      "node_type": "named entity",
      "LLM_familiarity": false,
      "description": "Onset neurons (ONs) are a type of neuron in the auditory brainstem that play a critical role in processing auditory information, particularly in relation to the timing of sound onset.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-4",
          "local_name": "onset neurons",
          "local_types": [
            "neuroanatomy",
            "neuroscience",
            "neuron type",
            "neural cell type",
            "biological entity",
            "biological structure",
            "neuron"
          ],
          "iri": "Entity-onset_neuron-Mention-1"
        }
      ],
      "relevance": 0.61572265625
    },
    "Entity-novel_feature": {
      "node_id": "novel_feature",
      "disambiguation_index": 0,
      "label": "novel features",
      "aliases": [
        "novel features"
      ],
      "types": [
        "feature type",
        "research concept",
        "feature",
        "innovation",
        "concept",
        "feature set"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The term 'novel features' refers to innovative auditory features derived from auditory nerve fibers and onset neurons that researchers are encouraged to explore using multi-layer perceptrons for improved performance in sound encoding and processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-7",
          "local_name": "novel features",
          "local_types": [
            "feature type",
            "research concept",
            "feature",
            "innovation",
            "concept",
            "feature set"
          ],
          "iri": "Entity-novel_feature-Mention-1"
        }
      ],
      "relevance": 0.6142578125
    },
    "Entity-feature": {
      "node_id": "feature",
      "disambiguation_index": 0,
      "label": "features",
      "aliases": [
        "features"
      ],
      "types": [
        "data attributes",
        "signal characteristics"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The term 'features' refers to the auditory characteristics derived from auditory nerve fibers (ANFs) that are used to enhance automatic speech recognition (ASR) accuracy.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "features",
          "local_types": [
            "data attributes",
            "signal characteristics"
          ],
          "iri": "Entity-feature-Mention-1"
        }
      ],
      "relevance": 0.609375
    },
    "Entity-an": {
      "node_id": "an",
      "disambiguation_index": 0,
      "label": "AN",
      "aliases": [
        "AN"
      ],
      "types": [
        "anatomy",
        "auditory nerve"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "AN refers to the auditory nerve, which is involved in encoding sound into nerve-action potentials and is crucial for auditory processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "AN",
          "local_types": [
            "anatomy",
            "auditory nerve"
          ],
          "iri": "Entity-an-Mention-1"
        }
      ],
      "relevance": 0.60693359375
    },
    "Entity-ons": {
      "node_id": "ons",
      "disambiguation_index": 0,
      "label": "ONs",
      "aliases": [
        "ONs"
      ],
      "types": [
        "anatomy",
        "onset neurons"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "ONs refers to onset neurons, which are critical for auditory processing in the auditory brainstem.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-4",
          "local_name": "ONs",
          "local_types": [
            "anatomy",
            "onset neurons"
          ],
          "iri": "Entity-ons-Mention-1"
        }
      ],
      "relevance": 0.60693359375
    },
    "Entity-sound": {
      "node_id": "sound",
      "disambiguation_index": 0,
      "label": "sound",
      "aliases": [
        "sound"
      ],
      "types": [
        "physical phenomenon"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "In this context, 'sound' refers to the analog pressure wave that is processed by the auditory system, specifically how it is encoded into discrete nerve-action potentials for neuronal processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "sound",
          "local_types": [
            "physical phenomenon"
          ],
          "iri": "Entity-sound-Mention-1"
        }
      ],
      "relevance": 0.6044921875
    },
    "Entity-pool_model_of_the_inner_hair_cell_synapse": {
      "node_id": "pool_model_of_the_inner_hair_cell_synapse",
      "disambiguation_index": 0,
      "label": "pool models of the inner hair cell synapse",
      "aliases": [
        "Recent pool models of the inner hair cell synapse",
        "pool models of the inner hair cell synapse"
      ],
      "types": [
        "biological structure",
        "model",
        "synapse"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The 'pool models of the inner hair cell synapse' refer to computational models that simulate the synaptic behavior of inner hair cells in the auditory system, specifically focusing on their response to stimuli and the associated dead time period after intense stimulation.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "pool models of the inner hair cell synapse",
          "local_types": [
            "synapse",
            "model"
          ],
          "iri": "Entity-pool_model_of_the_inner_hair_cell_synapse-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "Recent pool models of the inner hair cell synapse",
          "local_types": [
            "model",
            "biological structure"
          ],
          "iri": "Entity-pool_model_of_the_inner_hair_cell_synapse-Mention-2"
        }
      ],
      "relevance": 0.59912109375
    },
    "Entity-pool_model": {
      "node_id": "pool_model",
      "disambiguation_index": 0,
      "label": "pool models",
      "aliases": [
        "pool models"
      ],
      "types": [
        "computational model"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "Pool models refer to computational models of the inner hair cell synapse that aim to simulate neuronal responses to sound stimuli, specifically addressing the limitations in reproducing the dead time period following intense auditory stimuli.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "pool models",
          "local_types": [
            "computational model"
          ],
          "iri": "Entity-pool_model-Mention-1"
        }
      ],
      "relevance": 0.59912109375
    },
    "Entity-a_critical_step": {
      "node_id": "a_critical_step",
      "disambiguation_index": 0,
      "label": "A critical step",
      "aliases": [
        "A critical step"
      ],
      "types": [
        "process",
        "step"
      ],
      "node_type": "other",
      "LLM_familiarity": false,
      "description": "A critical step refers to the process of converting an analog pressure wave into discrete nerve-action potentials, which is essential for neuronal processing of sound.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "A critical step",
          "local_types": [
            "process",
            "step"
          ],
          "iri": "Entity-a_critical_step-Mention-1"
        }
      ],
      "relevance": 0.595703125
    },
    "Entity-asr": {
      "node_id": "asr",
      "disambiguation_index": 0,
      "label": "ASR",
      "aliases": [
        "ASR"
      ],
      "types": [
        "accuracy measure",
        "concept",
        "technology",
        "abbreviation",
        "automatic speech recognition",
        "signal processing"
      ],
      "node_type": "named entity",
      "LLM_familiarity": false,
      "description": "ASR refers to automatic speech recognition, a technology used to convert spoken language into text, which in this context is utilized to assess the accuracy of auditory features derived from auditory nerve fibers.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "ASR",
          "local_types": [
            "accuracy measure",
            "signal processing",
            "concept",
            "abbreviation"
          ],
          "iri": "Entity-asr-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "ASR",
          "local_types": [
            "automatic speech recognition",
            "technology"
          ],
          "iri": "Entity-asr-Mention-2"
        }
      ],
      "relevance": 0.59375
    },
    "Entity-feature_derived_from_an_fiber": {
      "node_id": "feature_derived_from_an_fiber",
      "disambiguation_index": 0,
      "label": "features derived from AN fibers",
      "aliases": [
        "features derived from AN fibers"
      ],
      "types": [
        "AN fibers",
        "feature"
      ],
      "node_type": "other",
      "LLM_familiarity": false,
      "description": "Features derived from AN fibers refer to auditory features that are extracted from the action potentials generated by auditory nerve fibers, which are crucial for processing sound in neuronal systems.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "features derived from AN fibers",
          "local_types": [
            "AN fibers",
            "feature"
          ],
          "iri": "Entity-feature_derived_from_an_fiber-Mention-1"
        }
      ],
      "relevance": 0.5869140625
    },
    "Entity-analog_pressure_wave": {
      "node_id": "analog_pressure_wave",
      "disambiguation_index": 0,
      "label": "analog pressure wave",
      "aliases": [
        "analog pressure wave"
      ],
      "types": [
        "sound",
        "physical phenomenon",
        "wave",
        "signal"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The analog pressure wave refers to the continuous sound wave that is transformed into discrete nerve-action potentials during the process of auditory encoding in the nervous system.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "analog pressure wave",
          "local_types": [
            "sound",
            "physical phenomenon",
            "wave",
            "signal"
          ],
          "iri": "Entity-analog_pressure_wave-Mention-1"
        }
      ],
      "relevance": 0.5849609375
    },
    "Entity-mlps": {
      "node_id": "mlps",
      "disambiguation_index": 0,
      "label": "MLPs",
      "aliases": [
        "MLPs"
      ],
      "types": [
        "multi-layer perceptrons",
        "neural network",
        "machine learning",
        "machine learning model",
        "model",
        "algorithm",
        "MLP"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "MLPs, or multi-layer perceptrons, are a type of artificial neural network used in machine learning that consists of multiple layers of nodes, where each layer is fully connected to the next.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-5",
          "local_name": "MLPs",
          "local_types": [
            "machine learning model",
            "neural network",
            "multi-layer perceptrons",
            "model"
          ],
          "iri": "Entity-mlps-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-7",
          "local_name": "MLPs",
          "local_types": [
            "machine learning",
            "machine learning model",
            "model",
            "algorithm",
            "MLP"
          ],
          "iri": "Entity-mlps-Mention-2"
        }
      ],
      "relevance": 0.58056640625
    },
    "Entity-an_fiber": {
      "node_id": "an_fiber",
      "disambiguation_index": 0,
      "label": "AN fibers",
      "aliases": [
        "AN fibers",
        "ANFs"
      ],
      "types": [
        "entity",
        "neuroscience",
        "nervous system",
        "biological structure",
        "auditory nerve fibers",
        "anatomy"
      ],
      "node_type": "named entity",
      "LLM_familiarity": false,
      "description": "AN fibers (ANFs) refer to the auditory nerve fibers that transmit sound information from the inner hair cells of the cochlea to the brain, playing a crucial role in auditory processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "AN fibers",
          "local_types": [
            "anatomy",
            "biological structure",
            "neuroscience",
            "nervous system"
          ],
          "iri": "Entity-an_fiber-Mention-1"
        },
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "ANFs",
          "local_types": [
            "anatomy",
            "entity",
            "auditory nerve fibers",
            "nervous system"
          ],
          "iri": "Entity-an_fiber-Mention-2"
        }
      ],
      "relevance": 0.57373046875
    },
    "Entity-auditory_processing": {
      "node_id": "auditory_processing",
      "disambiguation_index": 0,
      "label": "auditory processing",
      "aliases": [
        "auditory processing"
      ],
      "types": [
        "biological process",
        "process",
        "cognitive function",
        "neuroscience"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Auditory processing refers to the neurological and cognitive processes involved in the perception, interpretation, and understanding of sound stimuli by the brain.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-4",
          "local_name": "auditory processing",
          "local_types": [
            "biological process",
            "process",
            "cognitive function",
            "neuroscience"
          ],
          "iri": "Entity-auditory_processing-Mention-1"
        }
      ],
      "relevance": 0.57080078125
    },
    "Entity-phase_locking": {
      "node_id": "phase_locking",
      "disambiguation_index": 0,
      "label": "phase locking",
      "aliases": [
        "phase locking"
      ],
      "types": [
        "neural phenomenon",
        "phenomenon",
        "neuroscience concept",
        "auditory processing"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Phase locking is a neural phenomenon where the firing of neurons is synchronized to the phase of an external stimulus, often observed in auditory processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "phase locking",
          "local_types": [
            "neural phenomenon",
            "phenomenon",
            "neuroscience concept",
            "auditory processing"
          ],
          "iri": "Entity-phase_locking-Mention-1"
        }
      ],
      "relevance": 0.56982421875
    },
    "Entity-discrete_nerve-action_potential": {
      "node_id": "discrete_nerve-action_potential",
      "disambiguation_index": 0,
      "label": "discrete nerve-action potentials",
      "aliases": [
        "discrete nerve-action potentials"
      ],
      "types": [
        "neural signal",
        "nerve-action potential",
        "signal"
      ],
      "node_type": "other",
      "LLM_familiarity": false,
      "description": "Discrete nerve-action potentials refer to the quantized electrical signals generated by neurons in response to stimuli, which are essential for encoding sound for neuronal processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "discrete nerve-action potentials",
          "local_types": [
            "neural signal",
            "nerve-action potential",
            "signal"
          ],
          "iri": "Entity-discrete_nerve-action_potential-Mention-1"
        }
      ],
      "relevance": 0.55908203125
    },
    "Entity-inner_hair_cell_synapse": {
      "node_id": "inner_hair_cell_synapse",
      "disambiguation_index": 0,
      "label": "inner hair cell synapse",
      "aliases": [
        "inner hair cell synapse"
      ],
      "types": [
        "biological structure",
        "neuroscience"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "The inner hair cell synapse is a specialized junction in the auditory system where inner hair cells of the cochlea communicate with afferent neurons, playing a crucial role in the transmission of sound signals to the brain.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "inner hair cell synapse",
          "local_types": [
            "biological structure",
            "neuroscience"
          ],
          "iri": "Entity-inner_hair_cell_synapse-Mention-1"
        }
      ],
      "relevance": 0.55517578125
    },
    "Entity-auditory_feature": {
      "node_id": "auditory_feature",
      "disambiguation_index": 0,
      "label": "auditory features",
      "aliases": [
        "auditory features"
      ],
      "types": [
        "feature type",
        "signal processing",
        "feature"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Auditory features refer to the characteristics or attributes of sound that can be analyzed and processed, often used in the context of signal processing to describe various aspects of auditory stimuli.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-6",
          "local_name": "auditory features",
          "local_types": [
            "feature type",
            "signal processing",
            "feature"
          ],
          "iri": "Entity-auditory_feature-Mention-1"
        }
      ],
      "relevance": 0.55322265625
    },
    "Entity-the_dead_time_period_after_an_intense_stimulus": {
      "node_id": "the_dead_time_period_after_an_intense_stimulus",
      "disambiguation_index": 0,
      "label": "the dead time period after an intense stimulus",
      "aliases": [
        "the dead time period after an intense stimulus"
      ],
      "types": [
        "phenomenon"
      ],
      "node_type": "other",
      "LLM_familiarity": false,
      "description": "The dead time period after an intense stimulus refers to a brief interval during which the inner hair cell synapse in the auditory system fails to respond to subsequent stimuli following an intense auditory input, impacting the encoding of sound into nerve-action potentials.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "the dead time period after an intense stimulus",
          "local_types": [
            "phenomenon"
          ],
          "iri": "Entity-the_dead_time_period_after_an_intense_stimulus-Mention-1"
        }
      ],
      "relevance": 0.55322265625
    },
    "Entity-modulation-filtered_spec-trogram": {
      "node_id": "modulation-filtered_spec-trogram",
      "disambiguation_index": 0,
      "label": "Modulation-filtered Spec-troGram",
      "aliases": [
        "Modulation-filtered Spec-troGram"
      ],
      "types": [
        "method",
        "auditory feature"
      ],
      "node_type": "other",
      "LLM_familiarity": false,
      "description": "The Modulation-filtered Spec-troGram (MSG) is an auditory feature extraction method that enhances the representation of sound signals by filtering them based on modulation characteristics, thereby improving the accuracy of automatic speech recognition and auditory processing models.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-6",
          "local_name": "Modulation-filtered Spec-troGram",
          "local_types": [
            "method",
            "auditory feature"
          ],
          "iri": "Entity-modulation-filtered_spec-trogram-Mention-1"
        }
      ],
      "relevance": 0.5498046875
    },
    "Entity-zhang_et_al_.": {
      "node_id": "zhang_et_al_.",
      "disambiguation_index": 0,
      "label": "Zhang et al.",
      "aliases": [
        "Zhang et al."
      ],
      "types": [
        "researcher",
        "author"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "Zhang et al. refers to a group of researchers or authors who have contributed to the study of auditory processing and synaptic models in the context of inner hair cells.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "Zhang et al.",
          "local_types": [
            "researcher",
            "author"
          ],
          "iri": "Entity-zhang_et_al_.-Mention-1"
        }
      ],
      "relevance": 0.5458984375
    },
    "Entity-asr_accuracy": {
      "node_id": "asr_accuracy",
      "disambiguation_index": 0,
      "label": "ASR accuracy",
      "aliases": [
        "ASR accuracy"
      ],
      "types": [
        "evaluation measure",
        "metric",
        "performance metric"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "ASR accuracy refers to a performance metric that quantifies the effectiveness of automatic speech recognition systems in correctly transcribing spoken language.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "ASR accuracy",
          "local_types": [
            "evaluation measure",
            "metric",
            "performance metric"
          ],
          "iri": "Entity-asr_accuracy-Mention-1"
        }
      ],
      "relevance": 0.54443359375
    },
    "Entity-auditory_brainstem": {
      "node_id": "auditory_brainstem",
      "disambiguation_index": 0,
      "label": "auditory brainstem",
      "aliases": [
        "auditory brainstem"
      ],
      "types": [
        "neuroanatomy",
        "brain structure",
        "neuroscience",
        "brain region",
        "anatomy",
        "anatomical structure"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "The auditory brainstem is a collection of neural structures located in the brainstem that are involved in processing auditory information from the ears before it is relayed to higher brain centers.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-4",
          "local_name": "auditory brainstem",
          "local_types": [
            "neuroanatomy",
            "brain structure",
            "neuroscience",
            "brain region",
            "anatomy",
            "anatomical structure"
          ],
          "iri": "Entity-auditory_brainstem-Mention-1"
        }
      ],
      "relevance": 0.5390625
    },
    "Entity-dead_time_period": {
      "node_id": "dead_time_period",
      "disambiguation_index": 0,
      "label": "dead time period",
      "aliases": [
        "dead time period"
      ],
      "types": [
        "phenomenon",
        "temporal phenomenon"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "The 'dead time period' refers to a specific interval following an intense stimulus during which the inner hair cell synapse fails to respond, impacting the encoding of sound into nerve-action potentials.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "dead time period",
          "local_types": [
            "phenomenon",
            "temporal phenomenon"
          ],
          "iri": "Entity-dead_time_period-Mention-1"
        }
      ],
      "relevance": 0.5361328125
    },
    "Entity-multi-layer_perceptrons": {
      "node_id": "multi-layer_perceptrons",
      "disambiguation_index": 0,
      "label": "Multi-layer perceptrons",
      "aliases": [
        "Multi-layer perceptrons"
      ],
      "types": [
        "machine learning model",
        "neural network",
        "model",
        "artificial intelligence"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "Multi-layer perceptrons are a class of feedforward artificial neural networks that consist of multiple layers of nodes, where each layer is fully connected to the next, and are commonly used for supervised learning tasks in machine learning.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-5",
          "local_name": "Multi-layer perceptrons",
          "local_types": [
            "machine learning model",
            "neural network",
            "model",
            "artificial intelligence"
          ],
          "iri": "Entity-multi-layer_perceptrons-Mention-1"
        }
      ],
      "relevance": 0.53564453125
    },
    "Entity-msg": {
      "node_id": "msg",
      "disambiguation_index": 0,
      "label": "MSG",
      "aliases": [
        "MSG"
      ],
      "types": [
        "feature type",
        "Modulation-filtered Spec-troGram",
        "method",
        "auditory processing",
        "acronym",
        "auditory feature",
        "feature extraction method",
        "signal processing technique",
        "model"
      ],
      "node_type": "named entity",
      "LLM_familiarity": false,
      "description": "MSG refers to Modulation-filtered Spectrogram, a method used for auditory feature extraction in sound processing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-6",
          "local_name": "MSG",
          "local_types": [
            "feature type",
            "Modulation-filtered Spec-troGram",
            "method",
            "auditory processing",
            "acronym",
            "auditory feature",
            "feature extraction method",
            "signal processing technique",
            "model"
          ],
          "iri": "Entity-msg-Mention-1"
        }
      ],
      "relevance": 0.533203125
    },
    "Entity-auditory_nerve": {
      "node_id": "auditory_nerve",
      "disambiguation_index": 0,
      "label": "auditory nerve",
      "aliases": [
        "auditory nerve"
      ],
      "types": [
        "neuroscience",
        "nervous system",
        "biological structure",
        "anatomy",
        "nerve"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "The auditory nerve is a bundle of nerve fibers that transmits auditory information from the inner ear to the brain, playing a crucial role in the sense of hearing.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-3",
          "local_name": "auditory nerve",
          "local_types": [
            "neuroscience",
            "nervous system",
            "biological structure",
            "anatomy",
            "nerve"
          ],
          "iri": "Entity-auditory_nerve-Mention-1"
        }
      ],
      "relevance": 0.53125
    },
    "Entity-neuronal_processing": {
      "node_id": "neuronal_processing",
      "disambiguation_index": 0,
      "label": "neuronal processing",
      "aliases": [
        "neuronal processing"
      ],
      "types": [
        "biological process",
        "neuroscience"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Neuronal processing refers to the series of biological and neural mechanisms through which neurons encode, transmit, and interpret information, particularly in the context of sensory input and cognitive functions.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "neuronal processing",
          "local_types": [
            "biological process",
            "neuroscience"
          ],
          "iri": "Entity-neuronal_processing-Mention-1"
        }
      ],
      "relevance": 0.51806640625
    },
    "Entity-researcher_working_with_novel_feature": {
      "node_id": "researcher_working_with_novel_feature",
      "disambiguation_index": 0,
      "label": "researchers working with novel features",
      "aliases": [
        "researchers working with novel features"
      ],
      "types": [
        "researcher"
      ],
      "node_type": "general term",
      "LLM_familiarity": false,
      "description": "Researchers who are developing or utilizing innovative auditory features for sound processing and analysis.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-7",
          "local_name": "researchers working with novel features",
          "local_types": [
            "researcher"
          ],
          "iri": "Entity-researcher_working_with_novel_feature-Mention-1"
        }
      ],
      "relevance": 0.51171875
    },
    "Entity-automatic_speech_recognition": {
      "node_id": "automatic_speech_recognition",
      "disambiguation_index": 0,
      "label": "automatic speech recognition",
      "aliases": [
        "automatic speech recognition"
      ],
      "types": [
        "tool",
        "technology",
        "method",
        "artificial intelligence"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Automatic speech recognition is a technology that enables the conversion of spoken language into text by using algorithms and machine learning techniques.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "automatic speech recognition",
          "local_types": [
            "tool",
            "technology",
            "method",
            "artificial intelligence"
          ],
          "iri": "Entity-automatic_speech_recognition-Mention-1"
        }
      ],
      "relevance": 0.499755859375
    },
    "Entity-nerve-action_potential": {
      "node_id": "nerve-action_potential",
      "disambiguation_index": 0,
      "label": "nerve-action potentials",
      "aliases": [
        "nerve-action potentials"
      ],
      "types": [
        "neural activity",
        "neuroscience",
        "biological signal"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Nerve-action potentials are rapid electrical signals generated by neurons that transmit information through the nervous system.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-1",
          "local_name": "nerve-action potentials",
          "local_types": [
            "neural activity",
            "neuroscience",
            "biological signal"
          ],
          "iri": "Entity-nerve-action_potential-Mention-1"
        }
      ],
      "relevance": 0.469970703125
    },
    "Entity-gmms": {
      "node_id": "gmms",
      "disambiguation_index": 0,
      "label": "GMMs",
      "aliases": [
        "GMMs"
      ],
      "types": [
        "Gaussian mixture models",
        "statistical method",
        "machine learning model",
        "statistical model",
        "model"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "GMMs, or Gaussian mixture models, are probabilistic models used in statistics and machine learning to represent the presence of subpopulations within an overall population, where each subpopulation is modeled as a Gaussian distribution.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-5",
          "local_name": "GMMs",
          "local_types": [
            "Gaussian mixture models",
            "statistical method",
            "machine learning model",
            "statistical model",
            "model"
          ],
          "iri": "Entity-gmms-Mention-1"
        }
      ],
      "relevance": 0.380859375
    },
    "Entity-researcher": {
      "node_id": "researcher",
      "disambiguation_index": 0,
      "label": "researchers",
      "aliases": [
        "researchers"
      ],
      "types": [
        "academic",
        "person",
        "individual",
        "profession",
        "researcher"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Researchers are individuals engaged in systematic investigation and study to establish facts and reach new conclusions, often within academic or scientific fields.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-7",
          "local_name": "researchers",
          "local_types": [
            "academic",
            "person",
            "individual",
            "profession",
            "researcher"
          ],
          "iri": "Entity-researcher-Mention-1"
        }
      ],
      "relevance": 0.37890625
    },
    "Entity-gaussian_mixture_model": {
      "node_id": "gaussian_mixture_model",
      "disambiguation_index": 0,
      "label": "Gaussian mixture models",
      "aliases": [
        "Gaussian mixture models"
      ],
      "types": [
        "statistical method",
        "machine learning",
        "machine learning model",
        "statistical model",
        "model"
      ],
      "node_type": "named entity",
      "LLM_familiarity": true,
      "description": "Gaussian mixture models are probabilistic models that represent a distribution of data as a mixture of multiple Gaussian distributions, commonly used in statistical analysis and machine learning for clustering and density estimation.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-5",
          "local_name": "Gaussian mixture models",
          "local_types": [
            "statistical method",
            "machine learning",
            "machine learning model",
            "statistical model",
            "model"
          ],
          "iri": "Entity-gaussian_mixture_model-Mention-1"
        }
      ],
      "relevance": 0.371826171875
    },
    "Entity-visual_inspection": {
      "node_id": "visual_inspection",
      "disambiguation_index": 0,
      "label": "visual inspection",
      "aliases": [
        "visual inspection"
      ],
      "types": [
        "technique",
        "methodology",
        "research technique",
        "method"
      ],
      "node_type": "general term",
      "LLM_familiarity": true,
      "description": "Visual inspection is a technique used to assess and evaluate objects or phenomena through direct observation without the use of instruments or tools.",
      "mentions": [
        {
          "reference": "Paper-90-Section-1-Paragraph-1-Sentence-2",
          "local_name": "visual inspection",
          "local_types": [
            "technique",
            "methodology",
            "research technique",
            "method"
          ],
          "iri": "Entity-visual_inspection-Mention-1"
        }
      ],
      "relevance": 0.34033203125
    }
  },
  "summary": "A critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials . Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition -LRB- ASR -RRB- to investigate an offset adaptation -LRB- OA -RRB- model proposed by Zhang et al. -LSB- 1 -RSB- . OA improved phase locking in the auditory nerve -LRB- AN -RRB- and raised ASR accuracy for features derived from AN fibers -LRB- ANFs -RRB- . We also found that OA is crucial for auditory processing by onset neurons -LRB- ONs -RRB- in the next neuronal stage , the auditory brainstem . Multi-layer perceptrons -LRB- MLPs -RRB- performed much better than standard Gaussian mixture models -LRB- GMMs -RRB- for both our ANF-based and ON-based auditory features . Similar results were previously obtained with MSG -LRB- Modulation-filtered Spec-troGram -RRB- auditory features -LSB- 2 -RSB- . Thus we believe researchers working with novel features should consider trying MLPs .",
  "triples": [
    [
      "Entity-a_critical_step",
      "Predicate-occurs_when",
      "Entity-encoding_sound"
    ],
    [
      "Entity-analog_pressure_wave",
      "Predicate-is_coded_into",
      "Entity-nerve-action_potential"
    ],
    [
      "Entity-a_critical_step",
      "Predicate-occurs_in",
      "Entity-encoding_sound"
    ],
    [
      "Entity-analog_pressure_wave",
      "Predicate-is_coded_into",
      "Entity-discrete_nerve-action_potential"
    ],
    [
      "Entity-pool_model_of_the_inner_hair_cell_synapse",
      "Predicate-do_not_reproduce",
      "Entity-the_dead_time_period_after_an_intense_stimulus"
    ],
    [
      "Entity-visual_inspection",
      "Predicate-used_to_investigate",
      "Entity-offset_adaptation"
    ],
    [
      "Entity-automatic_speech_recognition",
      "Predicate-used_to_investigate",
      "Entity-offset_adaptation"
    ],
    [
      "Entity-offset_adaptation",
      "Predicate-model_proposed_by",
      "Entity-zhang_et_al_."
    ],
    [
      "Entity-oa",
      "Predicate-improved",
      "Entity-phase_locking"
    ],
    [
      "Entity-oa",
      "Predicate-raised",
      "Entity-asr_accuracy"
    ],
    [
      "Entity-asr_accuracy",
      "Predicate-for",
      "Entity-feature_derived_from_an_fiber"
    ],
    [
      "Entity-oa",
      "Predicate-is_crucial_for",
      "Entity-auditory_processing"
    ],
    [
      "Entity-onset_neuron",
      "Predicate-process",
      "Entity-auditory_processing"
    ],
    [
      "Entity-multi-layer_perceptrons",
      "Predicate-performed_better_than",
      "Entity-gaussian_mixture_model"
    ],
    [
      "Entity-mlps",
      "Predicate-performed_better_than",
      "Entity-gmms"
    ],
    [
      "Entity-msg",
      "Predicate-obtained_with",
      "Entity-auditory_feature"
    ],
    [
      "Entity-modulation-filtered_spec-trogram",
      "Predicate-obtained_with",
      "Entity-auditory_feature"
    ],
    [
      "Entity-researcher_working_with_novel_feature",
      "Predicate-should_consider_trying",
      "Entity-mlps"
    ],
    [
      "Entity-offset_adaptation",
      "Predicate-improves_performance_of_ON-based_auditory_features",
      "Entity-on-based_auditory_feature"
    ],
    [
      "Entity-offset_adaptation",
      "Predicate-is_a_model_of",
      "Entity-oa"
    ],
    [
      "Entity-oa",
      "Predicate-improves_performance_of_ON-based_auditory_features",
      "Entity-on-based_auditory_feature"
    ],
    [
      "Entity-encoding_sound",
      "Predicate-improved",
      "Entity-offset_adaptation"
    ],
    [
      "Entity-oa",
      "Predicate-improved",
      "Entity-encoding_sound"
    ],
    [
      "Entity-encoding_sound",
      "Predicate-utilized_in",
      "Entity-on-based_auditory_feature"
    ]
  ],
  "triples_typing": [
    [
      "Entity-ons",
      "skos:broader",
      "Entity-onset_neuron"
    ],
    [
      "Entity-on-based_auditory_feature",
      "skos:broader",
      "Entity-feature"
    ],
    [
      "Entity-oa",
      "skos:broader",
      "Entity-offset_adaptation"
    ],
    [
      "Entity-msg",
      "skos:broader",
      "Entity-auditory_feature"
    ],
    [
      "Entity-gmms",
      "skos:broader",
      "Entity-gaussian_mixture_model"
    ],
    [
      "Entity-auditory_feature",
      "skos:broader",
      "Entity-feature"
    ],
    [
      "Entity-asr",
      "skos:broader",
      "Entity-automatic_speech_recognition"
    ],
    [
      "Entity-discrete_nerve-action_potential",
      "skos:broader",
      "Entity-nerve-action_potential"
    ],
    [
      "Entity-msg",
      "skos:broader",
      "Entity-modulation-filtered_spec-trogram"
    ],
    [
      "Entity-on-based_auditory_feature",
      "skos:broader",
      "Entity-auditory_feature"
    ],
    [
      "Entity-an",
      "skos:broader",
      "Entity-auditory_nerve"
    ],
    [
      "Entity-analog_pressure_wave",
      "skos:broader",
      "Entity-sound"
    ],
    [
      "Entity-novel_feature",
      "skos:broader",
      "Entity-feature"
    ],
    [
      "Entity-zhang_et_al_.",
      "skos:broader",
      "Entity-researcher"
    ],
    [
      "Entity-an_fiber",
      "skos:broader",
      "Entity-auditory_nerve"
    ],
    [
      "Entity-phase_locking",
      "skos:broader",
      "Entity-auditory_processing"
    ],
    [
      "Entity-feature_derived_from_an_fiber",
      "skos:broader",
      "Entity-feature"
    ],
    [
      "Entity-mlps",
      "skos:broader",
      "Entity-multi-layer_perceptrons"
    ],
    [
      "Entity-msg",
      "skos:broader",
      "Entity-auditory_processing"
    ],
    [
      "Entity-researcher_working_with_novel_feature",
      "skos:broader",
      "Entity-researcher"
    ],
    [
      "Entity-modulation-filtered_spec-trogram",
      "skos:broader",
      "Entity-auditory_feature"
    ],
    [
      "Entity-feature_derived_from_an_fiber",
      "skos:broader",
      "Entity-an_fiber"
    ]
  ],
  "predicates": {
    "Predicate-occurs_when": {
      "label": "occurs when",
      "description": "The predicate 'occurs when' establishes a temporal or conditional relationship between the subject and the object, indicating that the subject is realized or takes place in the context or under the circumstances defined by the object.",
      "disambiguation_index": 0
    },
    "Predicate-is_coded_into": {
      "label": "is coded into",
      "description": "The predicate 'is coded into' indicates a process or transformation in which the subject is represented or encoded in a specific form or medium, resulting in the object. This implies that the subject undergoes a change that allows it to be interpreted, transmitted, or stored as the object, often involving a conversion of information from one format to another.",
      "disambiguation_index": 0
    },
    "Predicate-occurs_in": {
      "label": "occurs in",
      "description": "The predicate 'occurs in' establishes a relationship where the subject is identified as a component, event, or element that takes place within the context or framework defined by the object. It indicates that the subject is part of, or is happening during, the process or situation described by the object.",
      "disambiguation_index": 0
    },
    "Predicate-do_not_reproduce": {
      "label": "do not reproduce",
      "description": "The predicate 'do not reproduce' indicates that the subject fails to generate or create the object in question. It implies a lack of replication or manifestation of the characteristics, behaviors, or phenomena represented by the object, suggesting that the subject's processes or models are unable to reflect or account for the specified aspect.",
      "disambiguation_index": 0
    },
    "Predicate-used_to_investigate": {
      "label": "used to investigate",
      "description": "The predicate 'used to investigate' indicates a relationship where the subject serves as a method, tool, or approach employed to examine, analyze, or explore the object. It implies that the subject is utilized for the purpose of gaining insights, understanding, or uncovering information related to the object.",
      "disambiguation_index": 0
    },
    "Predicate-model_proposed_by": {
      "label": "model proposed by",
      "description": "The predicate 'model proposed by' establishes a relationship where the subject represents a specific concept, method, or framework that has been introduced or suggested, while the object identifies the authors or researchers responsible for the development or formulation of that model.",
      "disambiguation_index": 0
    },
    "Predicate-improved": {
      "label": "improved",
      "description": "The predicate 'improved' indicates a positive change or enhancement in the quality, performance, or effectiveness of the object as a result of the subject's influence or action. It suggests that the subject has contributed to making the object better in some measurable or observable way.",
      "disambiguation_index": 0
    },
    "Predicate-raised": {
      "label": "raised",
      "description": "The predicate 'raised' indicates an action or process in which the subject has increased, improved, or elevated the state, level, or value of the object. In the context of the example, it suggests that the subject (OA) has contributed to a positive change in the performance metric represented by the object (ASR accuracy). This implies a causal relationship where the subject's actions or interventions lead to a measurable enhancement in the specified outcome.",
      "disambiguation_index": 0
    },
    "Predicate-for": {
      "label": "for",
      "description": "The predicate 'for' indicates a relationship of purpose, benefit, or relevance between the subject and the object. It suggests that the subject is associated with, or serves a function related to, the object, often implying that the object provides a context, support, or justification for the subject's existence or evaluation.",
      "disambiguation_index": 0
    },
    "Predicate-is_crucial_for": {
      "label": "is crucial for",
      "description": "The predicate 'is crucial for' indicates that the subject plays an essential role in enabling, supporting, or facilitating the object. It suggests that the object cannot be effectively achieved, understood, or performed without the presence or influence of the subject, highlighting the importance of the relationship between the two.",
      "disambiguation_index": 0
    },
    "Predicate-process": {
      "label": "process",
      "description": "The predicate 'process' indicates an action or function performed by the subject that transforms, analyzes, or manipulates the object in some way. It signifies a relationship where the subject engages in a systematic series of operations or activities that lead to the understanding, interpretation, or alteration of the object.",
      "disambiguation_index": 0
    },
    "Predicate-performed_better_than": {
      "label": "performed better than",
      "description": "The predicate 'performed better than' establishes a comparative relationship between the subject and the object, indicating that the subject achieved superior results or outcomes in a specific context or evaluation compared to the object. This implies a measurable difference in performance, effectiveness, or efficiency, where the subject is recognized as the more successful entity in the comparison.",
      "disambiguation_index": 0
    },
    "Predicate-obtained_with": {
      "label": "obtained with",
      "description": "The predicate 'obtained with' indicates a relationship where the subject has acquired or achieved something through the use of the object. It implies that the object serves as a means, method, or tool that facilitates the acquisition or realization of the subject.",
      "disambiguation_index": 0
    },
    "Predicate-should_consider_trying": {
      "label": "should consider trying",
      "description": "The predicate 'should consider trying' indicates a recommendation or suggestion for the subject to explore or experiment with the object. It implies that the subject is encouraged to evaluate the potential benefits or effectiveness of the object in their context, suggesting that the object may offer valuable insights, improvements, or solutions that are worth investigating.",
      "disambiguation_index": 0
    },
    "Predicate-improves_performance_of_ON-based_auditory_features": {
      "label": "improves performance of ON-based auditory features",
      "description": "The predicate 'improves performance of ON-based auditory features' indicates a relationship where the subject contributes positively to the effectiveness or efficiency of the auditory features that are based on ON (a specific framework or model). This suggests that the subject enhances the functionality or quality of the auditory features, leading to better outcomes in their application or analysis.",
      "disambiguation_index": 0
    },
    "Predicate-is_a_model_of": {
      "label": "is a model of",
      "description": "The predicate 'is a model of' establishes a relationship where the subject represents a theoretical or conceptual framework that exemplifies, illustrates, or serves as a representation of the object. This indicates that the subject provides a systematic or structured approach to understanding, analyzing, or applying the principles embodied in the object.",
      "disambiguation_index": 0
    },
    "Predicate-utilized_in": {
      "label": "utilized in",
      "description": "The predicate 'utilized in' indicates that the subject is employed or applied within the context of the object, suggesting a functional relationship where the subject serves a purpose or contributes to the processes, characteristics, or outcomes associated with the object.",
      "disambiguation_index": 0
    },
    "skos:broader": {
      "label": "has a broader term",
      "description": "The predicate 'has a broader term' establishes a hierarchical relationship between the subject and the object, indicating that the subject is a specific instance or category that falls under a more general classification represented by the object. This relationship implies that the object encompasses a wider scope or range of concepts, of which the subject is a part.",
      "disambiguation_index": 0
    }
  }
}